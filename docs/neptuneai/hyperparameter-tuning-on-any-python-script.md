# å¦‚ä½•é€šè¿‡ 3 ä¸ªç®€å•çš„æ­¥éª¤åœ¨ä»»ä½• Python è„šæœ¬ä¸Šè¿›è¡Œè¶…å‚æ•°è°ƒä¼˜

> åŽŸæ–‡ï¼š<https://web.archive.org/web/https://neptune.ai/blog/hyperparameter-tuning-on-any-python-script>

ä½ å†™äº†ä¸€ä¸ª Python è„šæœ¬ï¼Œç”¨æ¥è®­ç»ƒå’Œè¯„ä¼°ä½ çš„æœºå™¨å­¦ä¹ æ¨¡åž‹ã€‚çŽ°åœ¨ï¼Œæ‚¨æƒ³è‡ªåŠ¨è°ƒæ•´è¶…å‚æ•°ä»¥æé«˜å…¶æ€§èƒ½å—ï¼Ÿ

æˆ‘æŠ“ä½ä½ äº†ï¼

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•å°†æ‚¨çš„è„šæœ¬è½¬æ¢æˆå¯ä»¥ç”¨ä»»ä½•è¶…å‚æ•°ä¼˜åŒ–åº“è¿›è¡Œä¼˜åŒ–çš„ç›®æ ‡å‡½æ•°ã€‚

åªéœ€ 3 ä¸ªæ­¥éª¤ï¼Œä½ å°±å¯ä»¥åƒæ²¡æœ‰æ˜Žå¤©ä¸€æ ·è°ƒæ•´æ¨¡åž‹å‚æ•°ã€‚

å‡†å¤‡å¥½äº†å—ï¼Ÿ

æˆ‘ä»¬èµ°å§ï¼

æˆ‘æƒ³ä½ çš„`main.py`è„šæœ¬åº”è¯¥æ˜¯è¿™æ ·çš„:

```py
import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import train_test_split

data = pd.read_csv('data/train.csv', nrows=10000)
X = data.drop(['ID_code', 'target'], axis=1)
y = data['target']
(X_train, X_valid, 
y_train, y_valid )= train_test_split(X, y, test_size=0.2, random_state=1234)

train_data = lgb.Dataset(X_train, label=y_train)
valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)

params = {'objective': 'binary',
          'metric': 'auc',
          'learning_rate': 0.4,
          'max_depth': 15,
          'num_leaves': 20,
          'feature_fraction': 0.8,
          'subsample': 0.2}

model = lgb.train(params, train_data,
                  num_boost_round=300,
                  early_stopping_rounds=30,
                  valid_sets=[valid_data],
                  valid_names=['valid'])

score = model.best_score['valid']['auc']
print('validation AUC:', score)
```

## æ­¥éª¤ 1:ä»Žä»£ç ä¸­åˆ†ç¦»æœç´¢å‚æ•°

èŽ·å–æ‚¨æƒ³è¦è°ƒæ•´çš„å‚æ•°ï¼Œå¹¶å°†å®ƒä»¬æ”¾åœ¨è„šæœ¬é¡¶éƒ¨çš„å­—å…¸ä¸­ã€‚è¿™æ ·åšå¯ä»¥æœ‰æ•ˆåœ°å°†æœç´¢å‚æ•°ä»Žä»£ç çš„å…¶ä½™éƒ¨åˆ†ä¸­åˆ†ç¦»å‡ºæ¥ã€‚

```py
import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import train_test_split

SEARCH_PARAMS = {'learning_rate': 0.4,
                 'max_depth': 15,
                 'num_leaves': 20,
                 'feature_fraction': 0.8,
                 'subsample': 0.2}

data = pd.read_csv('../data/train.csv', nrows=10000)
X = data.drop(['ID_code', 'target'], axis=1)
y = data['target']
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=1234)

train_data = lgb.Dataset(X_train, label=y_train)
valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)

params = {'objective': 'binary',
          'metric': 'auc',
          **SEARCH_PARAMS}

model = lgb.train(params, train_data,
                  num_boost_round=300,
                  early_stopping_rounds=30,
                  valid_sets=[valid_data],
                  valid_names=['valid'])

score = model.best_score['valid']['auc']
print('validation AUC:', score)
```

## æ­¥éª¤ 2:å°†åŸ¹è®­å’Œè¯„ä¼°æ‰“åŒ…æˆä¸€ä¸ªåŠŸèƒ½

çŽ°åœ¨ï¼Œæ‚¨å¯ä»¥å°†æ•´ä¸ªè®­ç»ƒå’Œè¯„ä¼°é€»è¾‘æ”¾åœ¨ä¸€ä¸ª`train_evaluate`å‡½æ•°ä¸­ã€‚è¯¥å‡½æ•°å°†å‚æ•°ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºéªŒè¯åˆ†æ•°ã€‚

```py
import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import train_test_split

SEARCH_PARAMS = {'learning_rate': 0.4,
                 'max_depth': 15,
                 'num_leaves': 20,
                 'feature_fraction': 0.8,
                 'subsample': 0.2}

def train_evaluate(search_params):
    data = pd.read_csv('../data/train.csv', nrows=10000)
    X = data.drop(['ID_code', 'target'], axis=1)
    y = data['target']
    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=1234)

    train_data = lgb.Dataset(X_train, label=y_train)
    valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)

    params = {'objective': 'binary',
              'metric': 'auc',
              **search_params}

    model = lgb.train(params, train_data,
                      num_boost_round=300,
                      early_stopping_rounds=30,
                      valid_sets=[valid_data],
                      valid_names=['valid'])

    score = model.best_score['valid']['auc']
    return score

if __name__ == '__main__':
    score = train_evaluate(SEARCH_PARAMS)
    print('validation AUC:', score)
```

## æ­¥éª¤ 3:è¿è¡Œ hypeparameter è°ƒæ•´è„šæœ¬

æˆ‘ä»¬å¿«åˆ°äº†ã€‚

ä½ çŽ°åœ¨éœ€è¦åšçš„å°±æ˜¯ä½¿ç”¨è¿™ä¸ª`train_evaluate`å‡½æ•°ä½œä¸ºä½ é€‰æ‹©çš„é»‘ç›’ä¼˜åŒ–åº“çš„ç›®æ ‡ã€‚

æˆ‘å°†ä½¿ç”¨ [Scikit Optimize](https://web.archive.org/web/20221007001227/https://scikit-optimize.github.io/) ï¼Œæˆ‘å·²ç»åœ¨[çš„å¦ä¸€ç¯‡æ–‡ç« ](/web/20221007001227/https://neptune.ai/blog/scikit-optimize)ä¸­è¯¦ç»†æè¿°äº†å®ƒï¼Œä½†æ˜¯ä½ å¯ä»¥ä½¿ç”¨ä»»ä½•è¶…å‚æ•°ä¼˜åŒ–åº“ã€‚

### äº†è§£æ›´å¤šä¿¡æ¯

ðŸ’¡ä½¿ç”¨ [Scikit-Optimize + Neptune é›†æˆï¼Œæ£€æŸ¥å¦‚ä½•åœ¨è¿è¡Œæ—¶å¯è§†åŒ–è¿è¡Œï¼Œè®°å½•æ¯æ¬¡è¿è¡Œæ—¶å°è¯•çš„å‚æ•°ï¼Œç­‰ç­‰ã€‚](https://web.archive.org/web/20221007001227/https://docs.neptune.ai/integrations-and-supported-tools/hyperparameter-optimization/scikit-optimize)

ç®€å•åœ°è¯´ï¼Œæˆ‘

*   å®šä¹‰æœç´¢ ***ç©ºé—´*** ï¼Œ
*   åˆ›å»ºå°†è¢«æœ€å°åŒ–çš„`objective`å‡½æ•°ï¼Œ
*   é€šè¿‡`skopt.forest_minimize`åŠŸèƒ½è¿è¡Œä¼˜åŒ–ã€‚

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘å°†ä»Ž **10** éšæœºé€‰æ‹©çš„å‚æ•°é›†å¼€å§‹ï¼Œå°è¯• **100** ç§ä¸åŒçš„é…ç½®ã€‚

```py
import skopt

from script_step2 import train_evaluate

SPACE = [
    skopt.space.Real(0.01, 0.5, name='learning_rate', prior='log-uniform'),
    skopt.space.Integer(1, 30, name='max_depth'),
    skopt.space.Integer(2, 100, name='num_leaves'),
    skopt.space.Real(0.1, 1.0, name='feature_fraction', prior='uniform'),
    skopt.space.Real(0.1, 1.0, name='subsample', prior='uniform')]

@skopt.utils.use_named_args(SPACE)
def objective(**params):
    return -1.0 * train_evaluate(params)

results = skopt.forest_minimize(objective, SPACE, n_calls=30, n_random_starts=10)
best_auc = -1.0 * results.fun
best_params = results.x

print('best result: ', best_auc)
print('best parameters: ', best_params)
```

è¿™å°±æ˜¯äº†ã€‚

***ç»“æžœ*** å¯¹è±¡åŒ…å«å…³äºŽäº§ç”Ÿå®ƒçš„**æœ€ä½³åˆ†æ•°** **å’Œå‚æ•°**çš„ä¿¡æ¯ã€‚

* * *

è¯·æ³¨æ„ï¼Œç”±äºŽæœ€è¿‘çš„ [API æ›´æ–°](/web/20221007001227/https://neptune.ai/blog/neptune-new)ï¼Œè¿™ç¯‡æ–‡ç« ä¹Ÿéœ€è¦ä¸€äº›æ”¹å˜â€”â€”æˆ‘ä»¬æ­£åœ¨åŠªåŠ›ï¼ä¸Žæ­¤åŒæ—¶ï¼Œè¯·æ£€æŸ¥[æµ·çŽ‹æ˜Ÿæ–‡æ¡£](https://web.archive.org/web/20221007001227/https://docs.neptune.ai/)ï¼Œé‚£é‡Œçš„ä¸€åˆ‡éƒ½æ˜¯æœ€æ–°çš„ï¼ðŸ¥³

* * *

### æ³¨æ„:

å¦‚æžœæ‚¨æƒ³**å¯è§†åŒ–æ‚¨çš„è®­ç»ƒå¹¶åœ¨è®­ç»ƒç»“æŸåŽä¿å­˜è¯Šæ–­å›¾è¡¨**ï¼Œæ‚¨å¯ä»¥æ·»åŠ ä¸€ä¸ªå›žè°ƒå’Œä¸€ä¸ªå‡½æ•°è°ƒç”¨æ¥**å°†æ¯ä¸ªè¶…å‚æ•°æœç´¢**è®°å½•åˆ° Neptuneã€‚åªéœ€ä½¿ç”¨ neptune-contrib åº“ä¸­çš„è¿™ä¸ª[åŠ©æ‰‹å‡½æ•°ã€‚](https://web.archive.org/web/20221007001227/https://neptune-contrib.readthedocs.io/_modules/neptunecontrib/monitoring/skopt.html#NeptuneMonitor)

```py
import neptune
import neptunecontrib.monitoring.skopt as sk_utils
import skopt

from script_step2 import train_evaluate

neptune.init('jakub-czakon/blog-hpo')
neptune.create_experiment('hpo-on-any-script', upload_source_files=['*.py'])

SPACE = [
    skopt.space.Real(0.01, 0.5, name='learning_rate', prior='log-uniform'),
    skopt.space.Integer(1, 30, name='max_depth'),
    skopt.space.Integer(2, 100, name='num_leaves'),
    skopt.space.Real(0.1, 1.0, name='feature_fraction', prior='uniform'),
    skopt.space.Real(0.1, 1.0, name='subsample', prior='uniform')]

@skopt.utils.use_named_args(SPACE)
def objective(**params):
    return -1.0 * train_evaluate(params)

monitor = sk_utils.NeptuneMonitor()
results = skopt.forest_minimize(objective, SPACE, n_calls=100, n_random_starts=10, callback=[monitor])
sk_utils.log_results(results)

neptune.stop()
```

çŽ°åœ¨ï¼Œå½“æ‚¨è¿è¡Œå‚æ•°æ‰«ææ—¶ï¼Œæ‚¨å°†çœ‹åˆ°ä»¥ä¸‹å†…å®¹:

## æœ€åŽçš„æƒ³æ³•

åœ¨æœ¬æ–‡ä¸­ï¼Œæ‚¨å·²ç»å­¦ä¹ äº†å¦‚ä½•é€šè¿‡ 3 ä¸ªæ­¥éª¤ä¼˜åŒ–å‡ ä¹Žæ‰€æœ‰ Python è„šæœ¬çš„è¶…å‚æ•°ã€‚

å¸Œæœ›æœ‰äº†è¿™äº›çŸ¥è¯†ï¼Œä½ ä¼šç”¨æ›´å°‘çš„åŠªåŠ›å»ºç«‹æ›´å¥½çš„æœºå™¨å­¦ä¹ æ¨¡åž‹ã€‚

å¿«ä¹è®­ç»ƒï¼

### é›…å„å¸ƒÂ·æŸ¥å­”

å¤§éƒ¨åˆ†æ˜¯ ML çš„äººã€‚æž„å»º MLOps å·¥å…·ï¼Œç¼–å†™æŠ€æœ¯èµ„æ–™ï¼Œåœ¨ Neptune è¿›è¡Œæƒ³æ³•å®žéªŒã€‚

* * *

**é˜…è¯»ä¸‹ä¸€ç¯‡**

## å¦‚ä½•è·Ÿè¸ªæœºå™¨å­¦ä¹ æ¨¡åž‹çš„è¶…å‚æ•°ï¼Ÿ

å¡ç±³å°”Â·å¡ä»€é©¬é›·å…‹|å‘å¸ƒäºŽ 2020 å¹´ 7 æœˆ 1 æ—¥

**æœºå™¨å­¦ä¹ ç®—æ³•å¯é€šè¿‡ç§°ä¸ºè¶…å‚æ•°**çš„å¤šä¸ªé‡è§„è¿›è¡Œè°ƒæ•´ã€‚æœ€è¿‘çš„æ·±åº¦å­¦ä¹ æ¨¡åž‹å¯ä»¥é€šè¿‡æ•°åä¸ªè¶…å‚æ•°è¿›è¡Œè°ƒæ•´ï¼Œè¿™äº›è¶…å‚æ•°ä¸Žæ•°æ®æ‰©å……å‚æ•°å’Œè®­ç»ƒç¨‹åºå‚æ•°ä¸€èµ·åˆ›å»ºäº†éžå¸¸å¤æ‚çš„ç©ºé—´ã€‚åœ¨å¼ºåŒ–å­¦ä¹ é¢†åŸŸï¼Œæ‚¨è¿˜åº”è¯¥è®¡ç®—çŽ¯å¢ƒå‚æ•°ã€‚

æ•°æ®ç§‘å­¦å®¶è¦**æŽ§åˆ¶å¥½** **è¶…å‚æ•°** **ç©ºé—´**ï¼Œæ‰èƒ½**ä½¿** **è¿›æ­¥**ã€‚

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†å‘æ‚¨å±•ç¤º**æœ€è¿‘çš„** **å®žè·µ**ï¼Œ**æç¤º&æŠ€å·§ï¼Œ**å’Œ**å·¥å…·**ä»¥æœ€å°çš„å¼€é”€é«˜æ•ˆåœ°è·Ÿè¸ªè¶…å‚æ•°ã€‚ä½ ä¼šå‘çŽ°è‡ªå·±æŽŒæŽ§äº†æœ€å¤æ‚çš„æ·±åº¦å­¦ä¹ å®žéªŒï¼

## ä¸ºä»€ä¹ˆæˆ‘åº”è¯¥è·Ÿè¸ªæˆ‘çš„è¶…å‚æ•°ï¼Ÿä¹Ÿå°±æ˜¯ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦ï¼Ÿ

å‡ ä¹Žæ¯ä¸€ä¸ªæ·±åº¦å­¦ä¹ å®žéªŒæŒ‡å—ï¼Œåƒ[è¿™æœ¬æ·±åº¦å­¦ä¹ ä¹¦ç±](https://web.archive.org/web/20221007001227/https://www.deeplearningbook.org/contents/guidelines.html)ï¼Œéƒ½å»ºè®®ä½ å¦‚ä½•è°ƒæ•´è¶…å‚æ•°ï¼Œä½¿æ¨¡åž‹æŒ‰é¢„æœŸå·¥ä½œã€‚åœ¨**å®žéªŒ-åˆ†æž-å­¦ä¹ å¾ªçŽ¯**ä¸­ï¼Œæ•°æ®ç§‘å­¦å®¶å¿…é¡»æŽ§åˆ¶æ­£åœ¨è¿›è¡Œçš„æ›´æ”¹ï¼Œä»¥ä¾¿å¾ªçŽ¯çš„â€œå­¦ä¹ â€éƒ¨åˆ†æ­£å¸¸å·¥ä½œã€‚

å“¦ï¼Œå¿˜äº†è¯´**éšæœºç§å­ä¹Ÿæ˜¯ä¸€ä¸ªè¶…å‚æ•°**(ç‰¹åˆ«æ˜¯åœ¨ RL é¢†åŸŸ:ä¾‹å¦‚æ£€æŸ¥[è¿™ä¸ª Reddit](https://web.archive.org/web/20221007001227/https://www.reddit.com/r/MachineLearning/comments/76th74/d_why_random_seeds_sometimes_have_quite_large/) )ã€‚

## è¶…å‚æ•°è·Ÿè¸ªçš„å½“å‰å®žè·µæ˜¯ä»€ä¹ˆï¼Ÿ

è®©æˆ‘ä»¬é€ä¸€å›žé¡¾ä¸€ä¸‹ç®¡ç†è¶…å‚æ•°çš„å¸¸è§åšæ³•ã€‚æˆ‘ä»¬å…³æ³¨äºŽå¦‚ä½•æž„å»ºã€ä¿å­˜å’Œä¼ é€’è¶…å‚æ•°ç»™ä½ çš„ ML è„šæœ¬ã€‚

[Continue reading ->](/web/20221007001227/https://neptune.ai/blog/how-to-track-hyperparameters)

* * *