# ICLR 2020 å¤§ä¼šæœ€ä½³æ·±åº¦å­¦ä¹ è®ºæ–‡

> åŽŸæ–‡ï¼š<https://web.archive.org/web/https://neptune.ai/blog/iclr-2020-deep-learning>

ä¸Šå‘¨ï¼Œæˆ‘å¾ˆé«˜å…´å‚åŠ äº†å­¦ä¹ è¡¨å¾å›½é™…ä¼šè®®( **ICLR** )ï¼Œè¿™æ˜¯ä¸€ä¸ªè‡´åŠ›äºŽæ·±åº¦å­¦ä¹ å„ä¸ªæ–¹é¢çš„**ç ”ç©¶çš„æ´»åŠ¨**ã€‚æœ€åˆï¼Œä¼šè®®æœ¬åº”åœ¨åŸƒå¡žä¿„æ¯”äºšçš„äºšçš„æ–¯äºšè´å·´ä¸¾è¡Œï¼Œç„¶è€Œï¼Œç”±äºŽæ–°åž‹å† çŠ¶ç—…æ¯’ç–«æƒ…ï¼Œä¼šè®®è™šæ‹ŸåŒ–äº†ã€‚æˆ‘æ•¢è‚¯å®šï¼Œå¯¹äºŽç»„ç»‡è€…æ¥è¯´ï¼Œå°†æ´»åŠ¨æ¬åˆ°ç½‘ä¸Šæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œä½†æˆ‘è®¤ä¸ºæ•ˆæžœéžå¸¸ä»¤äººæ»¡æ„ï¼Œæ­£å¦‚ä½ å¯ä»¥[åœ¨è¿™é‡Œ](https://web.archive.org/web/20220926095407/https://medium.com/@iclr_conf/gone-virtual-lessons-from-iclr2020-1743ce6164a3)çœ‹åˆ°çš„ï¼

è¶…è¿‡ 1300 åå‘è¨€äººå’Œ 5600 åä¸Žä¼šè€…è¯æ˜Žï¼Œè™šæ‹Ÿå½¢å¼æ›´å®¹æ˜“ä¸ºå…¬ä¼—æ‰€æŽ¥å—ï¼Œä½†åŒæ—¶ï¼Œä¼šè®®ä»ä¿æŒäº’åŠ¨å’Œå‚ä¸Žã€‚ä»Žä¼—å¤šæœ‰è¶£çš„ä»‹ç»ä¸­ï¼Œæˆ‘å†³å®š**é€‰æ‹© 16 ä¸ª**ï¼Œæœ‰å½±å“åŠ›ï¼Œå‘äººæ·±çœã€‚ä»¥ä¸‹æ˜¯æ¥è‡ª ICLR çš„**æœ€ä½³æ·±åº¦å­¦ä¹ è®ºæ–‡ã€‚**

## æœ€ä½³æ·±åº¦å­¦ä¹ è®ºæ–‡

### **1ã€‚å…³äºŽç¥žç»å¸¸å¾®åˆ†æ–¹ç¨‹çš„é²æ£’æ€§**

æ·±å…¥ç ”ç©¶ç¥žç»å¸¸å¾®åˆ†æ–¹ç¨‹æˆ–ç®€ç§° NeuralODE çš„é²æ£’æ€§ã€‚å°†å…¶ä½œä¸ºæž„å»ºæ›´å¼ºå¤§ç½‘ç»œçš„åŸºç¡€ã€‚

[**è®ºæ–‡**](https://web.archive.org/web/20220926095407/https://openreview.net/forum?id=B1e9Y2NYvS)

![](img/880c69551405fee39e9de219ec13bca0.png)

*The architecture of an ODENet. The neural ODE block serves as a dimension-preserving nonlinear mapping.*

* * *

### **2ã€‚ä¸ºä»€ä¹ˆæ¢¯åº¦è£å‰ªåŠ é€Ÿè®­ç»ƒ:é€‚åº”æ€§çš„ç†è®ºè¯æ˜Ž**

æ¢¯åº¦è£å‰ªå¯è¯æ˜Žåœ°åŠ é€Ÿäº†éžå…‰æ»‘éžå‡¸å‡½æ•°çš„æ¢¯åº¦ä¸‹é™ã€‚

*(TLï¼›åšå£«ï¼Œæ¥è‡ª[OpenReview.net](https://web.archive.org/web/20220926095407/http://openreview.net/))*

**[è®ºæ–‡](https://web.archive.org/web/20220926095407/https://openreview.net/forum?id=BJgnXpVYwS) | [ä»£ç ](https://web.archive.org/web/20220926095407/https://github.com/JingzhaoZhang/why-clipping-accelerates)**

![](img/e7e51ccb5e2002b222c1ecdd21cf01d4.png)

*Gradient norm vs local gradient Lipschitz constant on a log-scale along the training trajectory for AWD-LSTM (Merity et al., 2018) on PTB dataset. The colorbar indicates the number of iterations during training.Â *

### ç¬¬ä¸€ä½œè€…:å¼ äº¬å…†

[LinkedIn](https://web.archive.org/web/20220926095407/https://www.linkedin.com/in/jingzhao-zhang-6b0a09a4/) | [ç½‘ç«™](https://web.archive.org/web/20220926095407/https://sites.google.com/view/jingzhao/home)

* * *

### **3ã€‚ç”¨äºŽç›‘ç£è¡¨ç¤ºå­¦ä¹ çš„ç›®æ ‡åµŒå…¥è‡ªåŠ¨ç¼–ç å™¨**

ç”¨äºŽç›‘ç£é¢„æµ‹çš„æ–°çš„é€šç”¨ç›®æ ‡åµŒå…¥è‡ªåŠ¨ç¼–ç å™¨æ¡†æž¶ã€‚ä½œè€…ç»™å‡ºäº†ç†è®ºå’Œç»éªŒçš„è€ƒè™‘ã€‚

[**è®ºæ–‡**](https://web.archive.org/web/20220926095407/https://openreview.net/forum?id=BygXFkSYDH)

![](img/7e47b1f2c8712c69b96820e2b19b7a1d.png)

*(a) Feature-embedding and (b) Target-embedding autoencoders. Solid lines correspond to the (primary) prediction task; dashed lines to the (auxiliary) reconstruction task. Shared components are involved in both.*

* * *

### **4ã€‚ç†è§£å’ŒåŠ å¼ºå·®å¼‚åŒ–æž¶æž„æœç´¢**

æˆ‘ä»¬é€šè¿‡æŸ¥çœ‹æœ‰æ•ˆæ€§æŸå¤±çš„ Hessian çš„ç‰¹å¾å€¼æ¥ç ”ç©¶ DARTS(å¯åŒºåˆ†æž¶æž„æœç´¢)çš„å¤±æ•ˆæ¨¡å¼ï¼Œå¹¶åŸºäºŽæˆ‘ä»¬çš„åˆ†æžæå‡ºç¨³å¥æ€§ã€‚

*(TLï¼›åšå£«ï¼Œæ¥è‡ª[OpenReview.net](https://web.archive.org/web/20220926095407/http://openreview.net/))*

**[è®ºæ–‡](https://web.archive.org/web/20220926095407/https://openreview.net/forum?id=H1gDNyrKDS) | [ä»£ç ](https://web.archive.org/web/20220926095407/https://github.com/automl/RobustDARTS)**

![](img/25f96fea20a13c9e64dad2ffb0fcd899.png)

*The poor cells standard DARTS finds on spaces S1-S4\. For all spaces, DARTS chooses mostly parameter-less operations (skip connection) or even the harmful Noise operation. Shown are the normal cells on CIFAR-10.*

* * *

### **5ã€‚æ¯”è¾ƒç¥žç»ç½‘ç»œä¿®å‰ªä¸­çš„å€’å›žå’Œå¾®è°ƒ**

åœ¨ä¿®å‰ªç¥žç»ç½‘ç»œæ—¶ï¼Œä¸æ˜¯åœ¨ä¿®å‰ªåŽè¿›è¡Œå¾®è°ƒï¼Œè€Œæ˜¯å°†æƒé‡æˆ–å­¦ä¹ çŽ‡è®¡åˆ’å€’å›žå®ƒä»¬åœ¨è®­ç»ƒæ—¶çš„å€¼ï¼Œå¹¶ä»Žé‚£é‡Œé‡æ–°è®­ç»ƒï¼Œä»¥å®žçŽ°æ›´é«˜çš„ç²¾åº¦ã€‚

*(TLï¼›åšå£«ï¼Œæ¥è‡ª[OpenReview.net](https://web.archive.org/web/20220926095407/http://openreview.net/))*

**[è®ºæ–‡](https://web.archive.org/web/20220926095407/https://openreview.net/forum?id=S1gSj0NKvB) | [ä»£ç ](https://web.archive.org/web/20220926095407/https://github.com/lottery-ticket/rewinding-iclr20-public)**

![](img/36d010599859bd171426b15353a0960d.png)

*The best achievable accuracy across retraining times by one-shot pruning.*

* * *

### **6ã€‚ç¥žç»è¿ç®—å•å…ƒ**

ç¥žç»ç½‘ç»œè™½ç„¶èƒ½å¤Ÿé€¼è¿‘å¤æ‚çš„å‡½æ•°ï¼Œä½†åœ¨ç²¾ç¡®çš„ç®—æœ¯è¿ç®—æ–¹é¢ç›¸å½“å·®ã€‚è¿™é¡¹ä»»åŠ¡å¯¹æ·±åº¦å­¦ä¹ ç ”ç©¶äººå‘˜æ¥è¯´æ˜¯ä¸€ä¸ªé•¿æœŸçš„æŒ‘æˆ˜ã€‚è¿™é‡Œï¼Œæå‡ºäº†æ–°é¢–çš„ç¥žç»åŠ æ³•å•å…ƒ(NAU)å’Œç¥žç»ä¹˜æ³•å•å…ƒ(NMU)ï¼Œèƒ½å¤Ÿæ‰§è¡Œç²¾ç¡®çš„åŠ æ³•/å‡æ³•(NAU)å’Œå‘é‡çš„ä¹˜æ³•å­é›†(MNU)ã€‚è‘—åçš„ç¬¬ä¸€ä½œè€…æ˜¯ä¸€åç‹¬ç«‹çš„ç ”ç©¶å‘˜ðŸ™‚

**[è®ºæ–‡](https://web.archive.org/web/20220926095407/https://openreview.net/forum?id=H1gNOeHKPS) | [ä»£ç ](https://web.archive.org/web/20220926095407/https://github.com/AndreasMadsen/stable-nalu)**

![](img/c39110e868e6c1153e97730eb2710c1d.png)

*Visualization of the NMU, where the weights (W[i,j] ) controls gating between 1 (identity) or x[i], each intermediate result is then multiplied explicitly to form z[j].*

* * *

### 7 .**ã€‚æ·±åº¦ç¥žç»ç½‘ç»œä¼˜åŒ–è½¨è¿¹ä¸Šçš„å¹³è¡¡ç‚¹**

åœ¨æ·±åº¦ç¥žç»ç½‘ç»œè®­ç»ƒçš„æ—©æœŸé˜¶æ®µï¼Œå­˜åœ¨ä¸€ä¸ªâ€œå¹³è¡¡ç‚¹â€,å®ƒå†³å®šäº†æ•´ä¸ªä¼˜åŒ–è½¨è¿¹çš„æ€§è´¨ã€‚

*(TLï¼›åšå£«ï¼Œæ¥è‡ª[OpenReview.net](https://web.archive.org/web/20220926095407/http://openreview.net/))*

[**è®ºæ–‡**](https://web.archive.org/web/20220926095407/https://openreview.net/forum?id=r1g87C4KwB)

![](img/e0e66af325070416f71b39dac9a08426.png)

*Visualization of the early part of the training trajectories on CIFAR-10 (before reaching 65% training accuracy) of a simple CNN model optimized using SGD with learning rates Î· = 0.01 (red) and Î· = 0.001 (blue). Each model on the training trajectory, shown as a point, is represented by its test predictions embedded into a two-dimensional space using UMAP. The background color indicates the spectral norm of the covariance of gradients K (Î»Â¹[K], left) and the training accuracy (right). For lower Î·, after reaching what we call the break-even point, the trajectory is steered towards a region characterized by larger Î»Â¹[K] (left) for the same training accuracy (right).*

* * *

### **8ã€‚Hoppity:å­¦ä¹ å›¾å½¢è½¬æ¢æ¥æ£€æµ‹å’Œä¿®å¤ç¨‹åºä¸­çš„é”™è¯¯**

ä¸€ç§åŸºäºŽå­¦ä¹ çš„æ–¹æ³•ï¼Œç”¨äºŽæ£€æµ‹å’Œä¿®å¤ Javascript ä¸­çš„é”™è¯¯ã€‚

*(TLï¼›åšå£«ï¼Œæ¥è‡ª[OpenReview.net](https://web.archive.org/web/20220926095407/http://openreview.net/))*

[**è®ºæ–‡**](https://web.archive.org/web/20220926095407/https://openreview.net/forum?id=SJeqs6EFvB)

![](img/33b444bbffb20f355cc6be61871ade38.png)

*Example programs that illustrate limitations of existing approaches inculding both rulebased static analyzers and neural-based bug predictors.*

* * *

### **9ã€‚é€šè¿‡ä»£ç†é€‰æ‹©:æ·±åº¦å­¦ä¹ çš„é«˜æ•ˆæ•°æ®é€‰æ‹©**

æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨å°å¾—å¤šçš„ä»£ç†æ¨¡åž‹æ¥æ‰§è¡Œæ•°æ®é€‰æ‹©ï¼Œä»Žè€Œæ˜¾è‘—æé«˜æ·±åº¦å­¦ä¹ ä¸­æ•°æ®é€‰æ‹©çš„è®¡ç®—æ•ˆçŽ‡ã€‚

*(TLï¼›åšå£«ï¼Œæ¥è‡ª[OpenReview.net](https://web.archive.org/web/20220926095407/http://openreview.net/))*

**[è®ºæ–‡](https://web.archive.org/web/20220926095407/https://openreview.net/forum?id=HJg2b0VYDr) | [ä»£ç ](https://web.archive.org/web/20220926095407/https://github.com/stanford-futuredata/selection-via-proxy)**

![](img/3bff50d92128a35f9a20c5dfe8f5dfee.png)

*SVP applied to active learning (left) and core-set selection (right). In active learning, we followed the same iterative procedure of training and selecting points to label as traditional approaches but replaced the target model with a cheaper-to-compute proxy model. For core-set selection, we learned a feature representation over the data using a proxy model and used it to select points to train a larger, more accurate model. In both cases, we found the proxy and target model have high rank-order correlation, leading to similar selections and downstream results.*

* * *

### 10ã€‚æ¯”ç‰¹ä¸‹é™:é‡æ–°å®¡è§†ç¥žç»ç½‘ç»œçš„é‡å­åŒ–

ä½¿ç”¨æ—¨åœ¨æ›´å¥½çš„åŸŸå†…é‡æž„çš„ç»“æž„åŒ–é‡åŒ–æŠ€æœ¯æ¥åŽ‹ç¼©å·ç§¯ç¥žç»ç½‘ç»œã€‚

*(TLï¼›åšå£«ï¼Œæ¥è‡ª[OpenReview.net](https://web.archive.org/web/20220926095407/http://openreview.net/))*

**[è®ºæ–‡](https://web.archive.org/web/20220926095407/https://openreview.net/forum?id=rJehVyrKwH) | [ä»£ç ](https://web.archive.org/web/20220926095407/https://drive.google.com/file/d/12QK7onizf2ArpEBK706ly8bNfiM9cPzp/view?usp=sharing)**

![](img/ca3eb3e43d6f1eec920492b70222f3ef.png)

*Illustration of our method. We approximate a binary classifier Ï• that labels images as dogs or cats by quantizing its weights. Standard method: quantizing Ï• with the standard objective function (1) promotes a classifier Ï•b[standard] that tries to approximate Ï• over the entire input space and can thus perform badly for in-domain inputs. Our method: quantizing Ï• with our objective function (2) promotes a classifier Ï•b[activations] that performs well for in-domain inputs. Images lying in the hatched area of the input space are correctly classified by Ï•[activations] but incorrectly by Ï•[standard].*

* * *

### **11ã€‚ç”¨äºŽåœ¨åˆå§‹åŒ–æ—¶ä¿®å‰ªç¥žç»ç½‘ç»œçš„ä¿¡å·ä¼ æ’­è§‚ç‚¹**

æˆ‘ä»¬åœ¨åˆå§‹åŒ–æ—¶å½¢å¼åŒ–åœ°æè¿°äº†æœ‰æ•ˆå‰ªæžçš„åˆå§‹åŒ–æ¡ä»¶ï¼Œå¹¶åˆ†æžäº†ç”±æ­¤äº§ç”Ÿçš„å‰ªæžç½‘ç»œçš„ä¿¡å·ä¼ æ’­ç‰¹æ€§ï¼Œè¿™å¯¼è‡´äº†ä¸€ç§å¢žå¼ºå®ƒä»¬çš„å¯è®­ç»ƒæ€§å’Œå‰ªæžç»“æžœçš„æ–¹æ³•ã€‚

*(TLï¼›åšå£«ï¼Œæ¥è‡ª[OpenReview.net](https://web.archive.org/web/20220926095407/http://openreview.net/))*

[**è®ºæ–‡**](https://web.archive.org/web/20220926095407/https://openreview.net/forum?id=HJeTo2VFwH)

![](img/6494a8538b66f0368ac1194df26e9675.png)

*(left) layerwise sparsity patterns c âˆˆ {0, 1} ^(100Ã—100) obtained as a result of pruning for the sparsity level ÎºÂ¯ = {10, .., 90}%. Here, black(0)/white(1) pixels refer to pruned/retained parameters; (right) connection sensitivities (CS) measured for the parameters in each layer. All networks are initialized with Î³ = 1.0\. Unlike the linear case, the sparsity pattern for the tanh network is nonuniform over different layers. When pruning for a high sparsity level (e.g., ÎºÂ¯ = 90%), this becomes critical and leads to poor learning capability as there are only a few parameters left in later layers. This is explained by the connection sensitivity plot which shows that for the nonlinear network parameters in later layers have saturating, lower connection sensitivities than those in earlier layers.*

* * *

### **12ã€‚æ·±åº¦åŠç›‘ç£å¼‚å¸¸æ£€æµ‹**

æˆ‘ä»¬ä»‹ç»äº†æ·±åº¦ SADï¼Œä¸€ç§ç”¨äºŽä¸€èˆ¬åŠç›‘ç£å¼‚å¸¸æ£€æµ‹çš„æ·±åº¦æ–¹æ³•ï¼Œå®ƒç‰¹åˆ«åˆ©ç”¨äº†æ ‡è®°å¼‚å¸¸ã€‚

*(TLï¼›åšå£«ï¼Œæ¥è‡ª[OpenReview.net](https://web.archive.org/web/20220926095407/http://openreview.net/))*

**[è®ºæ–‡](https://web.archive.org/web/20220926095407/https://openreview.net/forum?id=HkgH0TEYwH) | [ä»£ç ](https://web.archive.org/web/20220926095407/https://github.com/lukasruff/Deep-SAD-PyTorch)**

![](img/f8a04f019c4a8f5f318940ceb1ea257d.png)

*The need for semi-supervised anomaly detection: The training data (shown in (a)) consists of (mostly normal) unlabeled data (gray) as well as a few labeled normal samples (blue) and labeled anomalies (orange). Figures (b)â€“(f) show the decision boundaries of the various learning paradigms at testing time along with novel anomalies that occur (bottom left in each plot). Our semi-supervised AD approach takes advantage of all training data: unlabeled samples, labeled normal samples, as well as labeled anomalies. This strikes a balance between one-class learning and classification.*

* * *

### 13ã€‚ä½¿ç”¨ç½‘æ ¼å•å…ƒçš„ç©ºé—´ç‰¹å¾åˆ†å¸ƒçš„å¤šå°ºåº¦è¡¨ç¤ºå­¦ä¹ 

æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸º Space2vec çš„è¡¨å¾å­¦ä¹ æ¨¡åž‹æ¥ç¼–ç åœ°ç‚¹çš„ç»å¯¹ä½ç½®å’Œç©ºé—´å…³ç³»ã€‚

*(TLï¼›åšå£«ï¼Œæ¥è‡ª[OpenReview.net](https://web.archive.org/web/20220926095407/http://openreview.net/))*

**[è®ºæ–‡](https://web.archive.org/web/20220926095407/https://openreview.net/forum?id=rJljdh4KDH) | [ä»£ç ](https://web.archive.org/web/20220926095407/https://github.com/gengchenmai/space2vec)**

![](img/e4e7a2c2607f599329ff94c594187945.png)

*The challenge of joint modeling distributions with very different characteristics. (a)(b) The POI locations (red dots) in Las Vegas and Space2Vec predicted conditional likelihood of Womenâ€™s Clothing (with a clustered distribution) and Education (with an even distribution). The dark area in (b) indicates that the downtown area has more POIs of other types than education. (c) Ripleyâ€™s K curves of POI types for which Space2Vec has the largest and smallest improvement over wrap (Mac Aodha et al., 2019). Each curve represents the number of POIs of a certain type inside certain radios centered at every POI of that type; (d) Ripleyâ€™s K curves renormalized by POI densities and shown in log-scale. To efficiently achieve multi-scale representation Space2Vec concatenates the grid cell encoding of 64 scales (with wave lengths ranging from 50 meters to 40k meters) as the first layer of a deep model, and trains with POI data in an unsupervised fashion.*

* * *

### **14ã€‚åŒ¹é…å¹³å‡çš„è”åˆå­¦ä¹ **

å…·æœ‰é€å±‚åŒ¹é…çš„é€šä¿¡é«˜æ•ˆè”é‚¦å­¦ä¹ ã€‚

*(TLï¼›åšå£«ï¼Œæ¥è‡ª[OpenReview.net](https://web.archive.org/web/20220926095407/http://openreview.net/))*

**[è®ºæ–‡](https://web.archive.org/web/20220926095407/https://openreview.net/forum?id=BkluqlSFDS) | [ä»£ç ](https://web.archive.org/web/20220926095407/https://github.com/IBM/FedMA)**

![](img/ea5e434af0d509bb4af865e0dedab621.png)

*Comparison among various federated learning methods with limited number of communications on LeNet trained on MNIST; VGG-9 trained on CIFAR-10 dataset; LSTM trained on Shakespeare dataset over: (a) homogeneous data partition (b) heterogeneous data partition.Â *

* * *

### 15ã€‚å˜è‰²é¾™:åŠ é€Ÿæ·±åº¦ç¥žç»ç½‘ç»œç¼–è¯‘çš„è‡ªé€‚åº”ä»£ç ä¼˜åŒ–

æ·±åº¦ç¥žç»ç½‘ç»œä¼˜åŒ–ç¼–è¯‘çš„å¼ºåŒ–å­¦ä¹ å’Œè‡ªé€‚åº”é‡‡æ ·ã€‚

*(TLï¼›åšå£«ï¼Œæ¥è‡ª[OpenReview.net](https://web.archive.org/web/20220926095407/http://openreview.net/))*

[**è®ºæ–‡**](https://web.archive.org/web/20220926095407/https://openreview.net/forum?id=rygG4AVFvH)

![](img/bd2a688f16efb653a3335161579d9e17.png)

*Overview of our model compilation workflow, and highlighted is the scope of this work.*

* * *

### 16ã€‚ç½‘ç»œåŽ»å·ç§¯

ä¸ºäº†æ›´å¥½åœ°è®­ç»ƒå·ç§¯ç½‘ç»œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç±»ä¼¼äºŽåŠ¨ç‰©è§†è§‰ç³»ç»Ÿçš„ç½‘ç»œåŽ»å·ç§¯æ–¹æ³•ã€‚

*(TLï¼›åšå£«ï¼Œæ¥è‡ª*[](https://web.archive.org/web/20220926095407/http://openreview.net/)**)**

 ***[è®ºæ–‡](https://web.archive.org/web/20220926095407/https://openreview.net/forum?id=rkeu30EtvS) | [ä»£ç ](https://web.archive.org/web/20220926095407/https://github.com/yechengxi/deconvolution)**

![](img/83469e3d258dfd2f7f85df364551218f.png)

*Performing convolution on this real world image using a correlative filter, such as a Gaussian kernel, adds correlations to the resulting image, which makes object recognition more difficult. The process of removing this blur is called deconvolution. What if, however, what we saw as the real world image was itself the result of some unknown correlative filter, which has made recognition more difficult? Our proposed network deconvolution operation can decorrelate underlying image features which allows neural networks to perform better.*

* * *

# æ‘˜è¦

ICLR å‡ºç‰ˆç‰©çš„æ·±åº¦å’Œå¹¿åº¦ç›¸å½“é¼“èˆžäººå¿ƒã€‚åœ¨è¿™é‡Œï¼Œæˆ‘åªæ˜¯å±•ç¤ºäº†ä¸“æ³¨äºŽâ€œæ·±åº¦å­¦ä¹ â€ä¸»é¢˜çš„å†°å±±ä¸€è§’ã€‚ç„¶è€Œï¼Œ[è¿™ä¸€åˆ†æž](https://web.archive.org/web/20220926095407/https://www.analyticsvidhya.com/blog/2020/05/key-takeaways-iclr-2020/)è¡¨æ˜Žï¼Œå¾ˆå°‘æœ‰å—æ¬¢è¿Žçš„åœ°åŒºï¼Œç‰¹åˆ«æ˜¯:

1.  æ·±åº¦å­¦ä¹ (åœ¨è¿™ç¯‡æ–‡ç« ä¸­è®¨è®º)
2.  å¼ºåŒ–å­¦ä¹ ([æ­¤å¤„](/web/20220926095407/https://neptune.ai/blog/iclr-2020-reinforcement-learning))
3.  ç”Ÿæˆæ¨¡åž‹([æ­¤å¤„](/web/20220926095407/https://neptune.ai/blog/iclr-2020-generative-models))
4.  è‡ªç„¶è¯­è¨€å¤„ç†/ç†è§£([æ­¤å¤„](/web/20220926095407/https://neptune.ai/blog/iclr-2020-nlp-nlu)

ä¸ºäº†å¯¹ ICLR å¤§å­¦çš„é¡¶çº§è®ºæ–‡æœ‰ä¸€ä¸ªæ›´å®Œæ•´çš„æ¦‚è¿°ï¼Œæˆ‘ä»¬æ­£åœ¨å»ºç«‹ä¸€ç³»åˆ—çš„å¸–å­ï¼Œæ¯ä¸ªå¸–å­éƒ½ä¸“æ³¨äºŽä¸Šé¢æåˆ°çš„ä¸€ä¸ªä¸»é¢˜ã€‚ä½ å¯èƒ½æƒ³è¦**æŸ¥çœ‹**ä»¥èŽ·å¾—æ›´å®Œæ•´çš„æ¦‚è¿°ã€‚

å¿«ä¹é˜…è¯»ï¼

### å¡ç±³å°”Â·å¡ä»€é©¬é›·å…‹

äººå·¥æ™ºèƒ½ç ”ç©¶å€¡å¯¼è€…ï¼Œåœ¨ MLOps é¢†åŸŸå·¥ä½œã€‚æ€»æ˜¯åœ¨å¯»æ‰¾æ–°çš„ ML å·¥å…·ã€è¿‡ç¨‹è‡ªåŠ¨åŒ–æŠ€å·§å’Œæœ‰è¶£çš„ ML è®ºæ–‡ã€‚å¶å°”ä¼šæœ‰åšå®¢ä½œè€…å’Œä¼šè®®å‘è¨€äººã€‚

* * *

**é˜…è¯»ä¸‹ä¸€ç¯‡**

## å¦‚ä½•ç»„ç»‡æ·±åº¦å­¦ä¹ é¡¹ç›®â€”â€”æœ€ä½³å®žè·µèŒƒä¾‹

13 åˆ†é’Ÿé˜…è¯»|ä½œè€… Nilesh Barla |å¹´ 5 æœˆ 31 æ—¥æ›´æ–°

ä¸€ä¸ªæˆåŠŸçš„æ·±åº¦å­¦ä¹ é¡¹ç›®ï¼Œä½ éœ€è¦å¾ˆå¤šè¿­ä»£ï¼Œå¾ˆå¤šæ—¶é—´ï¼Œå¾ˆå¤šåŠªåŠ›ã€‚ä¸ºäº†è®©è¿™ä¸ªè¿‡ç¨‹ä¸é‚£ä¹ˆç—›è‹¦ï¼Œä½ åº”è¯¥å°½é‡åˆ©ç”¨ä½ çš„èµ„æºã€‚

ä¸€ä¸ªå¥½çš„å¾ªåºæ¸è¿›çš„å·¥ä½œæµç¨‹å°†å¸®åŠ©ä½ åšåˆ°è¿™ä¸€ç‚¹ã€‚æœ‰äº†å®ƒï¼Œä½ çš„é¡¹ç›®å˜å¾—**é«˜æ•ˆã€å¯å¤åˆ¶ã€**å’Œ**å¯ç†è§£**ã€‚

åœ¨æœ¬æ–‡ä¸­ï¼Œæ‚¨å°†çœ‹åˆ°å¦‚ä½•æž„å»ºæ·±åº¦å­¦ä¹ é¡¹ç›®çš„å·¥ä½œâ€”â€”ä»Žå¼€å§‹åˆ°éƒ¨ç½²ï¼Œç›‘æŽ§éƒ¨ç½²çš„æ¨¡åž‹ï¼Œä»¥åŠä¸­é—´çš„ä¸€åˆ‡ã€‚

åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Neptune æ¥è¿è¡Œã€ç›‘æŽ§å’Œåˆ†æžæ‚¨çš„å®žéªŒã€‚Neptune æ˜¯æé«˜ ML é¡¹ç›®ç”Ÿäº§çŽ‡çš„ä¸€ä¸ªå¾ˆé…·çš„å·¥å…·ã€‚

åœ¨æœ¬æ–‡ä¸­ï¼Œæ‚¨å°†äº†è§£åˆ°:

1.  å…³äºŽé¡¹ç›®çš„ç”Ÿå‘½å‘¨æœŸã€‚
2.  å®šä¹‰é¡¹ç›®ç›®æ ‡çš„é‡è¦æ€§ã€‚
3.  æ ¹æ®é¡¹ç›®éœ€æ±‚æ”¶é›†æ•°æ®ã€‚
4.  æ¨¡åž‹è®­ç»ƒå’Œç»“æžœæŽ¢ç´¢ï¼ŒåŒ…æ‹¬:
    1.  ä¸ºæ›´å¥½çš„ç»“æžœå»ºç«‹åŸºçº¿ã€‚
    2.  é‡‡ç”¨çŽ°æœ‰çš„å¼€æºæœ€æ–°æ¨¡åž‹ç ”ç©¶è®ºæ–‡å’Œä»£ç åº“ä¸­çš„æŠ€æœ¯å’Œæ–¹æ³•ã€‚
    3.  å®žéªŒè·Ÿè¸ªå’Œç®¡ç†
5.  é¿å…æ¬ æ‹Ÿåˆå’Œè¿‡æ‹Ÿåˆçš„æ¨¡åž‹ä¼˜åŒ–æŠ€æœ¯ï¼Œä¾‹å¦‚:
    1.  æŽ§åˆ¶è¶…å‚æ•°
    2.  è§„èŒƒåŒ–
    3.  ä¿®å‰ª
6.  åœ¨éƒ¨ç½²ä¹‹å‰æµ‹è¯•å’Œè¯„ä¼°æ‚¨çš„é¡¹ç›®ã€‚
7.  æ¨¡åž‹éƒ¨ç½²
8.  é¡¹ç›®ç»´æŠ¤

[Continue reading ->](/web/20220926095407/https://neptune.ai/blog/how-to-organize-deep-learning-projects-best-practices)

* * **