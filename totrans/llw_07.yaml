- en: Some Math behind Neural Tangent Kernel
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸€äº›ç¥ç»åˆ‡å‘æ ¸èƒŒåçš„æ•°å­¦çŸ¥è¯†
- en: åŸæ–‡ï¼š[https://lilianweng.github.io/posts/2022-09-08-ntk/](https://lilianweng.github.io/posts/2022-09-08-ntk/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://lilianweng.github.io/posts/2022-09-08-ntk/](https://lilianweng.github.io/posts/2022-09-08-ntk/)
- en: Neural networks are [well known](https://lilianweng.github.io/posts/2019-03-14-overfit/)
    to be over-parameterized and can often easily fit data with near-zero training
    loss with decent generalization performance on test dataset. Although all these
    parameters are initialized at random, the optimization process can consistently
    lead to similarly good outcomes. And this is true even when the number of model
    parameters exceeds the number of training data points.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼—æ‰€å‘¨çŸ¥ï¼Œç¥ç»ç½‘ç»œæ˜¯è¿‡åº¦å‚æ•°åŒ–çš„ï¼Œé€šå¸¸å¯ä»¥è½»æ¾æ‹Ÿåˆå…·æœ‰æ¥è¿‘é›¶è®­ç»ƒæŸå¤±çš„æ•°æ®ï¼Œå¹¶åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šå…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚å°½ç®¡æ‰€æœ‰è¿™äº›å‚æ•°éƒ½æ˜¯éšæœºåˆå§‹åŒ–çš„ï¼Œä½†ä¼˜åŒ–è¿‡ç¨‹å¯ä»¥å§‹ç»ˆå¯¼è‡´ç±»ä¼¼çš„è‰¯å¥½ç»“æœã€‚å³ä½¿æ¨¡å‹å‚æ•°çš„æ•°é‡è¶…è¿‡è®­ç»ƒæ•°æ®ç‚¹çš„æ•°é‡ï¼Œè¿™ä¹Ÿæ˜¯æ­£ç¡®çš„ã€‚
- en: '**Neural tangent kernel (NTK)** ([Jacot et al. 2018](https://arxiv.org/abs/1806.07572))
    is a kernel to explain the evolution of neural networks during training via gradient
    descent. It leads to great insights into why neural networks with enough width
    can consistently converge to a global minimum when trained to minimize an empirical
    loss. In the post, we will do a deep dive into the motivation and definition of
    NTK, as well as the proof of a deterministic convergence at different initializations
    of neural networks with infinite width by characterizing NTK in such a setting.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¥ç»åˆ‡å‘æ ¸ï¼ˆNTKï¼‰**ï¼ˆ[Jacot et al. 2018](https://arxiv.org/abs/1806.07572)ï¼‰æ˜¯ä¸€ä¸ªæ ¸ï¼Œç”¨äºé€šè¿‡æ¢¯åº¦ä¸‹é™è§£é‡Šç¥ç»ç½‘ç»œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¼”å˜ã€‚å®ƒæ·±å…¥æ¢è®¨äº†ä¸ºä»€ä¹ˆå…·æœ‰è¶³å¤Ÿå®½åº¦çš„ç¥ç»ç½‘ç»œåœ¨è¢«è®­ç»ƒä»¥æœ€å°åŒ–ç»éªŒæŸå¤±æ—¶å¯ä»¥å§‹ç»ˆæ”¶æ•›åˆ°å…¨å±€æœ€å°å€¼ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨NTKçš„åŠ¨æœºå’Œå®šä¹‰ï¼Œä»¥åŠåœ¨ä¸åŒåˆå§‹åŒ–æ¡ä»¶ä¸‹å¯¹å…·æœ‰æ— é™å®½åº¦çš„ç¥ç»ç½‘ç»œçš„ç¡®å®šæ€§æ”¶æ•›çš„è¯æ˜ï¼Œé€šè¿‡åœ¨è¿™ç§è®¾ç½®ä¸­å¯¹NTKè¿›è¡Œè¡¨å¾ã€‚'
- en: ğŸ¤“ Different from my previous posts, this one mainly focuses on a small number
    of core papers, less on the breadth of the literature review in the field. There
    are many interesting works after NTK, with modification or expansion of the theory
    for understanding the learning dynamics of NNs, but they wonâ€™t be covered here.
    The goal is to show all the math behind NTK in a clear and easy-to-follow format,
    so the post is quite math-intensive. If you notice any mistakes, please let me
    know and I will be happy to correct them quickly. Thanks in advance!
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ğŸ¤“ ä¸æˆ‘ä¹‹å‰çš„æ–‡ç« ä¸åŒï¼Œè¿™ç¯‡ä¸»è¦å…³æ³¨å°‘é‡æ ¸å¿ƒè®ºæ–‡ï¼Œè€Œä¸æ˜¯å¹¿æ³›æ¶µç›–è¯¥é¢†åŸŸçš„æ–‡çŒ®ç»¼è¿°ã€‚NTKä¹‹åæœ‰è®¸å¤šæœ‰è¶£çš„å·¥ä½œï¼Œå¯¹ç†è§£ç¥ç»ç½‘ç»œå­¦ä¹ åŠ¨æ€è¿›è¡Œäº†ä¿®æ”¹æˆ–æ‰©å±•ï¼Œä½†å®ƒä»¬ä¸ä¼šåœ¨è¿™é‡Œæ¶µç›–ã€‚ç›®æ ‡æ˜¯ä»¥æ¸…æ™°æ˜“æ‡‚çš„æ ¼å¼å±•ç¤ºNTKèƒŒåçš„æ‰€æœ‰æ•°å­¦çŸ¥è¯†ï¼Œå› æ­¤æœ¬æ–‡å…·æœ‰ç›¸å½“é«˜çš„æ•°å­¦å¯†åº¦ã€‚å¦‚æœæ‚¨å‘ç°ä»»ä½•é”™è¯¯ï¼Œè¯·å‘Šè¯‰æˆ‘ï¼Œæˆ‘å°†å¾ˆä¹æ„å¿«é€Ÿæ›´æ­£ã€‚æå‰æ„Ÿè°¢ï¼
- en: Basics
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŸºç¡€çŸ¥è¯†
- en: This section contains reviews of several very basic concepts which are core
    to understanding of neural tangent kernel. Feel free to skip.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚åŒ…å«å¯¹å‡ ä¸ªéå¸¸åŸºæœ¬æ¦‚å¿µçš„å›é¡¾ï¼Œè¿™äº›æ¦‚å¿µæ˜¯ç†è§£ç¥ç»åˆ‡å‘æ ¸çš„æ ¸å¿ƒã€‚éšæ„è·³è¿‡ã€‚
- en: Vector-to-vector Derivative
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‘é‡å¯¹å‘é‡çš„å¯¼æ•°
- en: 'Given an input vector $\mathbf{x} \in \mathbb{R}^n$ (as a column vector) and
    a function $f: \mathbb{R}^n \to \mathbb{R}^m$, the derivative of $f$ with respective
    to $\mathbf{x}$ is a $m\times n$ matrix, also known as [*Jacobian matrix*](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant):'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç»™å®šè¾“å…¥å‘é‡ $\mathbf{x} \in \mathbb{R}^n$ï¼ˆä½œä¸ºåˆ—å‘é‡ï¼‰å’Œå‡½æ•° $f: \mathbb{R}^n \to \mathbb{R}^m$ï¼Œå…³äº
    $\mathbf{x}$ çš„å¯¼æ•°æ˜¯ä¸€ä¸ª $m\times n$ çŸ©é˜µï¼Œä¹Ÿç§°ä¸º[*é›…å¯æ¯”çŸ©é˜µ*](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant)ï¼š'
- en: $$ J = \frac{\partial f}{\partial \mathbf{x}} = \begin{bmatrix} \frac{\partial
    f_1}{\partial x_1} & \dots &\frac{\partial f_1}{\partial x_n} \\ \vdots & & \\
    \frac{\partial f_m}{\partial x_1} & \dots &\frac{\partial f_m}{\partial x_n} \\
    \end{bmatrix} \in \mathbb{R}^{m \times n} $$
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: $$ J = \frac{\partial f}{\partial \mathbf{x}} = \begin{bmatrix} \frac{\partial
    f_1}{\partial x_1} & \dots &\frac{\partial f_1}{\partial x_n} \\ \vdots & & \\
    \frac{\partial f_m}{\partial x_1} & \dots &\frac{\partial f_m}{\partial x_n} \\
    \end{bmatrix} \in \mathbb{R}^{m \times n} $$
- en: Throughout the post, I use integer subscript(s) to refer to a single entry out
    of a vector or matrix value; i.e. $x_i$ indicates the $i$-th value in the vector
    $\mathbf{x}$ and $f_i(.)$ is the $i$-th entry in the output of the function.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä½¿ç”¨æ•´æ•°ä¸‹æ ‡æ¥æŒ‡ä»£å‘é‡æˆ–çŸ©é˜µå€¼ä¸­çš„å•ä¸ªæ¡ç›®ï¼›å³ $x_i$ è¡¨ç¤ºå‘é‡ $\mathbf{x}$ ä¸­çš„ç¬¬ $i$ ä¸ªå€¼ï¼Œ$f_i(.)$ æ˜¯å‡½æ•°è¾“å‡ºä¸­çš„ç¬¬
    $i$ ä¸ªæ¡ç›®ã€‚
- en: The gradient of a vector with respect to a vector is defined as $\nabla_\mathbf{x}
    f = J^\top \in \mathbb{R}^{n \times m}$ and this formation is also valid when
    $m=1$ (i.e., scalar output).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå‘é‡å…³äºå‘é‡çš„æ¢¯åº¦å®šä¹‰ä¸º $\nabla_\mathbf{x} f = J^\top \in \mathbb{R}^{n \times m}$ï¼Œå½“
    $m=1$ï¼ˆå³ï¼Œæ ‡é‡è¾“å‡ºï¼‰æ—¶ï¼Œè¿™ç§å½¢å¼ä¹Ÿæ˜¯æœ‰æ•ˆçš„ã€‚
- en: Differential Equations
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¾®åˆ†æ–¹ç¨‹
- en: Differential equations describe the relationship between one or multiple functions
    and their derivatives. There are two main types of differential equations.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å¾®åˆ†æ–¹ç¨‹æè¿°ä¸€ä¸ªæˆ–å¤šä¸ªå‡½æ•°åŠå…¶å¯¼æ•°ä¹‹é—´çš„å…³ç³»ã€‚æœ‰ä¸¤ç§ä¸»è¦ç±»å‹çš„å¾®åˆ†æ–¹ç¨‹ã€‚
- en: (1) *ODE (Ordinary differential equation)* contains only an unknown function
    of one random variable. ODEs are the main form of differential equations used
    in this post. A general form of ODE looks like $(x, y, \frac{dy}{dx}, \dots, \frac{d^ny}{dx^n})
    = 0$.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1) *ODEï¼ˆå¸¸å¾®åˆ†æ–¹ç¨‹ï¼‰*åªåŒ…å«ä¸€ä¸ªæœªçŸ¥å‡½æ•°çš„ä¸€ä¸ªéšæœºå˜é‡ã€‚ODEsæ˜¯æœ¬æ–‡ä¸­ä½¿ç”¨çš„å¾®åˆ†æ–¹ç¨‹çš„ä¸»è¦å½¢å¼ã€‚ODEçš„ä¸€èˆ¬å½¢å¼å¦‚$(x, y, \frac{dy}{dx},
    \dots, \frac{d^ny}{dx^n}) = 0$ã€‚
- en: (2) *PDE (Partial differential equation)* contains unknown multivariable functions
    and their partial derivatives.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2) *PDEï¼ˆåå¾®åˆ†æ–¹ç¨‹ï¼‰*åŒ…å«æœªçŸ¥çš„å¤šå˜é‡å‡½æ•°åŠå…¶åå¯¼æ•°ã€‚
- en: Letâ€™s review the simplest case of differential equations and its solution. *Separation
    of variables* (Fourier method) can be used when all the terms containing one variable
    can be moved to one side, while the other terms are all moved to the other side.
    For example,
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹å¾®åˆ†æ–¹ç¨‹åŠå…¶è§£çš„æœ€ç®€å•æƒ…å†µã€‚*å˜é‡åˆ†ç¦»*ï¼ˆå‚…ç«‹å¶æ–¹æ³•ï¼‰å¯ç”¨äºå½“æ‰€æœ‰åŒ…å«ä¸€ä¸ªå˜é‡çš„é¡¹éƒ½ç§»åˆ°ä¸€è¾¹æ—¶ï¼Œè€Œå…¶ä»–é¡¹éƒ½ç§»åˆ°å¦ä¸€è¾¹ã€‚ä¾‹å¦‚ï¼Œ
- en: $$ \begin{aligned} \text{Given }a\text{ is a constant scalar:}\quad\frac{dy}{dx}
    &= ay \\ \text{Move same variables to the same side:}\quad\frac{dy}{y} &= adx
    \\ \text{Put integral on both sides:}\quad\int \frac{dy}{y} &= \int adx \\ \ln
    (y) &= ax + C' \\ \text{Finally}\quad y &= e^{ax + C'} = C e^{ax} \end{aligned}
    $$
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \text{ç»™å®š}a\text{æ˜¯ä¸€ä¸ªå¸¸æ•°æ ‡é‡ï¼š}\quad\frac{dy}{dx} &= ay \\ \text{å°†ç›¸åŒå˜é‡ç§»åˆ°åŒä¸€ä¾§ï¼š}\quad\frac{dy}{y}
    &= adx \\ \text{ä¸¤ä¾§åŠ ä¸Šç§¯åˆ†ï¼š}\quad\int \frac{dy}{y} &= \int adx \\ \ln (y) &= ax +
    C' \\ \text{æœ€ç»ˆ}\quad y &= e^{ax + C'} = C e^{ax} \end{aligned} $$
- en: Central Limit Theorem
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸­å¿ƒæé™å®šç†
- en: Given a collection of i.i.d. random variables, $x_1, \dots, x_N$ with mean $\mu$
    and variance $\sigma^2$, the *Central Limit Theorem (CTL)* states that the expectation
    would be Gaussian distributed when $N$ becomes really large.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šä¸€ç»„ç‹¬ç«‹åŒåˆ†å¸ƒçš„éšæœºå˜é‡ï¼Œ$x_1, \dots, x_N$ï¼Œå‡å€¼ä¸º$\mu$ï¼Œæ–¹å·®ä¸º$\sigma^2$ï¼Œ*ä¸­å¿ƒæé™å®šç†ï¼ˆCTLï¼‰*è¡¨æ˜å½“$N$å˜å¾—éå¸¸å¤§æ—¶ï¼ŒæœŸæœ›å€¼å°†å‘ˆé«˜æ–¯åˆ†å¸ƒã€‚
- en: $$ \bar{x} = \frac{1}{N}\sum_{i=1}^N x_i \sim \mathcal{N}(\mu, \frac{\sigma^2}{n})\quad\text{when
    }N \to \infty $$
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \bar{x} = \frac{1}{N}\sum_{i=1}^N x_i \sim \mathcal{N}(\mu, \frac{\sigma^2}{n})\quad\text{å½“}N
    \to \infty $$
- en: CTL can also apply to multidimensional vectors, and then instead of a single
    scale $\sigma^2$ we need to compute the covariance matrix of random variable $\Sigma$.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: CTLä¹Ÿå¯ä»¥åº”ç”¨äºå¤šç»´å‘é‡ï¼Œç„¶åæˆ‘ä»¬éœ€è¦è®¡ç®—éšæœºå˜é‡$\Sigma$çš„åæ–¹å·®çŸ©é˜µï¼Œè€Œä¸æ˜¯å•ä¸€å°ºåº¦$\sigma^2$ã€‚
- en: Taylor Expansion
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ³°å‹’å±•å¼€
- en: 'The [*Taylor expansion*](https://en.wikipedia.org/wiki/Taylor_series) is to
    express a function as an infinite sum of components, each represented in terms
    of this functionâ€™s derivatives. The Tayler expansion of a function $f(x)$ at $x=a$
    can be written as: $$ f(x) = f(a) + \sum_{k=1}^\infty \frac{1}{k!} (x - a)^k\nabla^k_xf(x)\vert_{x=a}
    $$ where $\nabla^k$ denotes the $k$-th derivative.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[*æ³°å‹’å±•å¼€*](https://en.wikipedia.org/wiki/Taylor_series)æ˜¯å°†ä¸€ä¸ªå‡½æ•°è¡¨ç¤ºä¸ºæ— é™é¡¹çš„ç»„æˆéƒ¨åˆ†ä¹‹å’Œï¼Œæ¯ä¸ªéƒ¨åˆ†éƒ½ç”¨è¯¥å‡½æ•°çš„å¯¼æ•°è¡¨ç¤ºã€‚å‡½æ•°$f(x)$åœ¨$x=a$å¤„çš„æ³°å‹’å±•å¼€å¯ä»¥å†™æˆï¼š$$
    f(x) = f(a) + \sum_{k=1}^\infty \frac{1}{k!} (x - a)^k\nabla^k_xf(x)\vert_{x=a}
    $$å…¶ä¸­$\nabla^k$è¡¨ç¤ºç¬¬$k$é˜¶å¯¼æ•°ã€‚'
- en: 'The first-order Taylor expansion is often used as a linear approximation of
    the function value:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€é˜¶æ³°å‹’å±•å¼€é€šå¸¸ç”¨ä½œå‡½æ•°å€¼çš„çº¿æ€§è¿‘ä¼¼ï¼š
- en: $$ f(x) \approx f(a) + (x - a)\nabla_x f(x)\vert_{x=a} $$
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: $$ f(x) \approx f(a) + (x - a)\nabla_x f(x)\vert_{x=a} $$
- en: Kernel & Kernel Methods
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ ¸å‡½æ•°å’Œæ ¸æ–¹æ³•
- en: 'A [*kernel*](https://en.wikipedia.org/wiki/Kernel_method) is essentially a
    similarity function between two data points, $K: \mathcal{X} \times \mathcal{X}
    \to \mathbb{R}$. It describes how sensitive the prediction for one data sample
    is to the prediction for the other; or in other words, how similar two data points
    are. The kernel should be symmetric, $K(x, xâ€™) = K(xâ€™, x)$.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¸€ä¸ª[*æ ¸å‡½æ•°*](https://en.wikipedia.org/wiki/Kernel_method)æœ¬è´¨ä¸Šæ˜¯ä¸¤ä¸ªæ•°æ®ç‚¹ä¹‹é—´çš„ç›¸ä¼¼æ€§å‡½æ•°ï¼Œ$K:
    \mathcal{X} \times \mathcal{X} \to \mathbb{R$ã€‚å®ƒæè¿°äº†å¯¹ä¸€ä¸ªæ•°æ®æ ·æœ¬çš„é¢„æµ‹å¯¹å¦ä¸€ä¸ªæ•°æ®æ ·æœ¬çš„é¢„æµ‹çš„æ•æ„Ÿç¨‹åº¦ï¼›æˆ–è€…æ¢å¥è¯è¯´ï¼Œä¸¤ä¸ªæ•°æ®ç‚¹æœ‰å¤šç›¸ä¼¼ã€‚æ ¸å‡½æ•°åº”è¯¥æ˜¯å¯¹ç§°çš„ï¼Œ$K(x,
    xâ€™) = K(xâ€™, x)$ã€‚'
- en: 'Depending on the problem structure, some kernels can be decomposed into two
    feature maps, one corresponding to one data point, and the kernel value is an
    inner product of these two features: $K(x, xâ€™) = \langle \varphi(x), \varphi(xâ€™)
    \rangle$.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®é—®é¢˜ç»“æ„ï¼Œä¸€äº›æ ¸å‡½æ•°å¯ä»¥åˆ†è§£ä¸ºä¸¤ä¸ªç‰¹å¾æ˜ å°„ï¼Œä¸€ä¸ªå¯¹åº”ä¸€ä¸ªæ•°æ®ç‚¹ï¼Œæ ¸å€¼æ˜¯è¿™ä¸¤ä¸ªç‰¹å¾çš„å†…ç§¯ï¼š$K(x, xâ€™) = \langle \varphi(x),
    \varphi(xâ€™) \rangle$ã€‚
- en: '*Kernel methods* are a type of non-parametric, instance-based machine learning
    algorithms. Assuming we have known all the labels of training samples $\{x^{(i)},
    y^{(i)}\}$, the label for a new input $x$ is predicted by a weighted sum $\sum_{i}
    K(x^{(i)}, x)y^{(i)}$.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ ¸æ–¹æ³•*æ˜¯ä¸€ç§éå‚æ•°ã€åŸºäºå®ä¾‹çš„æœºå™¨å­¦ä¹ ç®—æ³•ã€‚å‡è®¾æˆ‘ä»¬å·²çŸ¥æ‰€æœ‰è®­ç»ƒæ ·æœ¬$\{x^{(i)}, y^{(i)}\}$çš„æ ‡ç­¾ï¼Œé‚£ä¹ˆæ–°è¾“å…¥$x$çš„æ ‡ç­¾é€šè¿‡åŠ æƒå’Œ$\sum_{i}
    K(x^{(i)}, x)y^{(i)}$æ¥é¢„æµ‹ã€‚'
- en: Gaussian Processes
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é«˜æ–¯è¿‡ç¨‹
- en: '*Gaussian process (GP)* is a non-parametric method by modeling a multivariate
    Gaussian probability distribution over a collection of random variables. GP assumes
    a prior over functions and then updates the posterior over functions based on
    what data points are observed.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*é«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰*æ˜¯ä¸€ç§é€šè¿‡å¯¹ä¸€ç»„éšæœºå˜é‡å»ºæ¨¡å¤šå…ƒé«˜æ–¯æ¦‚ç‡åˆ†å¸ƒçš„éå‚æ•°æ–¹æ³•ã€‚GPå‡è®¾å‡½æ•°çš„å…ˆéªŒï¼Œç„¶åæ ¹æ®è§‚å¯Ÿåˆ°çš„æ•°æ®ç‚¹æ›´æ–°å‡½æ•°çš„åéªŒã€‚'
- en: Given a collection of data points $\{x^{(1)}, \dots, x^{(N)}\}$, GP assumes
    that they follow a jointly multivariate Gaussian distribution, defined by a mean
    $\mu(x)$ and a covariance matrix $\Sigma(x)$. Each entry at location $(i,j)$ in
    the covariance matrix $\Sigma(x)$ is defined by a kernel $\Sigma_{i,j} = K(x^{(i)},
    x^{(j)})$, also known as a *covariance function*. The core idea is â€“ if two data
    points are deemed similar by the kernel, the function outputs should be close,
    too. Making predictions with GP for unknown data points is equivalent to drawing
    samples from this distribution, via a conditional distribution of unknown data
    points given observed ones.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šæ•°æ®ç‚¹é›†åˆ$\{x^{(1)}, \dots, x^{(N)}\}$ï¼Œé«˜æ–¯è¿‡ç¨‹å‡è®¾å®ƒä»¬éµå¾ªä¸€ä¸ªè”åˆå¤šå…ƒé«˜æ–¯åˆ†å¸ƒï¼Œç”±å‡å€¼$\mu(x)$å’Œåæ–¹å·®çŸ©é˜µ$\Sigma(x)$å®šä¹‰ã€‚åæ–¹å·®çŸ©é˜µ$\Sigma(x)$ä¸­ä½ç½®$(i,j)$å¤„çš„æ¯ä¸ªæ¡ç›®ç”±ä¸€ä¸ªæ ¸$\Sigma_{i,j}
    = K(x^{(i)}, x^{(j)})$å®šä¹‰ï¼Œä¹Ÿç§°ä¸º*åæ–¹å·®å‡½æ•°*ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯ - å¦‚æœä¸¤ä¸ªæ•°æ®ç‚¹è¢«æ ¸è§†ä¸ºç›¸ä¼¼ï¼Œé‚£ä¹ˆå‡½æ•°è¾“å‡ºä¹Ÿåº”è¯¥æ¥è¿‘ã€‚ä½¿ç”¨é«˜æ–¯è¿‡ç¨‹å¯¹æœªçŸ¥æ•°æ®ç‚¹è¿›è¡Œé¢„æµ‹ç­‰åŒäºä»è¯¥åˆ†å¸ƒä¸­æŠ½å–æ ·æœ¬ï¼Œé€šè¿‡ç»™å®šè§‚å¯Ÿåˆ°çš„æ•°æ®ç‚¹çš„æœªçŸ¥æ•°æ®ç‚¹çš„æ¡ä»¶åˆ†å¸ƒã€‚
- en: Check [this post](https://distill.pub/2019/visual-exploration-gaussian-processes/)
    for a high-quality and highly visualization tutorial on what Gaussian Processes
    are.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹[è¿™ç¯‡æ–‡ç« ](https://distill.pub/2019/visual-exploration-gaussian-processes/)ï¼Œäº†è§£é«˜è´¨é‡ä¸”é«˜åº¦å¯è§†åŒ–çš„å…³äºé«˜æ–¯è¿‡ç¨‹çš„æ•™ç¨‹ã€‚
- en: Notation
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¦å·
- en: 'Let us consider a fully-connected neural networks with parameter $\theta$,
    $f(.;\theta): \mathbb{R}^{n_0} \to \mathbb{R}^{n_L}$. Layers are indexed from
    0 (input) to $L$ (output), each containing $n_0, \dots, n_L$ neurons, including
    the input of size $n_0$ and the output of size $n_L$. There are $P = \sum_{l=0}^{L-1}
    (n_l + 1) n_{l+1}$ parameters in total and thus we have $\theta \in \mathbb{R}^P$.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 'è®©æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªå…·æœ‰å‚æ•°$\theta$çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼Œ$f(.;\theta): \mathbb{R}^{n_0} \to \mathbb{R}^{n_L}$ã€‚å±‚ä»0ï¼ˆè¾“å…¥ï¼‰åˆ°$L$ï¼ˆè¾“å‡ºï¼‰è¿›è¡Œç´¢å¼•ï¼Œæ¯ä¸€å±‚åŒ…å«$n_0,
    \dots, n_L$ä¸ªç¥ç»å…ƒï¼ŒåŒ…æ‹¬å¤§å°ä¸º$n_0$çš„è¾“å…¥å’Œå¤§å°ä¸º$n_L$çš„è¾“å‡ºã€‚æ€»å…±æœ‰$P = \sum_{l=0}^{L-1} (n_l + 1) n_{l+1}$ä¸ªå‚æ•°ï¼Œå› æ­¤æˆ‘ä»¬æœ‰$\theta
    \in \mathbb{R}^P$ã€‚'
- en: The training dataset contains $N$ data points, $\mathcal{D}=\{\mathbf{x}^{(i)},
    y^{(i)}\}_{i=1}^N$. All the inputs are denoted as $\mathcal{X}=\{\mathbf{x}^{(i)}\}_{i=1}^N$
    and all the labels are denoted as $\mathcal{Y}=\{y^{(i)}\}_{i=1}^N$.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ•°æ®é›†åŒ…å«$N$ä¸ªæ•°æ®ç‚¹ï¼Œ$\mathcal{D}=\{\mathbf{x}^{(i)}, y^{(i)}\}_{i=1}^N$ã€‚æ‰€æœ‰è¾“å…¥è¢«è¡¨ç¤ºä¸º$\mathcal{X}=\{\mathbf{x}^{(i)}\}_{i=1}^N$ï¼Œæ‰€æœ‰æ ‡ç­¾è¢«è¡¨ç¤ºä¸º$\mathcal{Y}=\{y^{(i)}\}_{i=1}^N$ã€‚
- en: Now letâ€™s look into the forward pass computation in every layer in detail. For
    $l=0, \dots, L-1$, each layer $l$ defines an affine transformation $A^{(l)}$ with
    a weight matrix $\mathbf{w}^{(l)} \in \mathbb{R}^{n_{l} \times n_{l+1}}$ and a
    bias term $\mathbf{b}^{(l)} \in \mathbb{R}^{n_{l+1}}$, as well as a pointwise
    nonlinearity function $\sigma(.)$ which is [Lipschitz continuous](https://en.wikipedia.org/wiki/Lipschitz_continuity).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬è¯¦ç»†çœ‹ä¸€ä¸‹æ¯ä¸€å±‚ä¸­çš„å‰å‘ä¼ æ’­è®¡ç®—ã€‚å¯¹äº$l=0, \dots, L-1$ï¼Œæ¯ä¸€å±‚$l$å®šä¹‰ä¸€ä¸ªå¸¦æœ‰æƒé‡çŸ©é˜µ$\mathbf{w}^{(l)}
    \in \mathbb{R}^{n_{l} \times n_{l+1}}$å’Œåç½®é¡¹$\mathbf{b}^{(l)} \in \mathbb{R}^{n_{l+1}}$çš„ä»¿å°„å˜æ¢$A^{(l)}$ï¼Œä»¥åŠä¸€ä¸ªé€ç‚¹éçº¿æ€§å‡½æ•°$\sigma(.)$ï¼Œå®ƒæ˜¯[Lipschitzè¿ç»­çš„](https://en.wikipedia.org/wiki/Lipschitz_continuity)ã€‚
- en: $$ \begin{aligned} A^{(0)} &= \mathbf{x} \\ \tilde{A}^{(l+1)}(\mathbf{x}) &=
    \frac{1}{\sqrt{n_l}} {\mathbf{w}^{(l)}}^\top A^{(l)} + \beta\mathbf{b}^{(l)}\quad\in\mathbb{R}^{n_{l+1}}
    & \text{; pre-activations}\\ A^{(l+1)}(\mathbf{x}) &= \sigma(\tilde{A}^{(l+1)}(\mathbf{x}))\quad\in\mathbb{R}^{n_{l+1}}
    & \text{; post-activations} \end{aligned} $$
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} A^{(0)} &= \mathbf{x} \\ \tilde{A}^{(l+1)}(\mathbf{x}) &=
    \frac{1}{\sqrt{n_l}} {\mathbf{w}^{(l)}}^\top A^{(l)} + \beta\mathbf{b}^{(l)}\quad\in\mathbb{R}^{n_{l+1}}
    & \text{; é¢„æ¿€æ´»}\\ A^{(l+1)}(\mathbf{x}) &= \sigma(\tilde{A}^{(l+1)}(\mathbf{x}))\quad\in\mathbb{R}^{n_{l+1}}
    & \text{; åæ¿€æ´»} \end{aligned} $$
- en: Note that the *NTK parameterization* applies a rescale weight $1/\sqrt{n_l}$
    on the transformation to avoid divergence with infinite-width networks. The constant
    scalar $\beta \geq 0$ controls how much effort the bias terms have.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œ*NTK å‚æ•°åŒ–* åœ¨è½¬æ¢ä¸Šåº”ç”¨äº†ä¸€ä¸ªé‡æ–°ç¼©æ”¾æƒé‡ $1/\sqrt{n_l}$ï¼Œä»¥é¿å…ä¸æ— é™å®½åº¦ç½‘ç»œçš„å‘æ•£ã€‚å¸¸æ•°æ ‡é‡ $\beta \geq 0$
    æ§åˆ¶åç½®é¡¹çš„å½±å“ç¨‹åº¦ã€‚
- en: All the network parameters are initialized as an i.i.d Gaussian $\mathcal{N}(0,
    1)$ in the following analysis.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰ç½‘ç»œå‚æ•°åœ¨ä»¥ä¸‹åˆ†æä¸­éƒ½åˆå§‹åŒ–ä¸ºç‹¬ç«‹åŒåˆ†å¸ƒçš„é«˜æ–¯åˆ†å¸ƒ $\mathcal{N}(0, 1)$ã€‚
- en: Neural Tangent Kernel
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¥ç»åˆ‡å‘æ ¸
- en: '**Neural tangent kernel (NTK)** ([Jacot et al. 2018](https://arxiv.org/abs/1806.07572))
    is an important concept for understanding neural network training via gradient
    descent. At its core, it explains how updating the model parameters on one data
    sample affects the predictions for other samples.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¥ç»åˆ‡å‘æ ¸ (NTK)** ([Jacot ç­‰äººï¼Œ2018](https://arxiv.org/abs/1806.07572)) æ˜¯é€šè¿‡æ¢¯åº¦ä¸‹é™ç†è§£ç¥ç»ç½‘ç»œè®­ç»ƒçš„é‡è¦æ¦‚å¿µã€‚åœ¨å…¶æ ¸å¿ƒï¼Œå®ƒè§£é‡Šäº†æ›´æ–°æ¨¡å‹å‚æ•°å¯¹ä¸€ä¸ªæ•°æ®æ ·æœ¬çš„é¢„æµ‹å¦‚ä½•å½±å“å…¶ä»–æ ·æœ¬ã€‚'
- en: Letâ€™s start with the intuition behind NTK, step by step.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€æ­¥äº†è§£ NTK èƒŒåçš„ç›´è§‰ã€‚
- en: 'The empirical loss function $\mathcal{L}: \mathbb{R}^P \to \mathbb{R}_+$ to
    minimize during training is defined as follows, using a per-sample cost function
    $\ell: \mathbb{R}^{n_0} \times \mathbb{R}^{n_L} \to \mathbb{R}_+$:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¦åœ¨è®­ç»ƒæœŸé—´æœ€å°åŒ–çš„ç»éªŒæŸå¤±å‡½æ•° $\mathcal{L}: \mathbb{R}^P \to \mathbb{R}_+$ å®šä¹‰å¦‚ä¸‹ï¼Œä½¿ç”¨æ¯ä¸ªæ ·æœ¬çš„æˆæœ¬å‡½æ•°
    $\ell: \mathbb{R}^{n_0} \times \mathbb{R}^{n_L} \to \mathbb{R}_+$ï¼š'
- en: $$ \mathcal{L}(\theta) =\frac{1}{N} \sum_{i=1}^N \ell(f(\mathbf{x}^{(i)}; \theta),
    y^{(i)}) $$
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathcal{L}(\theta) =\frac{1}{N} \sum_{i=1}^N \ell(f(\mathbf{x}^{(i)}; \theta),
    y^{(i)}) $$
- en: 'and according to the chain rule. the gradient of the loss is:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®é“¾å¼æ³•åˆ™ï¼ŒæŸå¤±çš„æ¢¯åº¦æ˜¯ï¼š
- en: $$ \nabla_\theta \mathcal{L}(\theta)= \frac{1}{N} \sum_{i=1}^N \underbrace{\nabla_\theta
    f(\mathbf{x}^{(i)}; \theta)}_{\text{size }P \times n_L} \underbrace{\nabla_f \ell(f,
    y^{(i)})}_{\text{size } n_L \times 1} $$
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \nabla_\theta \mathcal{L}(\theta)= \frac{1}{N} \sum_{i=1}^N \underbrace{\nabla_\theta
    f(\mathbf{x}^{(i)}; \theta)}_{\text{å¤§å°ä¸º }P \times n_L} \underbrace{\nabla_f \ell(f,
    y^{(i)})}_{\text{å¤§å°ä¸º } n_L \times 1} $$
- en: 'When tracking how the network parameter $\theta$ evolves in time, each gradient
    descent update introduces a small incremental change of an infinitesimal step
    size. Because of the update step is small enough, it can be approximately viewed
    as a derivative on the time dimension:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å½“è·Ÿè¸ªç½‘ç»œå‚æ•° $\theta$ åœ¨æ—¶é—´ä¸Šçš„æ¼”å˜æ—¶ï¼Œæ¯æ¬¡æ¢¯åº¦ä¸‹é™æ›´æ–°éƒ½å¼•å…¥äº†ä¸€ä¸ªå¾®å°æ­¥é•¿çš„å¾®å°å¢é‡å˜åŒ–ã€‚ç”±äºæ›´æ–°æ­¥é•¿è¶³å¤Ÿå°ï¼Œå¯ä»¥è¿‘ä¼¼çœ‹ä½œæ˜¯æ—¶é—´ç»´åº¦ä¸Šçš„å¯¼æ•°ï¼š
- en: $$ \frac{d\theta}{d t} = - \nabla_\theta\mathcal{L}(\theta) = -\frac{1}{N} \sum_{i=1}^N
    \nabla_\theta f(\mathbf{x}^{(i)}; \theta) \nabla_f \ell(f, y^{(i)}) $$
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \frac{d\theta}{d t} = - \nabla_\theta\mathcal{L}(\theta) = -\frac{1}{N} \sum_{i=1}^N
    \nabla_\theta f(\mathbf{x}^{(i)}; \theta) \nabla_f \ell(f, y^{(i)}) $$
- en: 'Again, by the chain rule, the network output evolves according to the derivative:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å†æ¬¡ï¼Œæ ¹æ®é“¾å¼æ³•åˆ™ï¼Œç½‘ç»œè¾“å‡ºæ ¹æ®å¯¼æ•°çš„æ¼”å˜å¦‚ä¸‹ï¼š
- en: $$ \frac{df(\mathbf{x};\theta)}{dt} = \frac{df(\mathbf{x};\theta)}{d\theta}\frac{d\theta}{dt}
    = -\frac{1}{N} \sum_{i=1}^N \color{blue}{\underbrace{\nabla_\theta f(\mathbf{x};\theta)^\top
    \nabla_\theta f(\mathbf{x}^{(i)}; \theta)}_\text{Neural tangent kernel}} \color{black}{\nabla_f
    \ell(f, y^{(i)})} $$
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \frac{df(\mathbf{x};\theta)}{dt} = \frac{df(\mathbf{x};\theta)}{d\theta}\frac{d\theta}{dt}
    = -\frac{1}{N} \sum_{i=1}^N \color{blue}{\underbrace{\nabla_\theta f(\mathbf{x};\theta)^\top
    \nabla_\theta f(\mathbf{x}^{(i)}; \theta)}_\text{ç¥ç»åˆ‡å‘æ ¸}} \color{black}{\nabla_f
    \ell(f, y^{(i)})} $$
- en: 'Here we find the **Neural Tangent Kernel (NTK)**, as defined in the blue part
    in the above formula, $K: \mathbb{R}^{n_0}\times\mathbb{R}^{n_0} \to \mathbb{R}^{n_L
    \times n_L}$ :'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æ‰¾åˆ°äº†ä¸Šè¿°å…¬å¼ä¸­è“è‰²éƒ¨åˆ†å®šä¹‰çš„ **ç¥ç»åˆ‡å‘æ ¸ (NTK)**ï¼Œ$K: \mathbb{R}^{n_0}\times\mathbb{R}^{n_0}
    \to \mathbb{R}^{n_L \times n_L}$ï¼š'
- en: $$ K(\mathbf{x}, \mathbf{x}'; \theta) = \nabla_\theta f(\mathbf{x};\theta)^\top
    \nabla_\theta f(\mathbf{x}'; \theta) $$
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: $$ K(\mathbf{x}, \mathbf{x}'; \theta) = \nabla_\theta f(\mathbf{x};\theta)^\top
    \nabla_\theta f(\mathbf{x}'; \theta) $$
- en: 'where each entry in the output matrix at location $(m, n), 1 \leq m, n \leq
    n_L$ is:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºçŸ©é˜µä¸­æ¯ä¸ªä½ç½® $(m, n), 1 \leq m, n \leq n_L$ çš„æ¯ä¸ªæ¡ç›®æ˜¯ï¼š
- en: $$ K_{m,n}(\mathbf{x}, \mathbf{x}'; \theta) = \sum_{p=1}^P \frac{\partial f_m(\mathbf{x};\theta)}{\partial
    \theta_p} \frac{\partial f_n(\mathbf{x}';\theta)}{\partial \theta_p} $$
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: $$ K_{m,n}(\mathbf{x}, \mathbf{x}'; \theta) = \sum_{p=1}^P \frac{\partial f_m(\mathbf{x};\theta)}{\partial
    \theta_p} \frac{\partial f_n(\mathbf{x}';\theta)}{\partial \theta_p} $$
- en: The â€œfeature mapâ€ form of one input $\mathbf{x}$ is $\varphi(\mathbf{x}) = \nabla_\theta
    f(\mathbf{x};\theta)$.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªè¾“å…¥ $\mathbf{x}$ çš„â€œç‰¹å¾æ˜ å°„â€å½¢å¼æ˜¯ $\varphi(\mathbf{x}) = \nabla_\theta f(\mathbf{x};\theta)$ã€‚
- en: Infinite Width Networks
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ— é™å®½åº¦ç½‘ç»œ
- en: To understand why the effect of one gradient descent is so similar for different
    initializations of network parameters, several pioneering theoretical work starts
    with infinite width networks. We will look into detailed proof using NTK of how
    it guarantees that infinite width networks can converge to a global minimum when
    trained to minimize an empirical loss.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ç†è§£ä¸ºä»€ä¹ˆä¸€ä¸ªæ¢¯åº¦ä¸‹é™çš„æ•ˆæœå¯¹äºç½‘ç»œå‚æ•°çš„ä¸åŒåˆå§‹åŒ–å¦‚æ­¤ç›¸ä¼¼ï¼Œä¸€äº›å¼€åˆ›æ€§çš„ç†è®ºå·¥ä½œä»æ— é™å®½åº¦çš„ç½‘ç»œå¼€å§‹ã€‚æˆ‘ä»¬å°†é€šè¿‡ä½¿ç”¨ NTK æ¥è¯¦ç»†è¯æ˜ï¼Œæ— é™å®½åº¦çš„ç½‘ç»œåœ¨è®­ç»ƒä»¥æœ€å°åŒ–ç»éªŒæŸå¤±æ—¶å¯ä»¥æ”¶æ•›åˆ°å…¨å±€æœ€å°å€¼ã€‚
- en: Connection with Gaussian Processes
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸é«˜æ–¯è¿‡ç¨‹çš„è¿æ¥
- en: 'Deep neural networks have deep connection with gaussian processes ([Neal 1994](https://www.cs.toronto.edu/~radford/ftp/pin.pdf)).
    The output functions of a $L$-layer network, $f_i(\mathbf{x}; \theta)$ for $i=1,
    \dots, n_L$ , are i.i.d. centered Gaussian process of covariance $\Sigma^{(L)}$,
    defined recursively as:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦ç¥ç»ç½‘ç»œä¸é«˜æ–¯è¿‡ç¨‹æœ‰æ·±åˆ»çš„è”ç³»ï¼ˆ[Neal 1994](https://www.cs.toronto.edu/~radford/ftp/pin.pdf)ï¼‰ã€‚$L$
    å±‚ç½‘ç»œçš„è¾“å‡ºå‡½æ•° $f_i(\mathbf{x}; \theta)$ å¯¹äº $i=1, \dots, n_L$ï¼Œæ˜¯å…·æœ‰åæ–¹å·® $\Sigma^{(L)}$
    çš„ç‹¬ç«‹åŒåˆ†å¸ƒçš„ä¸­å¿ƒåŒ–é«˜æ–¯è¿‡ç¨‹ï¼Œé€’å½’å®šä¹‰å¦‚ä¸‹ï¼š
- en: $$ \begin{aligned} \Sigma^{(1)}(\mathbf{x}, \mathbf{x}') &= \frac{1}{n_0}\mathbf{x}^\top{\mathbf{x}'}
    + \beta^2 \\ \lambda^{(l+1)}(\mathbf{x}, \mathbf{x}') &= \begin{bmatrix} \Sigma^{(l)}(\mathbf{x},
    \mathbf{x}) & \Sigma^{(l)}(\mathbf{x}, \mathbf{x}') \\ \Sigma^{(l)}(\mathbf{x}',
    \mathbf{x}) & \Sigma^{(l)}(\mathbf{x}', \mathbf{x}') \end{bmatrix} \\ \Sigma^{(l+1)}(\mathbf{x},
    \mathbf{x}') &= \mathbb{E}_{f \sim \mathcal{N}(0, \lambda^{(l)})}[\sigma(f(\mathbf{x}))
    \sigma(f(\mathbf{x}'))] + \beta^2 \end{aligned} $$
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \Sigma^{(1)}(\mathbf{x}, \mathbf{x}') &= \frac{1}{n_0}\mathbf{x}^\top{\mathbf{x}'}
    + \beta^2 \\ \lambda^{(l+1)}(\mathbf{x}, \mathbf{x}') &= \begin{bmatrix} \Sigma^{(l)}(\mathbf{x},
    \mathbf{x}) & \Sigma^{(l)}(\mathbf{x}, \mathbf{x}') \\ \Sigma^{(l)}(\mathbf{x}',
    \mathbf{x}) & \Sigma^{(l)}(\mathbf{x}', \mathbf{x}') \end{bmatrix} \\ \Sigma^{(l+1)}(\mathbf{x},
    \mathbf{x}') &= \mathbb{E}_{f \sim \mathcal{N}(0, \lambda^{(l)})}[\sigma(f(\mathbf{x}))
    \sigma(f(\mathbf{x}'))] + \beta^2 \end{aligned} $$
- en: '[Lee & Bahri et al. (2018)](https://arxiv.org/abs/1711.00165) showed a proof
    by mathematical induction:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[Lee & Bahri ç­‰äºº (2018)](https://arxiv.org/abs/1711.00165) é€šè¿‡æ•°å­¦å½’çº³æ³•å±•ç¤ºäº†ä¸€ä¸ªè¯æ˜ï¼š'
- en: '(1) Letâ€™s start with $L=1$, when there is no nonlinearity function and the
    input is only processed by a simple affine transformation:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: (1) è®©æˆ‘ä»¬ä» $L=1$ å¼€å§‹ï¼Œå½“æ²¡æœ‰éçº¿æ€§å‡½æ•°ä¸”è¾“å…¥ä»…é€šè¿‡ç®€å•çš„ä»¿å°„å˜æ¢å¤„ç†æ—¶ï¼š
- en: $$ \begin{aligned} f(\mathbf{x};\theta) = \tilde{A}^{(1)}(\mathbf{x}) &= \frac{1}{\sqrt{n_0}}{\mathbf{w}^{(0)}}^\top\mathbf{x}
    + \beta\mathbf{b}^{(0)} \\ \text{where }\tilde{A}_m^{(1)}(\mathbf{x}) &= \frac{1}{\sqrt{n_0}}\sum_{i=1}^{n_0}
    w^{(0)}_{im}x_i + \beta b^{(0)}_m\quad \text{for }1 \leq m \leq n_1 \end{aligned}
    $$
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} f(\mathbf{x};\theta) = \tilde{A}^{(1)}(\mathbf{x}) &= \frac{1}{\sqrt{n_0}}{\mathbf{w}^{(0)}}^\top\mathbf{x}
    + \beta\mathbf{b}^{(0)} \\ \text{å…¶ä¸­ }\tilde{A}_m^{(1)}(\mathbf{x}) &= \frac{1}{\sqrt{n_0}}\sum_{i=1}^{n_0}
    w^{(0)}_{im}x_i + \beta b^{(0)}_m\quad \text{å¯¹äº }1 \leq m \leq n_1 \end{aligned}
    $$
- en: Since the weights and biases are initialized i.i.d., all the output dimensions
    of this network ${\tilde{A}^{(1)}_1(\mathbf{x}), \dots, \tilde{A}^{(1)}_{n_1}(\mathbf{x})}$
    are also i.i.d. Given different inputs, the $m$-th network outputs $\tilde{A}^{(1)}_m(.)$
    have a joint multivariate Gaussian distribution, equivalent to a Gaussian process
    with covariance function (We know that mean $\mu_w=\mu_b=0$ and variance $\sigma^2_w
    = \sigma^2_b=1$)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæƒé‡å’Œåç½®æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒåˆå§‹åŒ–çš„ï¼Œè¿™ä¸ªç½‘ç»œçš„æ‰€æœ‰è¾“å‡ºç»´åº¦ ${\tilde{A}^{(1)}_1(\mathbf{x}), \dots, \tilde{A}^{(1)}_{n_1}(\mathbf{x})}$
    ä¹Ÿæ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„ã€‚ç»™å®šä¸åŒçš„è¾“å…¥ï¼Œç¬¬ $m$ ä¸ªç½‘ç»œè¾“å‡º $\tilde{A}^{(1)}_m(.)$ å…·æœ‰è”åˆå¤šå…ƒé«˜æ–¯åˆ†å¸ƒï¼Œç›¸å½“äºå…·æœ‰åæ–¹å·®å‡½æ•°çš„é«˜æ–¯è¿‡ç¨‹ï¼ˆæˆ‘ä»¬çŸ¥é“å‡å€¼
    $\mu_w=\mu_b=0$ å’Œæ–¹å·® $\sigma^2_w = \sigma^2_b=1$ï¼‰
- en: $$ \begin{aligned} \Sigma^{(1)}(\mathbf{x}, \mathbf{x}') &= \mathbb{E}[\tilde{A}_m^{(1)}(\mathbf{x})\tilde{A}_m^{(1)}(\mathbf{x}')]
    \\ &= \mathbb{E}\Big[\Big( \frac{1}{\sqrt{n_0}}\sum_{i=1}^{n_0} w^{(0)}_{i,m}x_i
    + \beta b^{(0)}_m \Big) \Big( \frac{1}{\sqrt{n_0}}\sum_{i=1}^{n_0} w^{(0)}_{i,m}x'_i
    + \beta b^{(0)}_m \Big)\Big] \\ &= \frac{1}{n_0} \sigma^2_w \sum_{i=1}^{n_0} \sum_{j=1}^{n_0}
    x_i{x'}_j + \frac{\beta \mu_b}{\sqrt{n_0}} \sum_{i=1}^{n_0} w_{im}(x_i + x'_i)
    + \sigma^2_b \beta^2 \\ &= \frac{1}{n_0}\mathbf{x}^\top{\mathbf{x}'} + \beta^2
    \end{aligned} $$
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \Sigma^{(1)}(\mathbf{x}, \mathbf{x}') &= \mathbb{E}[\tilde{A}_m^{(1)}(\mathbf{x})\tilde{A}_m^{(1)}(\mathbf{x}')]
    \\ &= \mathbb{E}\Big[\Big( \frac{1}{\sqrt{n_0}}\sum_{i=1}^{n_0} w^{(0)}_{i,m}x_i
    + \beta b^{(0)}_m \Big) \Big( \frac{1}{\sqrt{n_0}}\sum_{i=1}^{n_0} w^{(0)}_{i,m}x'_i
    + \beta b^{(0)}_m \Big)\Big] \\ &= \frac{1}{n_0} \sigma^2_w \sum_{i=1}^{n_0} \sum_{j=1}^{n_0}
    x_i{x'}_j + \frac{\beta \mu_b}{\sqrt{n_0}} \sum_{i=1}^{n_0} w_{im}(x_i + x'_i)
    + \sigma^2_b \beta^2 \\ &= \frac{1}{n_0}\mathbf{x}^\top{\mathbf{x}'} + \beta^2
    \end{aligned} $$
- en: (2) Using induction, we first assume the proposition is true for $L=l$, a $l$-layer
    network, and thus $\tilde{A}^{(l)}_m(.)$ is a Gaussian process with covariance
    $\Sigma^{(l)}$ and $\{\tilde{A}^{(l)}_i\}_{i=1}^{n_l}$ are i.i.d.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: (2) ä½¿ç”¨å½’çº³æ³•ï¼Œæˆ‘ä»¬é¦–å…ˆå‡è®¾å‘½é¢˜å¯¹äº $L=l$ï¼Œä¸€ä¸ª $l$ å±‚ç½‘ç»œæˆç«‹ï¼Œå› æ­¤ $\tilde{A}^{(l)}_m(.)$ æ˜¯ä¸€ä¸ªå…·æœ‰åæ–¹å·® $\Sigma^{(l)}$
    çš„é«˜æ–¯è¿‡ç¨‹ï¼Œä¸” $\{\tilde{A}^{(l)}_i\}_{i=1}^{n_l}$ æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„ã€‚
- en: 'Then we need to prove the proposition is also true for $L=l+1$. We compute
    the outputs by:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬éœ€è¦è¯æ˜å¯¹äº $L=l+1$ æ—¶å‘½é¢˜ä¹Ÿæˆç«‹ã€‚æˆ‘ä»¬é€šè¿‡è®¡ç®—è¾“å‡ºæ¥ï¼š
- en: $$ \begin{aligned} f(\mathbf{x};\theta) = \tilde{A}^{(l+1)}(\mathbf{x}) &= \frac{1}{\sqrt{n_l}}{\mathbf{w}^{(l)}}^\top
    \sigma(\tilde{A}^{(l)}(\mathbf{x})) + \beta\mathbf{b}^{(l)} \\ \text{where }\tilde{A}^{(l+1)}_m(\mathbf{x})
    &= \frac{1}{\sqrt{n_l}}\sum_{i=1}^{n_l} w^{(l)}_{im}\sigma(\tilde{A}^{(l)}_i(\mathbf{x}))
    + \beta b^{(l)}_m \quad \text{for }1 \leq m \leq n_{l+1} \end{aligned} $$
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} f(\mathbf{x};\theta) = \tilde{A}^{(l+1)}(\mathbf{x}) &= \frac{1}{\sqrt{n_l}}{\mathbf{w}^{(l)}}^\top
    \sigma(\tilde{A}^{(l)}(\mathbf{x})) + \beta\mathbf{b}^{(l)} \\ \text{å…¶ä¸­ }\tilde{A}^{(l+1)}_m(\mathbf{x})
    &= \frac{1}{\sqrt{n_l}}\sum_{i=1}^{n_l} w^{(l)}_{im}\sigma(\tilde{A}^{(l)}_i(\mathbf{x}))
    + \beta b^{(l)}_m \quad \text{å¯¹äº }1 \leq m \leq n_{l+1} \end{aligned} $$
- en: 'We can infer that the expectation of the sum of contributions of the previous
    hidden layers is zero:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥æ¨æ–­å‰å‡ ä¸ªéšè—å±‚è´¡çŒ®çš„æœŸæœ›ä¸ºé›¶ï¼š
- en: $$ \begin{aligned} \mathbb{E}[w^{(l)}_{im}\sigma(\tilde{A}^{(l)}_i(\mathbf{x}))]
    &= \mathbb{E}[w^{(l)}_{im}]\mathbb{E}[\sigma(\tilde{A}^{(l)}_i(\mathbf{x}))] =
    \mu_w \mathbb{E}[\sigma(\tilde{A}^{(l)}_i(\mathbf{x}))] = 0 \\ \mathbb{E}[\big(w^{(l)}_{im}\sigma(\tilde{A}^{(l)}_i(\mathbf{x}))\big)^2]
    &= \mathbb{E}[{w^{(l)}_{im}}^2]\mathbb{E}[\sigma(\tilde{A}^{(l)}_i(\mathbf{x}))^2]
    = \sigma_w^2 \Sigma^{(l)}(\mathbf{x}, \mathbf{x}) = \Sigma^{(l)}(\mathbf{x}, \mathbf{x})
    \end{aligned} $$
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \mathbb{E}[w^{(l)}_{im}\sigma(\tilde{A}^{(l)}_i(\mathbf{x}))]
    &= \mathbb{E}[w^{(l)}_{im}]\mathbb{E}[\sigma(\tilde{A}^{(l)}_i(\mathbf{x}))] =
    \mu_w \mathbb{E}[\sigma(\tilde{A}^{(l)}_i(\mathbf{x}))] = 0 \\ \mathbb{E}[\big(w^{(l)}_{im}\sigma(\tilde{A}^{(l)}_i(\mathbf{x}))\big)^2]
    &= \mathbb{E}[{w^{(l)}_{im}}^2]\mathbb{E}[\sigma(\tilde{A}^{(l)}_i(\mathbf{x}))^2]
    = \sigma_w^2 \Sigma^{(l)}(\mathbf{x}, \mathbf{x}) = \Sigma^{(l)}(\mathbf{x}, \mathbf{x})
    \end{aligned} $$
- en: Since $\{\tilde{A}^{(l)}_i(\mathbf{x})\}_{i=1}^{n_l}$ are i.i.d., according
    to central limit theorem, when the hidden layer gets infinitely wide $n_l \to
    \infty$, $\tilde{A}^{(l+1)}_m(\mathbf{x})$ is Gaussian distributed with variance
    $\beta^2 + \text{Var}(\tilde{A}_i^{(l)}(\mathbf{x}))$. Note that ${\tilde{A}^{(l+1)}_1(\mathbf{x}),
    \dots, \tilde{A}^{(l+1)}_{n_{l+1}}(\mathbf{x})}$ are still i.i.d.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äº $\{\tilde{A}^{(l)}_i(\mathbf{x})\}_{i=1}^{n_l}$ æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„ï¼Œæ ¹æ®ä¸­å¿ƒæé™å®šç†ï¼Œå½“éšè—å±‚å˜å¾—æ— é™å®½æ—¶
    $n_l \to \infty$ï¼Œ$\tilde{A}^{(l+1)}_m(\mathbf{x})$ æœä»é«˜æ–¯åˆ†å¸ƒï¼Œæ–¹å·®ä¸º $\beta^2 + \text{Var}(\tilde{A}_i^{(l)}(\mathbf{x}))$ã€‚æ³¨æ„
    ${\tilde{A}^{(l+1)}_1(\mathbf{x}), \dots, \tilde{A}^{(l+1)}_{n_{l+1}}(\mathbf{x})}$
    ä»ç„¶æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„ã€‚
- en: '$\tilde{A}^{(l+1)}_m(.)$ is equivalent to a Gaussian process with covariance
    function:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: $\tilde{A}^{(l+1)}_m(.)$ ç­‰ä»·äºå…·æœ‰åæ–¹å·®å‡½æ•°çš„é«˜æ–¯è¿‡ç¨‹ï¼š
- en: $$ \begin{aligned} \Sigma^{(l+1)}(\mathbf{x}, \mathbf{x}') &= \mathbb{E}[\tilde{A}^{(l+1)}_m(\mathbf{x})\tilde{A}^{(l+1)}_m(\mathbf{x}')]
    \\ &= \frac{1}{n_l} \sigma\big(\tilde{A}^{(l)}_i(\mathbf{x})\big)^\top \sigma\big(\tilde{A}^{(l)}_i(\mathbf{x}')\big)
    + \beta^2 \quad\text{;similar to how we get }\Sigma^{(1)} \end{aligned} $$
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \Sigma^{(l+1)}(\mathbf{x}, \mathbf{x}') &= \mathbb{E}[\tilde{A}^{(l+1)}_m(\mathbf{x})\tilde{A}^{(l+1)}_m(\mathbf{x}')]
    \\ &= \frac{1}{n_l} \sigma\big(\tilde{A}^{(l)}_i(\mathbf{x})\big)^\top \sigma\big(\tilde{A}^{(l)}_i(\mathbf{x}')\big)
    + \beta^2 \quad\text{ï¼›ç±»ä¼¼äºæˆ‘ä»¬å¾—åˆ°çš„ }\Sigma^{(1)} \end{aligned} $$
- en: When $n_l \to \infty$, according to central limit theorem,
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ $n_l \to \infty$ æ—¶ï¼Œæ ¹æ®ä¸­å¿ƒæé™å®šç†ï¼Œ
- en: $$ \Sigma^{(l+1)}(\mathbf{x}, \mathbf{x}') \to \mathbb{E}_{f \sim \mathcal{N}(0,
    \Lambda^{(l)})}[\sigma(f(\mathbf{x}))^\top \sigma(f(\mathbf{x}'))] + \beta^2 $$
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \Sigma^{(l+1)}(\mathbf{x}, \mathbf{x}') \to \mathbb{E}_{f \sim \mathcal{N}(0,
    \Lambda^{(l)})}[\sigma(f(\mathbf{x}))^\top \sigma(f(\mathbf{x}'))] + \beta^2 $$
- en: The form of Gaussian processes in the above process is referred to as the *Neural
    Network Gaussian Process (NNGP)* ([Lee & Bahri et al. (2018)](https://arxiv.org/abs/1711.00165)).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°è¿‡ç¨‹ä¸­çš„é«˜æ–¯è¿‡ç¨‹å½¢å¼è¢«ç§°ä¸º*ç¥ç»ç½‘ç»œé«˜æ–¯è¿‡ç¨‹ï¼ˆNNGPï¼‰*ï¼ˆ[Lee & Bahri et al. (2018)](https://arxiv.org/abs/1711.00165)ï¼‰ã€‚
- en: Deterministic Neural Tangent Kernel
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¡®å®šæ€§ç¥ç»åˆ‡å‘æ ¸
- en: 'Finally we are now prepared enough to look into the most critical proposition
    from the NTK paper:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬ç°åœ¨å‡†å¤‡å¥½æ·±å…¥ç ”ç©¶NTKè®ºæ–‡ä¸­æœ€å…³é”®çš„å‘½é¢˜ï¼š
- en: '**When $n_1, \dots, n_L \to \infty$ (network with infinite width), the NTK
    converges to be:**'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**å½“ $n_1, \dots, n_L \to \infty$ï¼ˆæ— é™å®½åº¦çš„ç½‘ç»œï¼‰æ—¶ï¼ŒNTK æ”¶æ•›ä¸ºï¼š**'
- en: '**(1) deterministic at initialization, meaning that the kernel is irrelevant
    to the initialization values and only determined by the model architecture; and**'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**(1) åœ¨åˆå§‹åŒ–æ—¶æ˜¯ç¡®å®šæ€§çš„ï¼Œæ„å‘³ç€æ ¸ä¸åˆå§‹åŒ–å€¼æ— å…³ï¼Œä»…ç”±æ¨¡å‹æ¶æ„å†³å®šï¼›ä»¥åŠ**'
- en: '**(2) stays constant during training.**'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**(2) åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒä¸å˜ã€‚**'
- en: 'The proof depends on mathematical induction as well:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: è¯æ˜ä¾èµ–äºæ•°å­¦å½’çº³æ³•ï¼š
- en: (1) First of all, we always have $K^{(0)} = 0$. When $L=1$, we can get the representation
    of NTK directly. It is deterministic and does not depend on the network initialization.
    There is no hidden layer, so there is nothing to take on infinite width.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: (1) é¦–å…ˆï¼Œæˆ‘ä»¬æ€»æ˜¯æœ‰ $K^{(0)} = 0$ã€‚å½“ $L=1$ æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥å¾—åˆ° NTK çš„è¡¨ç¤ºã€‚å®ƒæ˜¯ç¡®å®šæ€§çš„ï¼Œä¸ä¾èµ–äºç½‘ç»œåˆå§‹åŒ–ã€‚æ²¡æœ‰éšè—å±‚ï¼Œå› æ­¤æ²¡æœ‰æ— é™å®½åº¦å¯å–ã€‚
- en: $$ \begin{aligned} f(\mathbf{x};\theta) &= \tilde{A}^{(1)}(\mathbf{x}) = \frac{1}{\sqrt{n_0}}
    {\mathbf{w}^{(0)}}^\top\mathbf{x} + \beta\mathbf{b}^{(0)} \\ K^{(1)}(\mathbf{x},
    \mathbf{x}';\theta) &= \Big(\frac{\partial f(\mathbf{x}';\theta)}{\partial \mathbf{w}^{(0)}}\Big)^\top
    \frac{\partial f(\mathbf{x};\theta)}{\partial \mathbf{w}^{(0)}} + \Big(\frac{\partial
    f(\mathbf{x}';\theta)}{\partial \mathbf{b}^{(0)}}\Big)^\top \frac{\partial f(\mathbf{x};\theta)}{\partial
    \mathbf{b}^{(0)}} \\ &= \frac{1}{n_0} \mathbf{x}^\top{\mathbf{x}'} + \beta^2 =
    \Sigma^{(1)}(\mathbf{x}, \mathbf{x}') \end{aligned} $$
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} f(\mathbf{x};\theta) &= \tilde{A}^{(1)}(\mathbf{x}) = \frac{1}{\sqrt{n_0}}
    {\mathbf{w}^{(0)}}^\top\mathbf{x} + \beta\mathbf{b}^{(0)} \\ K^{(1)}(\mathbf{x},
    \mathbf{x}';\theta) &= \Big(\frac{\partial f(\mathbf{x}';\theta)}{\partial \mathbf{w}^{(0)}}\Big)^\top
    \frac{\partial f(\mathbf{x};\theta)}{\partial \mathbf{w}^{(0)}} + \Big(\frac{\partial
    f(\mathbf{x}';\theta)}{\partial \mathbf{b}^{(0)}}\Big)^\top \frac{\partial f(\mathbf{x};\theta)}{\partial
    \mathbf{b}^{(0)}} \\ &= \frac{1}{n_0} \mathbf{x}^\top{\mathbf{x}'} + \beta^2 =
    \Sigma^{(1)}(\mathbf{x}, \mathbf{x}') \end{aligned} $$
- en: (2) Now when $L=l$, we assume that a $l$-layer network with $\tilde{P}$ parameters
    in total, $\tilde{\theta} = (\mathbf{w}^{(0)}, \dots, \mathbf{w}^{(l-1)}, \mathbf{b}^{(0)},
    \dots, \mathbf{b}^{(l-1)}) \in \mathbb{R}^\tilde{P}$, has a NTK converging to
    a deterministic limit when $n_1, \dots, n_{l-1} \to \infty$.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: (2) ç°åœ¨å½“ $L=l$ æ—¶ï¼Œæˆ‘ä»¬å‡è®¾ä¸€ä¸ªæ€»å…±æœ‰ $\tilde{P}$ ä¸ªå‚æ•°çš„ $l$ å±‚ç½‘ç»œï¼Œ$\tilde{\theta} = (\mathbf{w}^{(0)},
    \dots, \mathbf{w}^{(l-1)}, \mathbf{b}^{(0)}, \dots, \mathbf{b}^{(l-1)}) \in \mathbb{R}^\tilde{P}$ï¼Œåœ¨
    $n_1, \dots, n_{l-1} \to \infty$ æ—¶æ”¶æ•›åˆ°ç¡®å®šæ€§æé™ã€‚
- en: $$ K^{(l)}(\mathbf{x}, \mathbf{x}';\tilde{\theta}) = \nabla_{\tilde{\theta}}
    \tilde{A}^{(l)}(\mathbf{x})^\top \nabla_{\tilde{\theta}} \tilde{A}^{(l)}(\mathbf{x}')
    \to K^{(l)}_{\infty}(\mathbf{x}, \mathbf{x}') $$
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: $$ K^{(l)}(\mathbf{x}, \mathbf{x}';\tilde{\theta}) = \nabla_{\tilde{\theta}}
    \tilde{A}^{(l)}(\mathbf{x})^\top \nabla_{\tilde{\theta}} \tilde{A}^{(l)}(\mathbf{x}')
    \to K^{(l)}_{\infty}(\mathbf{x}, \mathbf{x}') $$
- en: Note that $K_\infty^{(l)}$ has no dependency on $\theta$.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ $K_\infty^{(l)}$ ä¸ä¾èµ–äº $\theta$ã€‚
- en: Next letâ€™s check the case $L=l+1$. Compared to a $l$-layer network, a $(l+1)$-layer
    network has additional weight matrix $\mathbf{w}^{(l)}$ and bias $\mathbf{b}^{(l)}$
    and thus the total parameters contain $\theta = (\tilde{\theta}, \mathbf{w}^{(l)},
    \mathbf{b}^{(l)})$.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥è®©æˆ‘ä»¬æ¥çœ‹çœ‹ $L=l+1$ çš„æƒ…å†µã€‚ä¸ $l$ å±‚ç½‘ç»œç›¸æ¯”ï¼Œä¸€ä¸ª $(l+1)$ å±‚ç½‘ç»œæœ‰é¢å¤–çš„æƒé‡çŸ©é˜µ $\mathbf{w}^{(l)}$
    å’Œåç½® $\mathbf{b}^{(l)}$ï¼Œå› æ­¤æ€»å‚æ•°åŒ…å« $\theta = (\tilde{\theta}, \mathbf{w}^{(l)}, \mathbf{b}^{(l)})$ã€‚
- en: 'The output function of this $(l+1)$-layer network is:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ª $(l+1)$ å±‚ç½‘ç»œçš„è¾“å‡ºå‡½æ•°æ˜¯ï¼š
- en: $$ f(\mathbf{x};\theta) = \tilde{A}^{(l+1)}(\mathbf{x};\theta) = \frac{1}{\sqrt{n_l}}
    {\mathbf{w}^{(l)}}^\top \sigma\big(\tilde{A}^{(l)}(\mathbf{x})\big) + \beta \mathbf{b}^{(l)}
    $$
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: $$ f(\mathbf{x};\theta) = \tilde{A}^{(l+1)}(\mathbf{x};\theta) = \frac{1}{\sqrt{n_l}}
    {\mathbf{w}^{(l)}}^\top \sigma\big(\tilde{A}^{(l)}(\mathbf{x})\big) + \beta \mathbf{b}^{(l)}
    $$
- en: 'And we know its derivative with respect to different sets of parameters; let
    denote $\tilde{A}^{(l)} = \tilde{A}^{(l)}(\mathbf{x})$ for brevity in the following
    equation:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çŸ¥é“å®ƒå¯¹ä¸åŒå‚æ•°é›†çš„å¯¼æ•°ï¼›ä¸ºäº†ç®€ä¾¿èµ·è§ï¼Œåœ¨ä»¥ä¸‹æ–¹ç¨‹ä¸­ç”¨ $\tilde{A}^{(l)} = \tilde{A}^{(l)}(\mathbf{x})$
    è¡¨ç¤ºï¼š
- en: $$ \begin{aligned} \nabla_{\color{blue}{\mathbf{w}^{(l)}}} f(\mathbf{x};\theta)
    &= \color{blue}{ \frac{1}{\sqrt{n_l}} \sigma\big(\tilde{A}^{(l)}\big)^\top } \color{black}{\quad
    \in \mathbb{R}^{1 \times n_l}} \\ \nabla_{\color{green}{\mathbf{b}^{(l)}}} f(\mathbf{x};\theta)
    &= \color{green}{ \beta } \\ \nabla_{\color{red}{\tilde{\theta}}} f(\mathbf{x};\theta)
    &= \frac{1}{\sqrt{n_l}} \nabla_\tilde{\theta}\sigma(\tilde{A}^{(l)}) \mathbf{w}^{(l)}
    \\ &= \color{red}{ \frac{1}{\sqrt{n_l}} \begin{bmatrix} \dot{\sigma}(\tilde{A}_1^{(l)})\frac{\partial
    \tilde{A}_1^{(l)}}{\partial \tilde{\theta}_1} & \dots & \dot{\sigma}(\tilde{A}_{n_l}^{(l)})\frac{\partial
    \tilde{A}_{n_l}^{(l)}}{\partial \tilde{\theta}_1} \\ \vdots \\ \dot{\sigma}(\tilde{A}_1^{(l)})\frac{\partial
    \tilde{A}_1^{(l)}}{\partial \tilde{\theta}_\tilde{P}} & \dots & \dot{\sigma}(\tilde{A}_{n_l}^{(l)})\frac{\partial
    \tilde{A}_{n_l}^{(l)}}{\partial \tilde{\theta}_\tilde{P}}\\ \end{bmatrix} \mathbf{w}^{(l)}
    \color{black}{\quad \in \mathbb{R}^{\tilde{P} \times n_{l+1}}} } \end{aligned}
    $$
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \nabla_{\color{blue}{\mathbf{w}^{(l)}}} f(\mathbf{x};\theta)
    &= \color{blue}{ \frac{1}{\sqrt{n_l}} \sigma\big(\tilde{A}^{(l)}\big)^\top } \color{black}{\quad
    \in \mathbb{R}^{1 \times n_l}} \\ \nabla_{\color{green}{\mathbf{b}^{(l)}}} f(\mathbf{x};\theta)
    &= \color{green}{ \beta } \\ \nabla_{\color{red}{\tilde{\theta}}} f(\mathbf{x};\theta)
    &= \frac{1}{\sqrt{n_l}} \nabla_\tilde{\theta}\sigma(\tilde{A}^{(l)}) \mathbf{w}^{(l)}
    \\ &= \color{red}{ \frac{1}{\sqrt{n_l}} \begin{bmatrix} \dot{\sigma}(\tilde{A}_1^{(l)})\frac{\partial
    \tilde{A}_1^{(l)}}{\partial \tilde{\theta}_1} & \dots & \dot{\sigma}(\tilde{A}_{n_l}^{(l)})\frac{\partial
    \tilde{A}_{n_l}^{(l)}}{\partial \tilde{\theta}_1} \\ \vdots \\ \dot{\sigma}(\tilde{A}_1^{(l)})\frac{\partial
    \tilde{A}_1^{(l)}}{\partial \tilde{\theta}_\tilde{P}} & \dots & \dot{\sigma}(\tilde{A}_{n_l}^{(l)})\frac{\partial
    \tilde{A}_{n_l}^{(l)}}{\partial \tilde{\theta}_\tilde{P}}\\ \end{bmatrix} \mathbf{w}^{(l)}
    \color{black}{\quad \in \mathbb{R}^{\tilde{P} \times n_{l+1}}} } \end{aligned}
    $$
- en: where $\dot{\sigma}$ is the derivative of $\sigma$ and each entry at location
    $(p, m), 1 \leq p \leq \tilde{P}, 1 \leq m \leq n_{l+1}$ in the matrix $\nabla_{\tilde{\theta}}
    f(\mathbf{x};\theta)$ can be written as
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­$\dot{\sigma}$æ˜¯$\sigma$çš„å¯¼æ•°ï¼ŒçŸ©é˜µ$\nabla_{\tilde{\theta}} f(\mathbf{x};\theta)$ä¸­ä½ç½®$(p,
    m), 1 \leq p \leq \tilde{P}, 1 \leq m \leq n_{l+1}$çš„æ¯ä¸ªæ¡ç›®å¯ä»¥å†™æˆ
- en: $$ \frac{\partial f_m(\mathbf{x};\theta)}{\partial \tilde{\theta}_p} = \sum_{i=1}^{n_l}
    w^{(l)}_{im} \dot{\sigma}\big(\tilde{A}_i^{(l)} \big) \nabla_{\tilde{\theta}_p}
    \tilde{A}_i^{(l)} $$
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \frac{\partial f_m(\mathbf{x};\theta)}{\partial \tilde{\theta}_p} = \sum_{i=1}^{n_l}
    w^{(l)}_{im} \dot{\sigma}\big(\tilde{A}_i^{(l)} \big) \nabla_{\tilde{\theta}_p}
    \tilde{A}_i^{(l)} $$
- en: 'The NTK for this $(l+1)$-layer network can be defined accordingly:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ª$(l+1)$å±‚ç½‘ç»œçš„NTKå¯ä»¥ç›¸åº”åœ°å®šä¹‰ä¸ºï¼š
- en: $$ \begin{aligned} & K^{(l+1)}(\mathbf{x}, \mathbf{x}'; \theta) \\ =& \nabla_{\theta}
    f(\mathbf{x};\theta)^\top \nabla_{\theta} f(\mathbf{x};\theta) \\ =& \color{blue}{\nabla_{\mathbf{w}^{(l)}}
    f(\mathbf{x};\theta)^\top \nabla_{\mathbf{w}^{(l)}} f(\mathbf{x};\theta)} + \color{green}{\nabla_{\mathbf{b}^{(l)}}
    f(\mathbf{x};\theta)^\top \nabla_{\mathbf{b}^{(l)}} f(\mathbf{x};\theta)} + \color{red}{\nabla_{\tilde{\theta}}
    f(\mathbf{x};\theta)^\top \nabla_{\tilde{\theta}} f(\mathbf{x};\theta)} \\ =&
    \frac{1}{n_l} \Big[ \color{blue}{\sigma(\tilde{A}^{(l)})\sigma(\tilde{A}^{(l)})^\top}
    + \color{green}{\beta^2} \\ &+ \color{red}{ {\mathbf{w}^{(l)}}^\top \begin{bmatrix}
    \dot{\sigma}(\tilde{A}_1^{(l)})\dot{\sigma}(\tilde{A}_1^{(l)})\sum_{p=1}^\tilde{P}
    \frac{\partial \tilde{A}_1^{(l)}}{\partial \tilde{\theta}_p}\frac{\partial \tilde{A}_1^{(l)}}{\partial
    \tilde{\theta}_p} & \dots & \dot{\sigma}(\tilde{A}_1^{(l)})\dot{\sigma}(\tilde{A}_{n_l}^{(l)})\sum_{p=1}^\tilde{P}
    \frac{\partial \tilde{A}_1^{(l)}}{\partial \tilde{\theta}_p}\frac{\partial \tilde{A}_{n_l}^{(l)}}{\partial
    \tilde{\theta}_p} \\ \vdots \\ \dot{\sigma}(\tilde{A}_{n_l}^{(l)})\dot{\sigma}(\tilde{A}_1^{(l)})\sum_{p=1}^\tilde{P}
    \frac{\partial \tilde{A}_{n_l}^{(l)}}{\partial \tilde{\theta}_p}\frac{\partial
    \tilde{A}_1^{(l)}}{\partial \tilde{\theta}_p} & \dots & \dot{\sigma}(\tilde{A}_{n_l}^{(l)})\dot{\sigma}(\tilde{A}_{n_l}^{(l)})\sum_{p=1}^\tilde{P}
    \frac{\partial \tilde{A}_{n_l}^{(l)}}{\partial \tilde{\theta}_p}\frac{\partial
    \tilde{A}_{n_l}^{(l)}}{\partial \tilde{\theta}_p} \\ \end{bmatrix} \mathbf{w}^{(l)}
    } \color{black}{\Big]} \\ =& \frac{1}{n_l} \Big[ \color{blue}{\sigma(\tilde{A}^{(l)})\sigma(\tilde{A}^{(l)})^\top}
    + \color{green}{\beta^2} \\ &+ \color{red}{ {\mathbf{w}^{(l)}}^\top \begin{bmatrix}
    \dot{\sigma}(\tilde{A}_1^{(l)})\dot{\sigma}(\tilde{A}_1^{(l)})K^{(l)}_{11} & \dots
    & \dot{\sigma}(\tilde{A}_1^{(l)})\dot{\sigma}(\tilde{A}_{n_l}^{(l)})K^{(l)}_{1n_l}
    \\ \vdots \\ \dot{\sigma}(\tilde{A}_{n_l}^{(l)})\dot{\sigma}(\tilde{A}_1^{(l)})K^{(l)}_{n_l1}
    & \dots & \dot{\sigma}(\tilde{A}_{n_l}^{(l)})\dot{\sigma}(\tilde{A}_{n_l}^{(l)})K^{(l)}_{n_ln_l}
    \\ \end{bmatrix} \mathbf{w}^{(l)} } \color{black}{\Big]} \end{aligned} $$
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} & K^{(l+1)}(\mathbf{x}, \mathbf{x}'; \theta) \\ =& \nabla_{\theta}
    f(\mathbf{x};\theta)^\top \nabla_{\theta} f(\mathbf{x};\theta) \\ =& \color{blue}{\nabla_{\mathbf{w}^{(l)}}
    f(\mathbf{x};\theta)^\top \nabla_{\mathbf{w}^{(l)}} f(\mathbf{x};\theta)} + \color{green}{\nabla_{\mathbf{b}^{(l)}}
    f(\mathbf{x};\theta)^\top \nabla_{\mathbf{b}^{(l)}} f(\mathbf{x};\theta)} + \color{red}{\nabla_{\tilde{\theta}}
    f(\mathbf{x};\theta)^\top \nabla_{\tilde{\theta}} f(\mathbf{x};\theta)} \\ =&
    \frac{1}{n_l} \Big[ \color{blue}{\sigma(\tilde{A}^{(l)})\sigma(\tilde{A}^{(l)})^\top}
    + \color{green}{\beta^2} \\ &+ \color{red}{ {\mathbf{w}^{(l)}}^\top \begin{bmatrix}
    \dot{\sigma}(\tilde{A}_1^{(l)})\dot{\sigma}(\tilde{A}_1^{(l)})\sum_{p=1}^\tilde{P}
    \frac{\partial \tilde{A}_1^{(l)}}{\partial \tilde{\theta}_p}\frac{\partial \tilde{A}_1^{(l)}}{\partial
    \tilde{\theta}_p} & \dots & \dot{\sigma}(\tilde{A}_1^{(l)})\dot{\sigma}(\tilde{A}_{n_l}^{(l)})\sum_{p=1}^\tilde{P}
    \frac{\partial \tilde{A}_1^{(l)}...
- en: 'where each individual entry at location $(m, n), 1 \leq m, n \leq n_{l+1}$
    of the matrix $K^{(l+1)}$ can be written as:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: çŸ©é˜µ $K^{(l+1)}$ ä¸­ä½ç½® $(m, n), 1 \leq m, n \leq n_{l+1}$ å¤„çš„æ¯ä¸ªå•ç‹¬æ¡ç›®å¯å†™ä¸ºï¼š
- en: $$ \begin{aligned} K^{(l+1)}_{mn} =& \frac{1}{n_l}\Big[ \color{blue}{\sigma(\tilde{A}_m^{(l)})\sigma(\tilde{A}_n^{(l)})}
    + \color{green}{\beta^2} + \color{red}{ \sum_{i=1}^{n_l} \sum_{j=1}^{n_l} w^{(l)}_{im}
    w^{(l)}_{in} \dot{\sigma}(\tilde{A}_i^{(l)}) \dot{\sigma}(\tilde{A}_{j}^{(l)})
    K_{ij}^{(l)} } \Big] \end{aligned} $$
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} K^{(l+1)}_{mn} =& \frac{1}{n_l}\Big[ \color{blue}{\sigma(\tilde{A}_m^{(l)})\sigma(\tilde{A}_n^{(l)})}
    + \color{green}{\beta^2} + \color{red}{ \sum_{i=1}^{n_l} \sum_{j=1}^{n_l} w^{(l)}_{im}
    w^{(l)}_{in} \dot{\sigma}(\tilde{A}_i^{(l)}) \dot{\sigma}(\tilde{A}_{j}^{(l)})
    K_{ij}^{(l)} } \Big] \end{aligned} $$
- en: 'When $n_l \to \infty$, the section in blue and green has the limit (See the
    proof in the [previous section](#connection-with-gaussian-processes)):'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ $n_l \to \infty$ æ—¶ï¼Œè“è‰²å’Œç»¿è‰²éƒ¨åˆ†çš„æé™ä¸ºï¼ˆè¯·å‚è§[å‰ä¸€èŠ‚](#connection-with-gaussian-processes)ä¸­çš„è¯æ˜ï¼‰ï¼š
- en: $$ \frac{1}{n_l}\sigma(\tilde{A}^{(l)})\sigma(\tilde{A}^{(l)}) + \beta^2\to
    \Sigma^{(l+1)} $$
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \frac{1}{n_l}\sigma(\tilde{A}^{(l)})\sigma(\tilde{A}^{(l)}) + \beta^2\to
    \Sigma^{(l+1)} $$
- en: 'and the red section has the limit:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: çº¢è‰²éƒ¨åˆ†çš„æé™ä¸ºï¼š
- en: $$ \sum_{i=1}^{n_l} \sum_{j=1}^{n_l} w^{(l)}_{im} w^{(l)}_{in} \dot{\sigma}(\tilde{A}_i^{(l)})
    \dot{\sigma}(\tilde{A}_{j}^{(l)}) K_{ij}^{(l)} \to \sum_{i=1}^{n_l} \sum_{j=1}^{n_l}
    w^{(l)}_{im} w^{(l)}_{in} \dot{\sigma}(\tilde{A}_i^{(l)}) \dot{\sigma}(\tilde{A}_{j}^{(l)})
    K_{\infty,ij}^{(l)} $$
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \sum_{i=1}^{n_l} \sum_{j=1}^{n_l} w^{(l)}_{im} w^{(l)}_{in} \dot{\sigma}(\tilde{A}_i^{(l)})
    \dot{\sigma}(\tilde{A}_{j}^{(l)}) K_{ij}^{(l)} \to \sum_{i=1}^{n_l} \sum_{j=1}^{n_l}
    w^{(l)}_{im} w^{(l)}_{in} \dot{\sigma}(\tilde{A}_i^{(l)}) \dot{\sigma}(\tilde{A}_{j}^{(l)})
    K_{\infty,ij}^{(l)} $$
- en: Later, [Arora et al. (2019)](https://arxiv.org/abs/1904.11955) provided a proof
    with a weaker limit, that does not require all the hidden layers to be infinitely
    wide, but only requires the minimum width to be sufficiently large.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: åæ¥ï¼Œ[Aroraç­‰äººï¼ˆ2019ï¼‰](https://arxiv.org/abs/1904.11955)æä¾›äº†ä¸€ä¸ªè¯æ˜ï¼Œå…·æœ‰æ›´å¼±çš„æé™ï¼Œä¸éœ€è¦æ‰€æœ‰éšè—å±‚éƒ½æ˜¯æ— é™å®½çš„ï¼Œåªéœ€è¦æœ€å°å®½åº¦è¶³å¤Ÿå¤§ã€‚
- en: Linearized Models
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: çº¿æ€§åŒ–æ¨¡å‹
- en: 'From the [previous section](#neural-tangent-kernel), according to the derivative
    chain rule, we have known that the gradient update on the output of an infinite
    width network is as follows; For brevity, we omit the inputs in the following
    analysis:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®[å‰ä¸€èŠ‚](#neural-tangent-kernel)ï¼Œæ ¹æ®å¯¼æ•°é“¾è§„åˆ™ï¼Œæˆ‘ä»¬å·²ç»çŸ¥é“å®½åº¦æ— é™ç½‘ç»œè¾“å‡ºçš„æ¢¯åº¦æ›´æ–°å¦‚ä¸‹ï¼›ä¸ºç®€æ´èµ·è§ï¼Œæˆ‘ä»¬åœ¨ä»¥ä¸‹åˆ†æä¸­çœç•¥è¾“å…¥ï¼š
- en: $$ \begin{aligned} \frac{df(\theta)}{dt} &= -\eta\nabla_\theta f(\theta)^\top
    \nabla_\theta f(\theta) \nabla_f \mathcal{L} & \\ &= -\eta\nabla_\theta f(\theta)^\top
    \nabla_\theta f(\theta) \nabla_f \mathcal{L} & \\ &= -\eta K(\theta) \nabla_f
    \mathcal{L} \\ &= \color{cyan}{-\eta K_\infty \nabla_f \mathcal{L}} & \text{;
    for infinite width network}\\ \end{aligned} $$
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \frac{df(\theta)}{dt} &= -\eta\nabla_\theta f(\theta)^\top
    \nabla_\theta f(\theta) \nabla_f \mathcal{L} & \\ &= -\eta\nabla_\theta f(\theta)^\top
    \nabla_\theta f(\theta) \nabla_f \mathcal{L} & \\ &= -\eta K(\theta) \nabla_f
    \mathcal{L} \\ &= \color{cyan}{-\eta K_\infty \nabla_f \mathcal{L}} & \text{ï¼›å¯¹äºå®½åº¦æ— é™çš„ç½‘ç»œ}\\
    \end{aligned} $$
- en: 'To track the evolution of $\theta$ in time, letâ€™s consider it as a function
    of time step $t$. With Taylor expansion, the network learning dynamics can be
    simplified as:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¿½è¸ª$\theta$éšæ—¶é—´çš„æ¼”å˜ï¼Œè®©æˆ‘ä»¬å°†å…¶è§†ä¸ºæ—¶é—´æ­¥é•¿$t$çš„å‡½æ•°ã€‚é€šè¿‡æ³°å‹’å±•å¼€ï¼Œç½‘ç»œå­¦ä¹ åŠ¨æ€å¯ä»¥ç®€åŒ–ä¸ºï¼š
- en: $$ f(\theta(t)) \approx f^\text{lin}(\theta(t)) = f(\theta(0)) + \underbrace{\nabla_\theta
    f(\theta(0))}_{\text{formally }\nabla_\theta f(\mathbf{x}; \theta) \vert_{\theta=\theta(0)}}
    (\theta(t) - \theta(0)) $$
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: $$ f(\theta(t)) \approx f^\text{lin}(\theta(t)) = f(\theta(0)) + \underbrace{\nabla_\theta
    f(\theta(0))}_{\text{å½¢å¼ä¸Š }\nabla_\theta f(\mathbf{x}; \theta) \vert_{\theta=\theta(0)}}
    (\theta(t) - \theta(0)) $$
- en: 'Such formation is commonly referred to as the *linearized* model, given $\theta(0)$,
    $f(\theta(0))$, and $\nabla_\theta f(\theta(0))$ are all constants. Assuming that
    the incremental time step $t$ is extremely small and the parameter is updated
    by gradient descent:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§å½¢å¼é€šå¸¸è¢«ç§°ä¸º*çº¿æ€§åŒ–*æ¨¡å‹ï¼Œå‡è®¾$\theta(0)$ï¼Œ$f(\theta(0))$å’Œ$\nabla_\theta f(\theta(0))$éƒ½æ˜¯å¸¸æ•°ã€‚å‡è®¾å¢é‡æ—¶é—´æ­¥$t$éå¸¸å°ï¼Œå‚æ•°é€šè¿‡æ¢¯åº¦ä¸‹é™æ›´æ–°ï¼š
- en: $$ \begin{aligned} \theta(t) - \theta(0) &= - \eta \nabla_\theta \mathcal{L}(\theta)
    = - \eta \nabla_\theta f(\theta)^\top \nabla_f \mathcal{L} \\ f^\text{lin}(\theta(t))
    - f(\theta(0)) &= - \eta\nabla_\theta f(\theta(0))^\top \nabla_\theta f(\mathcal{X};\theta(0))
    \nabla_f \mathcal{L} \\ \frac{df(\theta(t))}{dt} &= - \eta K(\theta(0)) \nabla_f
    \mathcal{L} \\ \frac{df(\theta(t))}{dt} &= \color{cyan}{- \eta K_\infty \nabla_f
    \mathcal{L}} & \text{; for infinite width network}\\ \end{aligned} $$
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \theta(t) - \theta(0) &= - \eta \nabla_\theta \mathcal{L}(\theta)
    = - \eta \nabla_\theta f(\theta)^\top \nabla_f \mathcal{L} \\ f^\text{lin}(\theta(t))
    - f(\theta(0)) &= - \eta\nabla_\theta f(\theta(0))^\top \nabla_\theta f(\mathcal{X};\theta(0))
    \nabla_f \mathcal{L} \\ \frac{df(\theta(t))}{dt} &= - \eta K(\theta(0)) \nabla_f
    \mathcal{L} \\ \frac{df(\theta(t))}{dt} &= \color{cyan}{- \eta K_\infty \nabla_f
    \mathcal{L}} & \text{ï¼›å¯¹äºå®½åº¦æ— é™çš„ç½‘ç»œ}\\ \end{aligned} $$
- en: Eventually we get the same learning dynamics, which implies that a neural network
    with infinite width can be considerably simplified as governed by the above linearized
    model ([Lee & Xiao, et al. 2019](https://arxiv.org/abs/1902.06720)).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆæˆ‘ä»¬å¾—åˆ°äº†ç›¸åŒçš„å­¦ä¹ åŠ¨æ€ï¼Œè¿™æ„å‘³ç€ä¸€ä¸ªå®½åº¦æ— é™çš„ç¥ç»ç½‘ç»œå¯ä»¥è¢«å¤§å¤§ç®€åŒ–ä¸ºä¸Šè¿°çº¿æ€§åŒ–æ¨¡å‹ï¼ˆ[Lee & Xiao, et al. 2019](https://arxiv.org/abs/1902.06720)ï¼‰æ‰€æ§åˆ¶ã€‚
- en: 'In a simple case when the empirical loss is an MSE loss, $\nabla_\theta \mathcal{L}(\theta)
    = f(\mathcal{X}; \theta) - \mathcal{Y}$, the dynamics of the network becomes a
    simple linear ODE and it can be solved in a closed form:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸€ä¸ªç®€å•çš„æƒ…å†µä¸‹ï¼Œå½“ç»éªŒæŸå¤±æ˜¯å‡æ–¹è¯¯å·®æŸå¤±æ—¶ï¼Œ$\nabla_\theta \mathcal{L}(\theta) = f(\mathcal{X};
    \theta) - \mathcal{Y}$ï¼Œç½‘ç»œçš„åŠ¨æ€å˜ä¸ºç®€å•çš„çº¿æ€§ODEï¼Œå¹¶ä¸”å¯ä»¥ä»¥å°é—­å½¢å¼è§£å†³ï¼š
- en: $$ \begin{aligned} \frac{df(\theta)}{dt} =& -\eta K_\infty (f(\theta) - \mathcal{Y})
    & \\ \frac{dg(\theta)}{dt} =& -\eta K_\infty g(\theta) & \text{; let }g(\theta)=f(\theta)
    - \mathcal{Y} \\ \int \frac{dg(\theta)}{g(\theta)} =& -\eta \int K_\infty dt &
    \\ g(\theta) &= C e^{-\eta K_\infty t} & \end{aligned} $$
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \frac{df(\theta)}{dt} =& -\eta K_\infty (f(\theta) - \mathcal{Y})
    & \\ \frac{dg(\theta)}{dt} =& -\eta K_\infty g(\theta) & \text{ï¼›è®©}g(\theta)=f(\theta)
    - \mathcal{Y} \\ \int \frac{dg(\theta)}{g(\theta)} =& -\eta \int K_\infty dt &
    \\ g(\theta) &= C e^{-\eta K_\infty t} & \end{aligned} $$
- en: When $t=0$, we have $C=f(\theta(0)) - \mathcal{Y}$ and therefore,
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: å½“$t=0$æ—¶ï¼Œæˆ‘ä»¬æœ‰$C=f(\theta(0)) - \mathcal{Y}$ï¼Œå› æ­¤ï¼Œ
- en: $$ f(\theta) = (f(\theta(0)) - \mathcal{Y})e^{-\eta K_\infty t} + \mathcal{Y}
    \\ = f(\theta(0))e^{-K_\infty t} + (I - e^{-\eta K_\infty t})\mathcal{Y} $$
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: $$ f(\theta) = (f(\theta(0)) - \mathcal{Y})e^{-\eta K_\infty t} + \mathcal{Y}
    \\ = f(\theta(0))e^{-K_\infty t} + (I - e^{-\eta K_\infty t})\mathcal{Y} $$
- en: Lazy Training
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‡’æƒ°è®­ç»ƒ
- en: People observe that when a neural network is heavily over-parameterized, the
    model is able to learn with the training loss quickly converging to zero but the
    network parameters hardly change. *Lazy training* refers to the phenomenon. In
    other words, when the loss $\mathcal{L}$ has a decent amount of reduction, the
    change in the differential of the network $f$ (aka the Jacobian matrix) is still
    very small.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: äººä»¬è§‚å¯Ÿåˆ°ï¼Œå½“ç¥ç»ç½‘ç»œè¿‡åº¦å‚æ•°åŒ–æ—¶ï¼Œæ¨¡å‹èƒ½å¤Ÿå¿«é€Ÿæ”¶æ•›åˆ°é›¶çš„è®­ç»ƒæŸå¤±ï¼Œä½†ç½‘ç»œå‚æ•°å‡ ä¹ä¸ä¼šæ”¹å˜ã€‚*æ‡’æƒ°è®­ç»ƒ*æŒ‡çš„å°±æ˜¯è¿™ç§ç°è±¡ã€‚æ¢å¥è¯è¯´ï¼Œå½“æŸå¤±$\mathcal{L}$æœ‰ç›¸å½“å¤§çš„å‡å°‘æ—¶ï¼Œç½‘ç»œ$f$çš„å¾®åˆ†ï¼ˆä¹Ÿç§°ä¸ºé›…å¯æ¯”çŸ©é˜µï¼‰çš„å˜åŒ–ä»ç„¶éå¸¸å°ã€‚
- en: 'Let $\theta(0)$ be the initial network parameters and $\theta(T)$ be the final
    network parameters when the loss has been minimized to zero. The delta change
    in parameter space can be approximated with first-order Taylor expansion:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: è®©$\theta(0)$ä¸ºåˆå§‹ç½‘ç»œå‚æ•°ï¼Œ$\theta(T)$ä¸ºæŸå¤±æœ€å°åŒ–ä¸ºé›¶æ—¶çš„æœ€ç»ˆç½‘ç»œå‚æ•°ã€‚å‚æ•°ç©ºé—´çš„å˜åŒ–å¯ä»¥ç”¨ä¸€é˜¶æ³°å‹’å±•å¼€æ¥è¿‘ä¼¼ï¼š
- en: $$ \begin{aligned} \hat{y} = f(\theta(T)) &\approx f(\theta(0)) + \nabla_\theta
    f(\theta(0)) (\theta(T) - \theta(0)) \\ \text{Thus }\Delta \theta &= \theta(T)
    - \theta(0) \approx \frac{\|\hat{y} - f(\theta(0))\|}{\| \nabla_\theta f(\theta(0))
    \|} \end{aligned} $$
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \hat{y} = f(\theta(T)) &\approx f(\theta(0)) + \nabla_\theta
    f(\theta(0)) (\theta(T) - \theta(0)) \\ \text{å› æ­¤ }\Delta \theta &= \theta(T) -
    \theta(0) \approx \frac{\|\hat{y} - f(\theta(0))\|}{\| \nabla_\theta f(\theta(0))
    \|} \end{aligned} $$
- en: 'Still following the first-order Taylor expansion, we can track the change in
    the differential of $f$:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ç„¶éµå¾ªä¸€é˜¶æ³°å‹’å±•å¼€ï¼Œæˆ‘ä»¬å¯ä»¥è·Ÿè¸ª$f$çš„å¾®åˆ†çš„å˜åŒ–ï¼š
- en: $$ \begin{aligned} \nabla_\theta f(\theta(T)) &\approx \nabla_\theta f(\theta(0))
    + \nabla^2_\theta f(\theta(0)) \Delta\theta \\ &= \nabla_\theta f(\theta(0)) +
    \nabla^2_\theta f(\theta(0)) \frac{\|\hat{y} - f(\mathbf{x};\theta(0))\|}{\| \nabla_\theta
    f(\theta(0)) \|} \\ \text{Thus }\Delta\big(\nabla_\theta f\big) &= \nabla_\theta
    f(\theta(T)) - \nabla_\theta f(\theta(0)) = \|\hat{y} - f(\mathbf{x};\theta(0))\|
    \frac{\nabla^2_\theta f(\theta(0))}{\| \nabla_\theta f(\theta(0)) \|} \end{aligned}
    $$
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \nabla_\theta f(\theta(T)) &\approx \nabla_\theta f(\theta(0))
    + \nabla^2_\theta f(\theta(0)) \Delta\theta \\ &= \nabla_\theta f(\theta(0)) +
    \nabla^2_\theta f(\theta(0)) \frac{\|\hat{y} - f(\mathbf{x};\theta(0))\|}{\| \nabla_\theta
    f(\theta(0)) \|} \\ \text{å› æ­¤ }\Delta\big(\nabla_\theta f\big) &= \nabla_\theta
    f(\theta(T)) - \nabla_\theta f(\theta(0)) = \|\hat{y} - f(\mathbf{x};\theta(0))\|
    \frac{\nabla^2_\theta f(\theta(0))}{\| \nabla_\theta f(\theta(0)) \|} \end{aligned}
    $$
- en: 'Let $\kappa(\theta)$ be the relative change of the differential of $f$ to the
    change in the parameter space:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: è®©$\kappa(\theta)$è¡¨ç¤º$f$çš„å¾®åˆ†ç›¸å¯¹äºå‚æ•°ç©ºé—´å˜åŒ–çš„ç›¸å¯¹å˜åŒ–ï¼š
- en: $$ \kappa(\theta = \frac{\Delta\big(\nabla_\theta f\big)}{\| \nabla_\theta f(\theta(0))
    \|} = \|\hat{y} - f(\theta(0))\| \frac{\nabla^2_\theta f(\theta(0))}{\| \nabla_\theta
    f(\theta(0)) \|^2} $$
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \kappa(\theta) = \frac{\Delta\big(\nabla_\theta f\big)}{\| \nabla_\theta
    f(\theta(0)) \|} = \|\hat{y} - f(\theta(0))\| \frac{\nabla^2_\theta f(\theta(0))}{\|
    \nabla_\theta f(\theta(0)) \|^2} $$
- en: '[Chizat et al. (2019)](https://arxiv.org/abs/1812.07956) showed the proof for
    a two-layer neural network that $\mathbb{E}[\kappa(\theta_0)] \to 0$ (getting
    into the lazy regime) when the number of hidden neurons $\to \infty$. Also, recommend
    [this post](https://rajatvd.github.io/NTK/) for more discussion on linearized
    models and lazy training.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[Chizat et al. (2019)](https://arxiv.org/abs/1812.07956)è¯æ˜äº†å¯¹äºä¸€ä¸ªä¸¤å±‚ç¥ç»ç½‘ç»œï¼Œå½“éšè—ç¥ç»å…ƒçš„æ•°é‡$\to
    \infty$æ—¶ï¼Œ$\mathbb{E}[\kappa(\theta_0)] \to 0$ï¼ˆè¿›å…¥æ‡’æƒ°çŠ¶æ€ï¼‰ã€‚æ­¤å¤–ï¼Œæ¨èé˜…è¯»[è¿™ç¯‡æ–‡ç« ](https://rajatvd.github.io/NTK/)ä»¥è·å–æ›´å¤šå…³äºçº¿æ€§åŒ–æ¨¡å‹å’Œæ‡’æƒ°è®­ç»ƒçš„è®¨è®ºã€‚'
- en: Citation
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¼•ç”¨
- en: 'Cited as:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: å¼•ç”¨ä¸ºï¼š
- en: Weng, Lilian. (Sep 2022). Some math behind neural tangent kernel. Lilâ€™Log. https://lilianweng.github.io/posts/2022-09-08-ntk/.
  id: totrans-128
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Weng, Lilian. (Sep 2022). Some math behind neural tangent kernel. Lilâ€™Log. https://lilianweng.github.io/posts/2022-09-08-ntk/.
- en: Or
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…
- en: '[PRE0]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: References
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] Jacot et al. [â€œNeural Tangent Kernel: Convergence and Generalization in
    Neural Networks.â€](https://arxiv.org/abs/1806.07572) NeuriPS 2018.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Jacotç­‰äºº [â€œç¥ç»åˆ‡å‘æ ¸ï¼šç¥ç»ç½‘ç»œä¸­çš„æ”¶æ•›å’Œæ³›åŒ–ã€‚â€](https://arxiv.org/abs/1806.07572) NeuriPS
    2018.'
- en: '[2]Radford M. Neal. â€œPriors for Infinite Networks.â€ Bayesian Learning for Neural
    Networks. Springer, New York, NY, 1996\. 29-53.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Radford M. Neal. â€œæ— é™ç½‘ç»œçš„å…ˆéªŒã€‚â€ ç¥ç»ç½‘ç»œçš„è´å¶æ–¯å­¦ä¹ ã€‚Springer, çº½çº¦, çº½çº¦, 1996. 29-53.'
- en: '[3] Lee & Bahri et al. [â€œDeep Neural Networks as Gaussian Processes.â€](https://arxiv.org/abs/1711.00165)
    ICLR 2018.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] æå’Œå·´é‡Œç­‰äºº [â€œæ·±åº¦ç¥ç»ç½‘ç»œä½œä¸ºé«˜æ–¯è¿‡ç¨‹ã€‚â€](https://arxiv.org/abs/1711.00165) ICLR 2018.'
- en: '[4] Chizat et al. [â€œOn Lazy Training in Differentiable Programmingâ€](https://arxiv.org/abs/1812.07956)
    NeuriPS 2019.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Chizatç­‰äºº [â€œå…³äºå¯å¾®ç¼–ç¨‹ä¸­çš„æ‡’æƒ°è®­ç»ƒâ€](https://arxiv.org/abs/1812.07956) NeuriPS 2019.'
- en: '[5] Lee & Xiao, et al. [â€œWide Neural Networks of Any Depth Evolve as Linear
    Models Under Gradient Descent.â€](https://arxiv.org/abs/1902.06720) NeuriPS 2019.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] æå’Œè‚–ç­‰äºº [â€œä»»æ„æ·±åº¦çš„å®½ç¥ç»ç½‘ç»œåœ¨æ¢¯åº¦ä¸‹é™ä¸‹æ¼”å˜ä¸ºçº¿æ€§æ¨¡å‹ã€‚â€](https://arxiv.org/abs/1902.06720) NeuriPS
    2019.'
- en: '[6] Arora, et al. [â€œOn Exact Computation with an Infinitely Wide Neural Net.â€](https://arxiv.org/abs/1904.11955)
    NeurIPS 2019.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Aroraç­‰äºº [â€œå…³äºæ— é™å®½ç¥ç»ç½‘ç»œçš„ç²¾ç¡®è®¡ç®—ã€‚â€](https://arxiv.org/abs/1904.11955) NeurIPS 2019.'
- en: '[7] (YouTube video) [â€œNeural Tangent Kernel: Convergence and Generalization
    in Neural Networksâ€](https://www.youtube.com/watch?v=raT2ECrvbag) by Arthur Jacot,
    Nov 2018.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] (YouTubeè§†é¢‘) [â€œç¥ç»åˆ‡å‘æ ¸ï¼šç¥ç»ç½‘ç»œä¸­çš„æ”¶æ•›å’Œæ³›åŒ–â€](https://www.youtube.com/watch?v=raT2ECrvbag)
    ç”±Arthur Jacot, 2018å¹´11æœˆ.'
- en: '[8] (YouTube video) [â€œLecture 7 - Deep Learning Foundations: Neural Tangent
    Kernelsâ€](https://www.youtube.com/watch?v=DObobAnELkU) by Soheil Feizi, Sep 2020.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] (YouTubeè§†é¢‘) [â€œè®²åº§7 - æ·±åº¦å­¦ä¹ åŸºç¡€ï¼šç¥ç»åˆ‡å‘æ ¸â€](https://www.youtube.com/watch?v=DObobAnELkU)
    ç”±Soheil Feizi, 2020å¹´9æœˆ.'
- en: '[9] [â€œUnderstanding the Neural Tangent Kernel.â€](https://rajatvd.github.io/NTK/)
    Rajatâ€™s Blog.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] [â€œç†è§£ç¥ç»åˆ‡å‘æ ¸ã€‚â€](https://rajatvd.github.io/NTK/) Rajatçš„åšå®¢.'
- en: '[10] [â€œNeural Tangent Kernel.â€](https://appliedprobability.blog/2021/03/10/neural-tangent-kernel/)Applied
    Probability Notes, Mar 2021.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] [â€œç¥ç»åˆ‡å‘æ ¸ã€‚â€](https://appliedprobability.blog/2021/03/10/neural-tangent-kernel/)
    åº”ç”¨æ¦‚ç‡ç¬”è®°, 2021å¹´3æœˆ.'
- en: '[11] [â€œSome Intuition on the Neural Tangent Kernel.â€](https://www.inference.vc/neural-tangent-kernels-some-intuition-for-kernel-gradient-descent/)
    inFERENCe, Nov 2020.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] [â€œå…³äºç¥ç»åˆ‡å‘æ ¸çš„ä¸€äº›ç›´è§‰ã€‚â€](https://www.inference.vc/neural-tangent-kernels-some-intuition-for-kernel-gradient-descent/)
    inFERENCe, 2020å¹´11æœˆ.'
