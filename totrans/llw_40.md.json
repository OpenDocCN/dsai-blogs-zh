["```py\nimport sys from nltk.corpus import stopwords from nltk.tokenize import sent_tokenize   STOP_WORDS = set(stopwords.words('english'))   def get_words(txt):  return filter( lambda x: x not in STOP_WORDS, re.findall(r'\\b(\\w+)\\b', txt) )   def parse_sentence_words(input_file_names):  \"\"\"Returns a list of a list of words. Each sublist is a sentence.\"\"\" sentence_words = [] for file_name in input_file_names: for line in open(file_name): line = line.strip().lower() line = line.decode('unicode_escape').encode('ascii','ignore') sent_words = map(get_words, sent_tokenize(line)) sent_words = filter(lambda sw: len(sw) > 1, sent_words) if len(sent_words) > 1: sentence_words += sent_words return sentence_words   # You would see five .txt files after unzip 'a_song_of_ice_and_fire.zip' input_file_names = [\"001ssb.txt\", \"002ssb.txt\", \"003ssb.txt\",  \"004ssb.txt\", \"005ssb.txt\"] GOT_SENTENCE_WORDS= parse_sentence_words(input_file_names) \n```", "```py\nfrom gensim.models import Word2Vec   # size: the dimensionality of the embedding vectors. # window: the maximum distance between the current and predicted word within a sentence. model = Word2Vec(GOT_SENTENCE_WORDS, size=128, window=3, min_count=5, workers=4) model.wv.save_word2vec_format(\"got_word2vec.txt\", binary=False) \n```", "```py\n@article{weng2017wordembedding,\n  title   = \"Learning word embedding\",\n  author  = \"Weng, Lilian\",\n  journal = \"lilianweng.github.io\",\n  year    = \"2017\",\n  url     = \"https://lilianweng.github.io/posts/2017-10-15-word-embedding/\"\n} \n```"]