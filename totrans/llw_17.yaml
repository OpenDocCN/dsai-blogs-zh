- en: How to Build an Open-Domain Question Answering System?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•æ„å»ºä¸€ä¸ªå¼€æ”¾é¢†åŸŸé—®ç­”ç³»ç»Ÿï¼Ÿ
- en: åŸæ–‡ï¼š[https://lilianweng.github.io/posts/2020-10-29-odqa/](https://lilianweng.github.io/posts/2020-10-29-odqa/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://lilianweng.github.io/posts/2020-10-29-odqa/](https://lilianweng.github.io/posts/2020-10-29-odqa/)
- en: '[Updated on 2020-11-12: add [an example](#openai-api-example) on closed-book
    factual QA using OpenAI API (beta).'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[2020-11-12æ›´æ–°ï¼šæ·»åŠ [ä¸€ä¸ªç¤ºä¾‹](#openai-api-example)ï¼Œå±•ç¤ºä½¿ç”¨OpenAI APIï¼ˆbetaï¼‰è¿›è¡Œé—­åˆä¹¦äº‹å®é—®ç­”çš„ç¤ºä¾‹ã€‚'
- en: A model that can answer any question with regard to factual knowledge can lead
    to many useful and practical applications, such as working as a chatbot or an
    AI assistantğŸ¤–. In this post, we will review several common approaches for building
    such an open-domain question answering system.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªèƒ½å¤Ÿå›ç­”å…³äºäº‹å®çŸ¥è¯†çš„ä»»ä½•é—®é¢˜çš„æ¨¡å‹å¯ä»¥å¯¼è‡´è®¸å¤šæœ‰ç”¨å’Œå®ç”¨çš„åº”ç”¨ï¼Œä¾‹å¦‚ä½œä¸ºèŠå¤©æœºå™¨äººæˆ–AIåŠ©æ‰‹ğŸ¤–ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†å›é¡¾æ„å»ºè¿™æ ·ä¸€ä¸ªå¼€æ”¾é¢†åŸŸé—®ç­”ç³»ç»Ÿçš„å‡ ç§å¸¸è§æ–¹æ³•ã€‚
- en: 'Disclaimers given so many papers in the wild:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: é‰´äºé‡å¤–æœ‰å¦‚æ­¤å¤šçš„è®ºæ–‡ï¼š
- en: Assume we have access to a powerful pretrained [language model](https://lilianweng.github.io/posts/2019-01-31-lm/).
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬å¯ä»¥è®¿é—®ä¸€ä¸ªå¼ºå¤§çš„é¢„è®­ç»ƒ[è¯­è¨€æ¨¡å‹](https://lilianweng.github.io/posts/2019-01-31-lm/)ã€‚
- en: We do not cover how to use structured knowledge base (e.g. Freebase, WikiData)
    here.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸åœ¨è¿™é‡Œè®¨è®ºå¦‚ä½•ä½¿ç”¨ç»“æ„åŒ–çŸ¥è¯†åº“ï¼ˆä¾‹å¦‚Freebaseï¼ŒWikiDataï¼‰ã€‚
- en: We only focus on a single-turn QA instead of a multi-turn conversation style
    QA.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åªå…³æ³¨å•è½®é—®ç­”ï¼Œè€Œä¸æ˜¯å¤šè½®å¯¹è¯å¼é—®ç­”ã€‚
- en: We mostly focus on QA models that contain neural networks, specially Transformer-based
    language models.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸»è¦å…³æ³¨åŒ…å«ç¥ç»ç½‘ç»œçš„é—®ç­”æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åŸºäºTransformerçš„è¯­è¨€æ¨¡å‹ã€‚
- en: I admit that I missed a lot of papers with architectures designed specifically
    for QA tasks between 2017-2019ğŸ˜”
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘æ‰¿è®¤æˆ‘é”™è¿‡äº†å¾ˆå¤šåœ¨2017-2019å¹´é—´ä¸“é—¨è®¾è®¡ç”¨äºé—®ç­”ä»»åŠ¡çš„æ¶æ„çš„è®ºæ–‡ğŸ˜”
- en: What is Open-Domain Question Answering?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯å¼€æ”¾é¢†åŸŸé—®ç­”ï¼Ÿ
- en: '**Open-domain Question Answering (ODQA)** is a type of language tasks, asking
    a model to produce answers to factoid questions in natural language. The true
    answer is objective, so it is simple to evaluate model performance.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¼€æ”¾é¢†åŸŸé—®ç­”ï¼ˆODQAï¼‰**æ˜¯ä¸€ç§è¯­è¨€ä»»åŠ¡ç±»å‹ï¼Œè¦æ±‚æ¨¡å‹ä»¥è‡ªç„¶è¯­è¨€äº§ç”Ÿå¯¹äº‹å®æ€§é—®é¢˜çš„ç­”æ¡ˆã€‚çœŸå®ç­”æ¡ˆæ˜¯å®¢è§‚çš„ï¼Œå› æ­¤è¯„ä¼°æ¨¡å‹æ€§èƒ½å¾ˆç®€å•ã€‚'
- en: For example,
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œ
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The â€œopen-domainâ€ part refers to the lack of the relevant context for any arbitrarily
    asked factual question. In the above case, the model only takes as the input the
    question but no article about â€œwhy Einstein didnâ€™t win a Nobel Prize for the theory
    of relativityâ€ is provided, where the term â€œthe law of the photoelectric effectâ€
    is likely mentioned. In the case when both the question and the context are provided,
    the task is known as **Reading comprehension (RC)**.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: â€œå¼€æ”¾é¢†åŸŸâ€éƒ¨åˆ†æŒ‡çš„æ˜¯å¯¹äºä»»æ„æå‡ºçš„äº‹å®æ€§é—®é¢˜ç¼ºä¹ç›¸å…³èƒŒæ™¯ä¿¡æ¯ã€‚åœ¨ä¸Šè¿°æƒ…å†µä¸‹ï¼Œæ¨¡å‹åªæ¥å—é—®é¢˜ä½œä¸ºè¾“å…¥ï¼Œä½†æ²¡æœ‰æä¾›å…³äºâ€œä¸ºä»€ä¹ˆçˆ±å› æ–¯å¦å› ç›¸å¯¹è®ºæœªè·è¯ºè´å°”å¥–â€çš„æ–‡ç« ï¼Œå…¶ä¸­å¯èƒ½æåˆ°â€œå…‰ç”µæ•ˆåº”å®šå¾‹â€çš„æœ¯è¯­ã€‚å½“æä¾›é—®é¢˜å’Œä¸Šä¸‹æ–‡æ—¶ï¼Œä»»åŠ¡è¢«ç§°ä¸º**é˜…è¯»ç†è§£ï¼ˆRCï¼‰**ã€‚
- en: An ODQA model may work with or without *access to an external source of knowledge*
    (e.g. Wikipedia) and these two conditions are referred to as *open-book* or *closed-book*
    question answering, respectively.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªODQAæ¨¡å‹å¯ä»¥ä½¿ç”¨æˆ–ä¸ä½¿ç”¨*å¤–éƒ¨çŸ¥è¯†æº*ï¼ˆä¾‹å¦‚ç»´åŸºç™¾ç§‘ï¼‰ï¼Œè¿™ä¸¤ç§æ¡ä»¶åˆ†åˆ«ç§°ä¸º*å¼€æ”¾ä¹¦*æˆ–*é—­åˆä¹¦*é—®ç­”ã€‚
- en: 'When considering different types of open-domain questions, I like the classification
    by [Lewis, et al., 2020](https://arxiv.org/abs/2008.02637), in increasing order
    of difficulty:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è€ƒè™‘ä¸åŒç±»å‹çš„å¼€æ”¾é¢†åŸŸé—®é¢˜æ—¶ï¼Œæˆ‘å–œæ¬¢æŒ‰ç…§[Lewisç­‰äººï¼Œ2020](https://arxiv.org/abs/2008.02637)çš„åˆ†ç±»ï¼ŒæŒ‰ç…§éš¾åº¦é€’å¢çš„é¡ºåºï¼š
- en: A model is able to correctly memorize and respond with the answer to a question
    that has been seen at training time.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ¨¡å‹èƒ½å¤Ÿæ­£ç¡®åœ°è®°å¿†å¹¶å›ç­”åœ¨è®­ç»ƒæ—¶è§è¿‡çš„é—®é¢˜çš„ç­”æ¡ˆã€‚
- en: A model is able to answer novel questions at test time and choose an answer
    from the set of answers it has seen during training.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ¨¡å‹èƒ½å¤Ÿåœ¨æµ‹è¯•æ—¶å›ç­”æ–°é¢–çš„é—®é¢˜ï¼Œå¹¶ä»è®­ç»ƒæ—¶è§è¿‡çš„ç­”æ¡ˆé›†ä¸­é€‰æ‹©ä¸€ä¸ªç­”æ¡ˆã€‚
- en: A model is able to answer novel questions which have answers not contained in
    the training dataset.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ¨¡å‹èƒ½å¤Ÿå›ç­”è®­ç»ƒæ•°æ®é›†ä¸­æ²¡æœ‰åŒ…å«ç­”æ¡ˆçš„æ–°é¢–é—®é¢˜ã€‚
- en: '![](../Images/ef5af34b63b5c143825c6fbb409dea14.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ef5af34b63b5c143825c6fbb409dea14.png)'
- en: Fig. 1\. Overview of three frameworks discussed in this post.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1ã€‚æœ¬æ–‡è®¨è®ºçš„ä¸‰ç§æ¡†æ¶æ¦‚è¿°ã€‚
- en: Notation
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¦å·
- en: Given a question $x$ and a ground truth answer span $y$, the context passage
    containing the true answer is labelled as $z \in \mathcal{Z}$, where $\mathcal{Z}$
    is an external knowledge corpus. Wikipedia is a common choice for such an external
    knowledge source.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šä¸€ä¸ªé—®é¢˜$x$å’Œä¸€ä¸ªåœ°é¢çœŸå®ç­”æ¡ˆè·¨åº¦$y$ï¼ŒåŒ…å«çœŸå®ç­”æ¡ˆçš„ä¸Šä¸‹æ–‡æ®µè½è¢«æ ‡è®°ä¸º$z \in \mathcal{Z}$ï¼Œå…¶ä¸­$\mathcal{Z}$æ˜¯ä¸€ä¸ªå¤–éƒ¨çŸ¥è¯†è¯­æ–™åº“ã€‚ç»´åŸºç™¾ç§‘æ˜¯è¿™æ ·ä¸€ä¸ªå¤–éƒ¨çŸ¥è¯†æ¥æºçš„å¸¸è§é€‰æ‹©ã€‚
- en: Concerns of QA data fine-tuning
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: QAæ•°æ®å¾®è°ƒçš„é—®é¢˜
- en: Before we dive into the details of many models below. I would like to point
    out one concern of fine-tuning a model with common QA datasets, which appears
    as one fine-tuning step in several ODQA models. It could be concerning, because
    there is a significant overlap between questions in the train and test sets in
    several public QA datasets.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬æ·±å…¥è®¨è®ºä¸‹é¢è®¸å¤šæ¨¡å‹çš„ç»†èŠ‚ä¹‹å‰ï¼Œæˆ‘æƒ³æŒ‡å‡ºä¸€ä¸ªå…³äºä½¿ç”¨å¸¸è§QAæ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„é—®é¢˜ï¼Œè¿™åœ¨å‡ ä¸ªODQAæ¨¡å‹ä¸­ä½œä¸ºä¸€ä¸ªå¾®è°ƒæ­¥éª¤å‡ºç°ã€‚è¿™å¯èƒ½ä»¤äººæ‹…å¿§ï¼Œå› ä¸ºåœ¨å‡ ä¸ªå…¬å…±QAæ•°æ®é›†ä¸­ï¼Œè®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­çš„é—®é¢˜å­˜åœ¨æ˜¾è‘—é‡å ã€‚
- en: '[Lewis, et al., (2020)](https://arxiv.org/abs/2008.02637) ([code](https://github.com/facebookresearch/QA-Overlap))
    found that 58-71% of test-time answers are also present somewhere in the training
    sets and 28-34% of test-set questions have a near-duplicate paraphrase in their
    corresponding training sets. In their experiments, several models performed notably
    worse when duplicated or paraphrased questions were removed from the training
    set.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[Lewisç­‰äººï¼ˆ2020ï¼‰](https://arxiv.org/abs/2008.02637)ï¼ˆ[ä»£ç ](https://github.com/facebookresearch/QA-Overlap)ï¼‰å‘ç°58-71%çš„æµ‹è¯•æ—¶é—´ç­”æ¡ˆä¹Ÿå‡ºç°åœ¨æŸå¤„çš„è®­ç»ƒé›†ä¸­ï¼Œ28-34%çš„æµ‹è¯•é›†é—®é¢˜åœ¨å…¶å¯¹åº”çš„è®­ç»ƒé›†ä¸­æœ‰è¿‘ä¼¼é‡å¤çš„é‡Šä¹‰ã€‚åœ¨ä»–ä»¬çš„å®éªŒä¸­ï¼Œå½“ä»è®­ç»ƒé›†ä¸­åˆ é™¤é‡å¤æˆ–é‡Šä¹‰é—®é¢˜æ—¶ï¼Œå‡ ä¸ªæ¨¡å‹çš„è¡¨ç°æ˜æ˜¾è¾ƒå·®ã€‚'
- en: 'Open-book QA: Retriever-Reader'
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¼€æ”¾å¼é—®ç­”ï¼šæ£€ç´¢å™¨-é˜…è¯»å™¨
- en: Given a factoid question, if a language model has no context or is not big enough
    to memorize the context which exists in the training dataset, it is unlikely to
    guess the correct answer. In an open-book exam, students are allowed to refer
    to external resources like notes and books while answering test questions. Similarly,
    a ODQA system can be paired with a rich knowledge base to identify relevant documents
    as evidence of answers.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šä¸€ä¸ªäº‹å®æ€§é—®é¢˜ï¼Œå¦‚æœè¯­è¨€æ¨¡å‹æ²¡æœ‰ä¸Šä¸‹æ–‡æˆ–ä¸è¶³å¤Ÿå¤§ä»¥è®°ä½è®­ç»ƒæ•°æ®é›†ä¸­å­˜åœ¨çš„ä¸Šä¸‹æ–‡ï¼Œé‚£ä¹ˆçŒœæµ‹æ­£ç¡®ç­”æ¡ˆçš„å¯èƒ½æ€§å¾ˆå°ã€‚åœ¨å¼€å·è€ƒè¯•ä¸­ï¼Œå­¦ç”Ÿå¯ä»¥åœ¨å›ç­”æµ‹è¯•é—®é¢˜æ—¶å‚è€ƒå¤–éƒ¨èµ„æºï¼Œå¦‚ç¬”è®°å’Œä¹¦ç±ã€‚ç±»ä¼¼åœ°ï¼ŒODQAç³»ç»Ÿå¯ä»¥ä¸ä¸°å¯Œçš„çŸ¥è¯†åº“é…å¯¹ï¼Œä»¥è¯†åˆ«ç›¸å…³æ–‡æ¡£ä½œä¸ºç­”æ¡ˆçš„è¯æ®ã€‚
- en: We can decompose the process of finding answers to given questions into two
    stages,
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å°†å›ç­”ç»™å®šé—®é¢˜çš„è¿‡ç¨‹åˆ†è§£ä¸ºä¸¤ä¸ªé˜¶æ®µï¼Œ
- en: Find the related context in an external repository of knowledge;
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨å¤–éƒ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç›¸å…³å†…å®¹ï¼›
- en: Process the retrieved context to *extract* an answer.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¤„ç†æ£€ç´¢åˆ°çš„å†…å®¹ä»¥*æå–*ç­”æ¡ˆã€‚
- en: '![](../Images/31e9dac723bf29e2d59178871b39a297.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31e9dac723bf29e2d59178871b39a297.png)'
- en: Fig. 2\. The retriever-reader QA framework combines information retrieval with
    machine reading comprehension.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2. æ£€ç´¢å™¨-é˜…è¯»å™¨é—®ç­”æ¡†æ¶å°†ä¿¡æ¯æ£€ç´¢ä¸æœºå™¨é˜…è¯»ç†è§£ç»“åˆèµ·æ¥ã€‚
- en: Such a retriever + reader framework was first proposed in **DrQA** (â€œDocument
    retriever Question-Answeringâ€ by [Chen et al., 2017](https://arxiv.org/abs/1704.00051);
    [code](https://github.com/facebookresearch/DrQA)). The retriever and the reader
    components can be set up and trained independently, or jointly trained [end-to-end](#end-to-end-joint-training).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·çš„æ£€ç´¢å™¨+é˜…è¯»å™¨æ¡†æ¶æœ€åˆæ˜¯ç”±**DrQA**ï¼ˆç”±[Chenç­‰äººï¼Œ2017](https://arxiv.org/abs/1704.00051)æå‡ºï¼›[ä»£ç ](https://github.com/facebookresearch/DrQA)ï¼‰æå‡ºçš„ã€‚æ£€ç´¢å™¨å’Œé˜…è¯»å™¨ç»„ä»¶å¯ä»¥ç‹¬ç«‹è®¾ç½®å’Œè®­ç»ƒï¼Œæˆ–è€…è¿›è¡Œè”åˆè®­ç»ƒ[ç«¯åˆ°ç«¯](#end-to-end-joint-training)ã€‚
- en: Retriever Model
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ£€ç´¢å™¨æ¨¡å‹
- en: Two popular approaches for implementing the retriever is to use the information
    retrieval (IR) system that depends on (1) the classic non-learning-based [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)
    features (â€œclassic IRâ€) or (2) dense embedding vectors of text produced by neural
    networks (â€œneural IRâ€).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å®ç°æ£€ç´¢å™¨çš„ä¸¤ç§æµè¡Œæ–¹æ³•æ˜¯ä½¿ç”¨ä¾èµ–äºï¼ˆ1ï¼‰ç»å…¸éå­¦ä¹ å‹[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)ç‰¹å¾çš„ä¿¡æ¯æ£€ç´¢ï¼ˆâ€œç»å…¸IRâ€ï¼‰æˆ–ï¼ˆ2ï¼‰ç”±ç¥ç»ç½‘ç»œç”Ÿæˆçš„æ–‡æœ¬çš„å¯†é›†åµŒå…¥å‘é‡çš„å¯†é›†åµŒå…¥å‘é‡ï¼ˆâ€œç¥ç»IRâ€ï¼‰ã€‚
- en: Classic IR
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç»å…¸IR
- en: '**DrQA** ([Chen et al., 2017](https://arxiv.org/abs/1704.00051)) adopts an
    efficient non-learning-based search engine based on the [vector space model](https://en.wikipedia.org/wiki/Vector_space_model).
    Every query and document is modelled as a bag-of-word vector, where each term
    is weighted by TF-IDF (term frequency $\times$ inverse document frequency).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**DrQA**ï¼ˆ[Chenç­‰äººï¼Œ2017](https://arxiv.org/abs/1704.00051)ï¼‰é‡‡ç”¨äº†åŸºäº[å‘é‡ç©ºé—´æ¨¡å‹](https://en.wikipedia.org/wiki/Vector_space_model)çš„é«˜æ•ˆéå­¦ä¹ å‹æœç´¢å¼•æ“ã€‚æ¯ä¸ªæŸ¥è¯¢å’Œæ–‡æ¡£éƒ½è¢«å»ºæ¨¡ä¸ºè¯è¢‹å‘é‡ï¼Œå…¶ä¸­æ¯ä¸ªæœ¯è¯­éƒ½ç”±TF-IDFï¼ˆè¯é¢‘$\times$é€†æ–‡æ¡£é¢‘ç‡ï¼‰åŠ æƒã€‚'
- en: '$$ \begin{aligned} \text{tf-idf}(t, d, \mathcal{D}) &= \text{tf}(t, d) \times
    \text{idf}(t, \mathcal{D}) \\ \text{tf}(t, d) &= \log(1 + \text{freq}(t, d)) \\
    \text{idf}(t, \mathcal{D}) &= \log \Big( \frac{\vert\mathcal{D}\vert}{\vert d\in\mathcal{D}:
    t\in d\vert} \Big) \end{aligned} $$'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '$$ \begin{aligned} \text{tf-idf}(t, d, \mathcal{D}) &= \text{tf}(t, d) \times
    \text{idf}(t, \mathcal{D}) \\ \text{tf}(t, d) &= \log(1 + \text{freq}(t, d)) \\
    \text{idf}(t, \mathcal{D}) &= \log \Big( \frac{\vert\mathcal{D}\vert}{\vert d\in\mathcal{D}:
    t\in d\vert} \Big) \end{aligned} $$'
- en: where $t$ is a unigram or bigram term in a document $d$ from a collection of
    documents $\mathcal{D}$ . $\text{freq}(t, d)$ measures how many times a term $t$
    appears in $d$. Note that the term-frequency here includes bigram counts too,
    which is found to be very helpful because the local word order is taken into consideration
    via bigrams. As part of the implementation, DrQA maps the bigrams of $2^{24}$
    bins using unsigned murmur3 hash.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­$t$æ˜¯æ¥è‡ªæ–‡æ¡£$d$çš„å•ä¸ªè¯æˆ–åŒè¯æœ¯è¯­ï¼Œæ¥è‡ªæ–‡æ¡£é›†åˆ$\mathcal{D}$ã€‚$\text{freq}(t, d)$è¡¡é‡æœ¯è¯­$t$åœ¨$d$ä¸­å‡ºç°çš„æ¬¡æ•°ã€‚è¯·æ³¨æ„ï¼Œè¿™é‡Œçš„è¯é¢‘ä¹ŸåŒ…æ‹¬åŒè¯è®¡æ•°ï¼Œè¿™æ˜¯éå¸¸æœ‰å¸®åŠ©çš„ï¼Œå› ä¸ºé€šè¿‡åŒè¯è€ƒè™‘äº†å±€éƒ¨è¯åºã€‚ä½œä¸ºå®ç°çš„ä¸€éƒ¨åˆ†ï¼ŒDrQAä½¿ç”¨æ— ç¬¦å·murmur3å“ˆå¸Œå°†$2^{24}$ä¸ªbinçš„åŒè¯æ˜ å°„ã€‚
- en: Precisely, DrQA implemented Wikipedia as its knowledge source and this choice
    has became a default setting for many ODQA studies since then. The non-ML document
    retriever returns the top $k=5$ most relevant Wikipedia articles given a question.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å…·ä½“æ¥è¯´ï¼ŒDrQAå°†ç»´åŸºç™¾ç§‘ä½œä¸ºå…¶çŸ¥è¯†æ¥æºï¼Œè¿™ä¸ªé€‰æ‹©è‡ªé‚£æ—¶ä»¥æ¥å·²æˆä¸ºè®¸å¤šODQAç ”ç©¶çš„é»˜è®¤è®¾ç½®ã€‚éæœºå™¨å­¦ä¹ æ–‡æ¡£æ£€ç´¢å™¨åœ¨ç»™å®šé—®é¢˜çš„æƒ…å†µä¸‹è¿”å›å‰$k=5$ä¸ªæœ€ç›¸å…³çš„ç»´åŸºç™¾ç§‘æ–‡ç« ã€‚
- en: '**BERTserini** ([Yang et al., 2019](https://arxiv.org/abs/1902.01718)) pairs
    the open-source [*Anserini*](https://github.com/castorini/anserini) IR toolkit
    as the retriever with a fine-tuned pre-trained BERT model as the reader. The top
    $k$ documents ($k=10$) are retrieved via the `post-v3.0` branch of Anserini with
    the query treated as a bag of words. The retrieved text segments are ranked by
    [BM25](https://en.wikipedia.org/wiki/Okapi_BM25), a classic TF-IDF-based retrieval
    scoring function. In terms of the effect of text granularity on performance, they
    found that paragraph retrieval > sentence retrieval > article retrieval.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**BERTserini**ï¼ˆ[Yangç­‰ï¼Œ2019](https://arxiv.org/abs/1902.01718)ï¼‰å°†å¼€æº[*Anserini*](https://github.com/castorini/anserini)ä¿¡æ¯æ£€ç´¢å·¥å…·åŒ…ä¸ç»è¿‡å¾®è°ƒçš„é¢„è®­ç»ƒBERTæ¨¡å‹é…å¯¹ä½œä¸ºé˜…è¯»å™¨ã€‚é€šè¿‡`post-v3.0`åˆ†æ”¯çš„Anseriniæ£€ç´¢å‰$k$ä¸ªæ–‡æ¡£ï¼ˆ$k=10$ï¼‰ï¼Œå°†æŸ¥è¯¢è§†ä¸ºè¯è¢‹ã€‚æ£€ç´¢åˆ°çš„æ–‡æœ¬æ®µé€šè¿‡[BM25](https://en.wikipedia.org/wiki/Okapi_BM25)è¿›è¡Œæ’åï¼Œè¿™æ˜¯ä¸€ç§ç»å…¸çš„åŸºäºTF-IDFçš„æ£€ç´¢è¯„åˆ†å‡½æ•°ã€‚åœ¨æ–‡æœ¬ç²’åº¦å¯¹æ€§èƒ½çš„å½±å“æ–¹é¢ï¼Œä»–ä»¬å‘ç°æ®µè½æ£€ç´¢
    > å¥å­æ£€ç´¢ > æ–‡ç« æ£€ç´¢ã€‚'
- en: '![](../Images/029b4c5dba542aa068e30d1ad6a2b08e.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/029b4c5dba542aa068e30d1ad6a2b08e.png)'
- en: 'Fig. 3\. An illustration of BERTserini architecture. (Image source: [Yang et
    al., 2019](https://arxiv.org/abs/1902.01718))'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3\. BERTseriniæ¶æ„ç¤ºæ„å›¾ã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼š[Yangç­‰ï¼Œ2019](https://arxiv.org/abs/1902.01718)ï¼‰
- en: '*ElasticSearch + BM25* is used by the **Multi-passage BERT** QA model ([Wang
    et al., 2019](https://arxiv.org/abs/1908.08167)). They found that splitting articles
    into passages with the length of 100 words by *sliding window* brings 4% improvements,
    since splitting documents into passages without overlap may cause some near-boundary
    evidence to lose useful contexts.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*ElasticSearch + BM25*è¢«**å¤šæ®µBERT**é—®ç­”æ¨¡å‹æ‰€ä½¿ç”¨ï¼ˆ[Wangç­‰ï¼Œ2019](https://arxiv.org/abs/1908.08167)ï¼‰ã€‚ä»–ä»¬å‘ç°ï¼Œé€šè¿‡*æ»‘åŠ¨çª—å£*å°†æ–‡ç« åˆ†å‰²æˆé•¿åº¦ä¸º100ä¸ªå•è¯çš„æ®µè½å¯ä»¥å¸¦æ¥4%çš„æ”¹è¿›ï¼Œå› ä¸ºå°†æ–‡æ¡£åˆ†å‰²æˆæ²¡æœ‰é‡å çš„æ®µè½å¯èƒ½å¯¼è‡´ä¸€äº›æ¥è¿‘è¾¹ç•Œçš„è¯æ®å¤±å»æœ‰ç”¨çš„ä¸Šä¸‹æ–‡ã€‚'
- en: Neural IR
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¥ç»ä¿¡æ¯æ£€ç´¢
- en: There is a long history in learning a low-dimensional representation of text,
    denser than raw term-based vectors ([Deerwester et al., 1990](http://lsa.colorado.edu/papers/JASIS.lsi.90.pdf);
    [Yih, et al., 2011](https://www.aclweb.org/anthology/W11-0329/)). Dense representations
    can be learned through matrix decomposition or some neural network architectures
    (e.g. MLP, LSTM, bidirectional LSTM, etc). When involving neural networks, such
    approaches are referred to as â€œNeural IRâ€, Neural IR is a new category of methods
    for retrieval problems, but it is not necessary to perform better/superior than
    classic IR ([Lim, 2018](https://sigir.org/wp-content/uploads/2019/01/p040.pdf)).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å­¦ä¹ æ–‡æœ¬çš„ä½ç»´è¡¨ç¤ºå·²æœ‰å¾ˆé•¿çš„å†å²ï¼Œæ¯”åŸå§‹åŸºäºæœ¯è¯­çš„å‘é‡æ›´å¯†é›†ï¼ˆ[Deerwesterç­‰ï¼Œ1990](http://lsa.colorado.edu/papers/JASIS.lsi.90.pdf);
    [Yihç­‰ï¼Œ2011](https://www.aclweb.org/anthology/W11-0329/)ï¼‰ã€‚å¯†é›†è¡¨ç¤ºå¯ä»¥é€šè¿‡çŸ©é˜µåˆ†è§£æˆ–ä¸€äº›ç¥ç»ç½‘ç»œæ¶æ„ï¼ˆä¾‹å¦‚MLPï¼ŒLSTMï¼ŒåŒå‘LSTMç­‰ï¼‰å­¦ä¹ ã€‚å½“æ¶‰åŠç¥ç»ç½‘ç»œæ—¶ï¼Œè¿™äº›æ–¹æ³•è¢«ç§°ä¸ºâ€œç¥ç»ä¿¡æ¯æ£€ç´¢â€ï¼Œç¥ç»ä¿¡æ¯æ£€ç´¢æ˜¯æ£€ç´¢é—®é¢˜çš„æ–°ç±»åˆ«æ–¹æ³•ï¼Œä½†ä¸ä¸€å®šæ¯”ç»å…¸ä¿¡æ¯æ£€ç´¢è¡¨ç°æ›´å¥½/æ›´ä¼˜è¶Šï¼ˆ[Limï¼Œ2018](https://sigir.org/wp-content/uploads/2019/01/p040.pdf)ï¼‰ã€‚
- en: 'After the success of many large-scale [general language models](https://lilianweng.github.io/posts/2019-01-31-lm/),
    many QA models embrace the following approach:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®¸å¤šå¤§è§„æ¨¡[é€šç”¨è¯­è¨€æ¨¡å‹](https://lilianweng.github.io/posts/2019-01-31-lm/)å–å¾—æˆåŠŸä¹‹åï¼Œè®¸å¤šé—®ç­”æ¨¡å‹é‡‡ç”¨ä»¥ä¸‹æ–¹æ³•ï¼š
- en: $$ h_x = E_x(x)\quad h_z = E_z(z)\quad \text{score}(x, z) = h_x^\top h_z $$
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: $$ h_x = E_x(x)\quad h_z = E_z(z)\quad \text{score}(x, z) = h_x^\top h_z $$
- en: Extract the dense representations of a question $x$ and a context passage $z$
    by feeding them into a language model;
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€šè¿‡å°†é—®é¢˜$x$å’Œä¸Šä¸‹æ–‡æ®µè½$z$çš„å¯†é›†è¡¨ç¤ºé¦ˆé€åˆ°è¯­è¨€æ¨¡å‹ä¸­æ¥æå–ï¼›
- en: Use the dot-product of these two representations as the retrieval score to rank
    and select most relevant passages.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¿™ä¸¤ä¸ªè¡¨ç¤ºçš„ç‚¹ç§¯ä½œä¸ºæ£€ç´¢åˆ†æ•°æ¥æ’åå’Œé€‰æ‹©æœ€ç›¸å…³çš„æ®µè½ã€‚
- en: ORQA, REALM and DPR all use such a scoring function for context retrieval, which
    will be described in detail in a [later section](#end-to-end-joint-training) on
    the end-to-end QA model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ORQAã€REALM å’Œ DPR éƒ½ä½¿ç”¨è¿™æ ·ä¸€ä¸ªç”¨äºä¸Šä¸‹æ–‡æ£€ç´¢çš„è¯„åˆ†å‡½æ•°ï¼Œå°†åœ¨[åç»­éƒ¨åˆ†](#end-to-end-joint-training)ä¸­è¯¦ç»†æè¿°ç«¯åˆ°ç«¯
    QA æ¨¡å‹ã€‚
- en: An extreme approach, investigated by **DenSPI** (â€œDense-Sparse Phrase Indexâ€;
    [Seo et al., 2019](https://arxiv.org/abs/1906.05807)), is to encode all the text
    in the knowledge corpus at the *phrase* level and then only rely on the retriever
    to identify the most relevant phrase as the predicted answer. In this way, the
    retriever+reader pipeline is reduced to only retriever. Of course, the index would
    be much larger and the retrieval problem is more challenging.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§æç«¯çš„æ–¹æ³•ï¼Œç”±**DenSPI**ï¼ˆâ€œDense-Sparse Phrase Indexâ€; [Seo et al., 2019](https://arxiv.org/abs/1906.05807)ï¼‰è°ƒæŸ¥ï¼Œæ˜¯åœ¨çŸ¥è¯†è¯­æ–™åº“ä¸­ä»¥*çŸ­è¯­*çº§åˆ«ç¼–ç æ‰€æœ‰æ–‡æœ¬ï¼Œç„¶åä»…ä¾èµ–æ£€ç´¢å™¨è¯†åˆ«æœ€ç›¸å…³çš„çŸ­è¯­ä½œä¸ºé¢„æµ‹ç­”æ¡ˆã€‚è¿™æ ·ï¼Œæ£€ç´¢å™¨+é˜…è¯»å™¨æµæ°´çº¿ä»…å‡å°‘åˆ°æ£€ç´¢å™¨ã€‚å½“ç„¶ï¼Œç´¢å¼•ä¼šæ›´å¤§ï¼Œæ£€ç´¢é—®é¢˜æ›´å…·æŒ‘æˆ˜æ€§ã€‚
- en: DenSPI introduces a *query-agnostic* indexable representation of document phrases.
    Precisely it encodes query-agnostic representations of text spans in Wikipedia
    offline and looks for the answer at inference time by performing nearest neighbor
    search. It can drastically speed up the inference time, because there is no need
    to re-encode documents for every new query, which is often required by a reader
    model.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: DenSPI å¼•å…¥äº†ä¸€ä¸ªæ–‡æ¡£çŸ­è¯­çš„*æŸ¥è¯¢æ— å…³*å¯ç´¢å¼•è¡¨ç¤ºã€‚å‡†ç¡®åœ°è¯´ï¼Œå®ƒåœ¨ç»´åŸºç™¾ç§‘ç¦»çº¿ä¸­ç¼–ç æ–‡æœ¬è·¨åº¦çš„æŸ¥è¯¢æ— å…³è¡¨ç¤ºï¼Œå¹¶åœ¨æ¨ç†æ—¶é€šè¿‡æ‰§è¡Œæœ€è¿‘é‚»æœç´¢æ¥æŸ¥æ‰¾ç­”æ¡ˆã€‚è¿™å¯ä»¥æå¤§åœ°åŠ å¿«æ¨ç†æ—¶é—´ï¼Œå› ä¸ºä¸éœ€è¦ä¸ºæ¯ä¸ªæ–°æŸ¥è¯¢é‡æ–°ç¼–ç æ–‡æ¡£ï¼Œè¿™é€šå¸¸æ˜¯è¯»è€…æ¨¡å‹æ‰€è¦æ±‚çš„ã€‚
- en: 'Given a question $x$ and a fixed set of (Wikipedia) documents, $z_1, \dots,
    z_K$ and each document $z_k$ contains $N_k$ words, $z_k = \langle z_k^{(1)}, \dots,
    z_k^{(N_k)}\rangle$. An ODQA model is a scoring function $F$ for each candidate
    phrase span $z_k^{(i:j)}, 1 \leq i \leq j \leq N_k$, such that the truth answer
    is the phrase with maximum score: $y = {\arg\max}_{k,i,j} F(x, z_k^{(i:j)})$.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šä¸€ä¸ªé—®é¢˜ $x$ å’Œä¸€ç»„å›ºå®šçš„ï¼ˆç»´åŸºç™¾ç§‘ï¼‰æ–‡æ¡£ï¼Œ$z_1, \dots, z_K$ï¼Œæ¯ä¸ªæ–‡æ¡£ $z_k$ åŒ…å« $N_k$ ä¸ªè¯ï¼Œ$z_k = \langle
    z_k^{(1)}, \dots, z_k^{(N_k)}\rangle$ã€‚ä¸€ä¸ª ODQA æ¨¡å‹æ˜¯ä¸€ä¸ªä¸ºæ¯ä¸ªå€™é€‰çŸ­è¯­è·¨åº¦ $z_k^{(i:j)}, 1 \leq
    i \leq j \leq N_k$ è®¡ç®—å¾—åˆ†çš„å‡½æ•° $F$ï¼Œä½¿å¾—çœŸå®ç­”æ¡ˆæ˜¯å¾—åˆ†æœ€é«˜çš„çŸ­è¯­ï¼š$y = {\arg\max}_{k,i,j} F(x, z_k^{(i:j)})$ã€‚
- en: 'The phrase representation $z_k^{(i:j)}$ combines both dense and sparse vectors,
    $z_k^{(i:j)} = [d_k^{(i:j)}, s_k^{(i:j)}] \in \mathbb{R}^{d^d + d^s}$ (note that
    $d^d \ll d^s$):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: çŸ­è¯­è¡¨ç¤º $z_k^{(i:j)}$ ç»“åˆäº†å¯†é›†å‘é‡å’Œç¨€ç–å‘é‡ï¼Œ$z_k^{(i:j)} = [d_k^{(i:j)}, s_k^{(i:j)}] \in
    \mathbb{R}^{d^d + d^s}$ï¼ˆæ³¨æ„ $d^d \ll d^s$ï¼‰ï¼š
- en: The dense vector $d_k^{(i:j)}$ is effective for encoding local *syntactic* and
    *semantic* cues, as what can be learned by a pretrained language model.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯†é›†å‘é‡ $d_k^{(i:j)}$ æœ‰æ•ˆåœ°ç¼–ç äº†æœ¬åœ°*å¥æ³•*å’Œ*è¯­ä¹‰*çº¿ç´¢ï¼Œå°±åƒé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æ‰€å­¦åˆ°çš„é‚£æ ·ã€‚
- en: The sparse vector $s_k^{(i:j)}$ is superior at encoding precise *lexical* information.
    The sparse vector is term-frequency-based encoding. DenSPI uses 2-gram term-frequency
    same as DrQA, resulting a highly sparse representation ($d^s \approx 16$M)
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¨€ç–å‘é‡ $s_k^{(i:j)}$ åœ¨ç¼–ç ç²¾ç¡®çš„*è¯æ±‡*ä¿¡æ¯æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚è¿™ä¸ªç¨€ç–å‘é‡æ˜¯åŸºäºè¯é¢‘çš„ç¼–ç ã€‚DenSPI ä½¿ç”¨ä¸ DrQA ç›¸åŒçš„ 2-gram
    è¯é¢‘ï¼Œå¯¼è‡´é«˜åº¦ç¨€ç–çš„è¡¨ç¤ºï¼ˆ$d^s \approx 16$Mï¼‰ã€‚
- en: The dense vector $d^{(i:j)}$ is further decomposed into three parts, $d^{(i:j)}
    = [a_i, b_j, c_{ij}] \in \mathbb{R}^{2d^b + 1}$ where $2d^b + 1 = d^d$. All three
    components are learned based on different columns of the fine-tuned BERT representations.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å¯†é›†å‘é‡ $d^{(i:j)}$ è¿›ä¸€æ­¥åˆ†è§£ä¸ºä¸‰éƒ¨åˆ†ï¼Œ$d^{(i:j)} = [a_i, b_j, c_{ij}] \in \mathbb{R}^{2d^b
    + 1}$ï¼Œå…¶ä¸­ $2d^b + 1 = d^d$ã€‚æ‰€æœ‰ä¸‰ä¸ªç»„ä»¶éƒ½æ˜¯åŸºäºå¾®è°ƒåçš„ BERT è¡¨ç¤ºå­¦ä¹ çš„ã€‚
- en: A vector $a_i$ encodes the *start* position for the $i$-th word of the document;
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‘é‡ $a_i$ ç¼–ç æ–‡æ¡£ä¸­ç¬¬ $i$ ä¸ªè¯çš„*èµ·å§‹*ä½ç½®ï¼›
- en: A vector $b_j$ encodes the *end* position for the $j$-th word of the document;
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‘é‡ $b_j$ ç¼–ç æ–‡æ¡£ä¸­ç¬¬ $j$ ä¸ªè¯çš„*ç»“æŸ*ä½ç½®ï¼›
- en: A scalar $c_{ij}$ measures the *coherency* between the start and the end vectors,
    helping avoid non-constituent phrases during inference.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ ‡é‡ $c_{ij}$ è¡¡é‡äº†èµ·å§‹å‘é‡å’Œç»“æŸå‘é‡ä¹‹é—´çš„*è¿è´¯æ€§*ï¼Œæœ‰åŠ©äºåœ¨æ¨ç†è¿‡ç¨‹ä¸­é¿å…éæˆåˆ†çŸ­è¯­ã€‚
- en: For all possible $(i,j,k)$ tuples where $j-i < J$, the text span embeddings
    are precomputed and stored as a *phrase index*. The maximum span length $J$ is
    a predefined scalar constant.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ‰€æœ‰å¯èƒ½çš„ $(i,j,k)$ å…ƒç»„ï¼Œå…¶ä¸­ $j-i < J$ï¼Œæ–‡æœ¬è·¨åº¦åµŒå…¥è¢«é¢„å…ˆè®¡ç®—å¹¶å­˜å‚¨ä¸º*çŸ­è¯­ç´¢å¼•*ã€‚æœ€å¤§è·¨åº¦é•¿åº¦ $J$ æ˜¯ä¸€ä¸ªé¢„å®šä¹‰çš„æ ‡é‡å¸¸æ•°ã€‚
- en: '![](../Images/ac4d275deeb39bc73a001ce0bdc33c29.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac4d275deeb39bc73a001ce0bdc33c29.png)'
- en: 'Fig. 4\. An illustration of Dense-Sparse Phrase Index (DenSPI) architecture.
    (Image source: [Seo et al., 2019](https://arxiv.org/abs/1906.05807))'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾4\. Dense-Sparse Phrase Indexï¼ˆDenSPIï¼‰æ¶æ„ç¤ºæ„å›¾ã€‚ (å›¾ç‰‡æ¥æº: [Seo et al., 2019](https://arxiv.org/abs/1906.05807))'
- en: At the inference time, the question is mapped into the same vector space $x=[dâ€™,
    sâ€™] \in \mathbb{R}^{d^d + d^s}$, where the dense vector $dâ€™$ is extracted from
    the BERT embedding of the special `[CLS]` symbol. The same BERT model is shared
    for encoding both questions and phrases. The final answer is predicted by $k^*,
    i^*, j^* = \arg\max x^\top z_k^{(i:j)}$.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨ç†æ—¶ï¼Œé—®é¢˜è¢«æ˜ å°„åˆ°ç›¸åŒçš„å‘é‡ç©ºé—´ $x=[dâ€™, sâ€™] \in \mathbb{R}^{d^d + d^s}$ï¼Œå…¶ä¸­å¯†é›†å‘é‡ $dâ€™$ æ˜¯ä»ç‰¹æ®Šçš„
    `[CLS]` ç¬¦å·çš„BERTåµŒå…¥ä¸­æå–çš„ã€‚ç›¸åŒçš„BERTæ¨¡å‹ç”¨äºç¼–ç é—®é¢˜å’ŒçŸ­è¯­ã€‚æœ€ç»ˆç­”æ¡ˆç”± $k^*, i^*, j^* = \arg\max x^\top
    z_k^{(i:j)}$ é¢„æµ‹ã€‚
- en: Reader Model
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯»è€…æ¨¡å‹
- en: The reader model learns to solve the reading comprehension task â€” extract an
    answer for a given question from a given context document. Here we only discuss
    approaches for machine comprehension using neural networks.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: è¯»è€…æ¨¡å‹å­¦ä¹ è§£å†³é˜…è¯»ç†è§£ä»»åŠ¡ â€” ä»ç»™å®šä¸Šä¸‹æ–‡æ–‡æ¡£ä¸­æå–é—®é¢˜çš„ç­”æ¡ˆã€‚è¿™é‡Œæˆ‘ä»¬åªè®¨è®ºä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œæœºå™¨ç†è§£çš„æ–¹æ³•ã€‚
- en: Bi-directional LSTM
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åŒå‘ LSTM
- en: 'The reader model for answer detection of **DrQA** ([Chen et al., 2017](https://arxiv.org/abs/1704.00051))
    is a 3-layer bidirectional LSTM with hidden size 128\. Every relevant paragraph
    of retrieved Wikipedia articles is encoded by a sequence of feature vector, $\{\tilde{\mathbf{z}}_1,
    \dots, \tilde{\mathbf{z}}_m \}$. Each feature vector $\hat{\mathbf{z}}_i \in \mathbb{R}^{d_z}$
    is expected to capture useful contextual information around one token $z_i$. The
    feature consists of several categories of features:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**DrQA**çš„ç­”æ¡ˆæ£€æµ‹è¯»è€…æ¨¡å‹ï¼ˆ[Chen et al., 2017](https://arxiv.org/abs/1704.00051)ï¼‰æ˜¯ä¸€ä¸ªå…·æœ‰éšè—å¤§å°ä¸º128çš„3å±‚åŒå‘LSTMã€‚æ£€ç´¢åˆ°çš„ç»´åŸºç™¾ç§‘æ–‡ç« çš„æ¯ä¸ªç›¸å…³æ®µè½éƒ½è¢«ç¼–ç ä¸ºç‰¹å¾å‘é‡åºåˆ—ï¼Œ$\{\tilde{\mathbf{z}}_1,
    \dots, \tilde{\mathbf{z}}_m \}$ã€‚æ¯ä¸ªç‰¹å¾å‘é‡ $\hat{\mathbf{z}}_i \in \mathbb{R}^{d_z}$
    é¢„è®¡èƒ½å¤Ÿæ•è·å›´ç»•ä¸€ä¸ªæ ‡è®° $z_i$ çš„æœ‰ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¯¥ç‰¹å¾åŒ…æ‹¬å‡ ç±»ç‰¹å¾ï¼š'
- en: 'Word embeddings: A 300d [Glove](https://lilianweng.github.io/posts/2017-10-15-word-embedding/#glove-global-vectors)
    word embedding trained from 800B Web crawl data, $f_\text{embed} = E_g(z_i)$.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¯åµŒå…¥ï¼šä»800B Webçˆ¬å–æ•°æ®è®­ç»ƒçš„300d [Glove](https://lilianweng.github.io/posts/2017-10-15-word-embedding/#glove-global-vectors)è¯åµŒå…¥ï¼Œ$f_\text{embed}
    = E_g(z_i)$ã€‚
- en: 'Exact match: Whether a word $z_i$ appears in the question $x$, $f_\text{match}
    = \mathbb{I}(z_i \in x)$.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç²¾ç¡®åŒ¹é…ï¼šä¸€ä¸ªè¯ $z_i$ æ˜¯å¦å‡ºç°åœ¨é—®é¢˜ $x$ ä¸­ï¼Œ$f_\text{match} = \mathbb{I}(z_i \in x)$ã€‚
- en: 'Token features: This includes POS (part-of-speech) tagging, NER (named entity
    recognition), and TF (term-frequency), $f_\text{token}(z_i) = (\text{POS}(z_i),
    \text{NER}(z_i), \text{TF}(z_i))$.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ ‡è®°ç‰¹å¾ï¼šåŒ…æ‹¬POSï¼ˆè¯æ€§æ ‡æ³¨ï¼‰ã€NERï¼ˆå‘½åå®ä½“è¯†åˆ«ï¼‰å’ŒTFï¼ˆè¯é¢‘ï¼‰ï¼Œ$f_\text{token}(z_i) = (\text{POS}(z_i),
    \text{NER}(z_i), \text{TF}(z_i))$ã€‚
- en: 'Aligned question embedding: The attention score $y_{ij}$ is designed to capture
    inter-sentence matching and similarity between the paragraph token $z_i$ and the
    question word $x_j$. This feature adds soft alignments between similar but non-identical
    words.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹é½çš„é—®é¢˜åµŒå…¥ï¼šæ³¨æ„åŠ›åˆ†æ•° $y_{ij}$ è®¾è®¡ç”¨äºæ•è·å¥å­é—´åŒ¹é…å’Œæ®µè½æ ‡è®° $z_i$ ä¸é—®é¢˜è¯ $x_j$ ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚è¯¥ç‰¹å¾å¢åŠ äº†ç±»ä¼¼ä½†éç›¸åŒå•è¯ä¹‹é—´çš„è½¯å¯¹é½ã€‚
- en: $$ \begin{aligned} f_\text{align}(z_i) &= \sum_j y_{i,j} E_g(x_j) \\ y_{i,j}
    &= \frac{\exp(\alpha(E_g(z_i))^\top \alpha(E_g(x_j)) )}{\sum_{j'} \exp(\alpha(E_g(z_i))^\top
    \alpha(E_g(x_{j'})) ) } \end{aligned} $$
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} f_\text{align}(z_i) &= \sum_j y_{i,j} E_g(x_j) \\ y_{i,j}
    &= \frac{\exp(\alpha(E_g(z_i))^\top \alpha(E_g(x_j)) )}{\sum_{j'} \exp(\alpha(E_g(z_i))^\top
    \alpha(E_g(x_{j'})) ) } \end{aligned} $$
- en: where $\alpha$ is a single dense layer with ReLU and $E_g(.)$ is the glove word
    embedding.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\alpha$ æ˜¯ä¸€ä¸ªå…·æœ‰ReLUçš„å•ä¸ªå¯†é›†å±‚ï¼Œ$E_g(.)$ æ˜¯gloveè¯åµŒå…¥ã€‚
- en: 'The feature vector of a paragraph of $m$ tokens is fed into LSTM to obtain
    the final paragraph vectors:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªåŒ…å«$m$ä¸ªæ ‡è®°çš„æ®µè½çš„ç‰¹å¾å‘é‡è¢«é¦ˆé€åˆ°LSTMä¸­ä»¥è·å¾—æœ€ç»ˆæ®µè½å‘é‡ï¼š
- en: $$ \begin{aligned} \mathbf{z} = \{\mathbf{z}_1, \dots, \mathbf{z}_m\} &= \text{LSTM}(\{\tilde{\mathbf{z}}_1,
    \dots, \tilde{\mathbf{z}}_m\}) \\ \text{where } \tilde{\mathbf{z}}_i &= \{f_\text{embed},
    f_\text{match}, f_\text{token}, f_\text{align}\} \end{aligned} $$
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \mathbf{z} = \{\mathbf{z}_1, \dots, \mathbf{z}_m\} &= \text{LSTM}(\{\tilde{\mathbf{z}}_1,
    \dots, \tilde{\mathbf{z}}_m\}) \\ \text{where } \tilde{\mathbf{z}}_i &= \{f_\text{embed},
    f_\text{match}, f_\text{token}, f_\text{align}\} \end{aligned} $$
- en: 'The question is encoded as a weighted sum of the embeddings of every word in
    the question:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜è¢«ç¼–ç ä¸ºé—®é¢˜ä¸­æ¯ä¸ªå•è¯çš„åµŒå…¥çš„åŠ æƒå’Œï¼š
- en: $$ \mathbf{x} = \sum_j b_j E(x_j) \quad b_j = \text{softmax}(\mathbf{w}^\top
    E(x_j)) $$
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathbf{x} = \sum_j b_j E(x_j) \quad b_j = \text{softmax}(\mathbf{w}^\top
    E(x_j)) $$
- en: where $\mathbf{w}$ is a weight vector to learn.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\mathbf{w}$ æ˜¯ä¸€ä¸ªå­¦ä¹ çš„æƒé‡å‘é‡ã€‚
- en: Once the feature vectors are constructed for the question and all the related
    paragraphs, the reader needs to predict the probabilities of each position in
    a paragraph to be the start and the end of an answer span, $p_\text{start}(i_s)$
    and $p_\text{end}(i_s)$, respectively. Across all the paragraphs, the optimal
    span is returned as the final answer with maximum $p_\text{start}(i_s) \times
    p_\text{end}(i_e) $.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦ä¸ºé—®é¢˜å’Œæ‰€æœ‰ç›¸å…³æ®µè½æ„å»ºäº†ç‰¹å¾å‘é‡ï¼Œé˜…è¯»å™¨éœ€è¦é¢„æµ‹æ®µè½ä¸­æ¯ä¸ªä½ç½®æˆä¸ºç­”æ¡ˆèŒƒå›´èµ·å§‹å’Œç»“æŸçš„æ¦‚ç‡ï¼Œåˆ†åˆ«ä¸º $p_\text{start}(i_s)$ å’Œ
    $p_\text{end}(i_s)$ã€‚åœ¨æ‰€æœ‰æ®µè½ä¸­ï¼Œä»¥æœ€å¤§çš„ $p_\text{start}(i_s) \times p_\text{end}(i_e)
    $ è¿”å›æœ€ä½³èŒƒå›´ä½œä¸ºæœ€ç»ˆç­”æ¡ˆã€‚
- en: $$ \begin{aligned} p_\text{start}(i_s) \propto \exp(\mathbf{z}_{i_s} \mathbf{W}_s
    \mathbf{x}) \\ p_\text{end}(i_e) \propto \exp(\mathbf{z}_{i_e} \mathbf{W}_e \mathbf{x})
    \\ \text{ s.t. } i_s \leq i_e \leq i_s + 15 \end{aligned} $$
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} p_\text{start}(i_s) \propto \exp(\mathbf{z}_{i_s} \mathbf{W}_s
    \mathbf{x}) \\ p_\text{end}(i_e) \propto \exp(\mathbf{z}_{i_e} \mathbf{W}_e \mathbf{x})
    \\ \text{ s.t. } i_s \leq i_e \leq i_s + 15 \end{aligned} $$
- en: where $\mathbf{W}_s$ and $\mathbf{W}_e$ are learned parameters.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\mathbf{W}_s$ å’Œ $\mathbf{W}_e$ æ˜¯å­¦ä¹ çš„å‚æ•°ã€‚
- en: BERT-universe
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BERT-universe
- en: 'Following the success of [BERT](https://lilianweng.github.io/posts/2019-01-31-lm/#bert)
    ([Devlin et al., 2018](https://arxiv.org/abs/1810.04805)), many QA models develop
    the machine comprehension component based on BERT. Letâ€™s define the BERT model
    as a function that can take one or multiple strings (concatenated by `[SEP]`)
    as input and outputs a set of BERT encoding vectors for the special `[CLS]` token
    and every input token:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[BERT](https://lilianweng.github.io/posts/2019-01-31-lm/#bert)ï¼ˆ[Devlin et al.,
    2018](https://arxiv.org/abs/1810.04805)ï¼‰å–å¾—æˆåŠŸä¹‹åï¼Œè®¸å¤šé—®ç­”æ¨¡å‹åŸºäºBERTå¼€å‘äº†æœºå™¨ç†è§£ç»„ä»¶ã€‚è®©æˆ‘ä»¬å°†BERTæ¨¡å‹å®šä¹‰ä¸ºä¸€ä¸ªå‡½æ•°ï¼Œå¯ä»¥æ¥å—ä¸€ä¸ªæˆ–å¤šä¸ªå­—ç¬¦ä¸²ï¼ˆç”±
    `[SEP]` è¿æ¥ï¼‰ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºç‰¹æ®Š `[CLS]` æ ‡è®°å’Œæ¯ä¸ªè¾“å…¥æ ‡è®°çš„ä¸€ç»„BERTç¼–ç å‘é‡ï¼š
- en: $$ \text{BERT}(s_1, s_2, \dots) = [\mathbf{h}^\texttt{[CLS]}, \mathbf{h}^{(1)},
    \mathbf{h}^{(2)}, \dots] $$
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \text{BERT}(s_1, s_2, \dots) = [\mathbf{h}^\texttt{[CLS]}, \mathbf{h}^{(1)},
    \mathbf{h}^{(2)}, \dots] $$
- en: where $\mathbf{h}^\texttt{[CLS]}$ is the embedding vector for the special `[CLS]`
    token and $\mathbf{h}^{(i)}$ is the embedding vector for the $i$-th token.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\mathbf{h}^\texttt{[CLS]}$ æ˜¯ç‰¹æ®Š `[CLS]` æ ‡è®°çš„åµŒå…¥å‘é‡ï¼Œ$\mathbf{h}^{(i)}$ æ˜¯ç¬¬ $i$
    ä¸ªæ ‡è®°çš„åµŒå…¥å‘é‡ã€‚
- en: To use BERT for reading comprehension, it learns two additional weights, $\mathbf{W}_s$
    and $\mathbf{W}_e$, and $\text{softmax}(\mathbf{h}^{(i)}\mathbf{W}_s)$ and $\text{softmax}(\mathbf{h}^{(i)}\mathbf{W}_e)$
    define two probability distributions of start and end position of the predicted
    span per token.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨BERTè¿›è¡Œé˜…è¯»ç†è§£ï¼Œå®ƒå­¦ä¹ ä¸¤ä¸ªé¢å¤–çš„æƒé‡ï¼Œ$\mathbf{W}_s$ å’Œ $\mathbf{W}_e$ï¼Œå¹¶ä¸” $\text{softmax}(\mathbf{h}^{(i)}\mathbf{W}_s)$
    å’Œ $\text{softmax}(\mathbf{h}^{(i)}\mathbf{W}_e)$ å®šä¹‰äº†æ¯ä¸ªæ ‡è®°é¢„æµ‹èŒƒå›´çš„èµ·å§‹å’Œç»“æŸä½ç½®çš„ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒã€‚
- en: '**BERTserini** ([Yang et al., 2019](https://arxiv.org/abs/1902.01718)) utilizes
    a pre-trained BERT model to work as the reader. Their experiments showed that
    *fine-tuning* pretrained BERT with SQuAD is sufficient to achieve high accuracy
    in identifying answer spans.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**BERTserini**ï¼ˆ[Yang et al., 2019](https://arxiv.org/abs/1902.01718)ï¼‰åˆ©ç”¨é¢„è®­ç»ƒçš„BERTæ¨¡å‹ä½œä¸ºé˜…è¯»å™¨ã€‚ä»–ä»¬çš„å®éªŒè¡¨æ˜ï¼Œç”¨SQuADå¯¹é¢„è®­ç»ƒçš„BERTè¿›è¡Œå¾®è°ƒå°±è¶³ä»¥åœ¨è¯†åˆ«ç­”æ¡ˆèŒƒå›´æ–¹é¢å–å¾—é«˜å‡†ç¡®åº¦ã€‚'
- en: '![](../Images/dd711961782416f2d66312e8efcce9b1.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd711961782416f2d66312e8efcce9b1.png)'
- en: 'Fig. 5\. How BERT is used to solve question-answering tasks. (Image source:
    [Devlin et al., 2018](https://arxiv.org/abs/1810.04805))'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5\. BERTå¦‚ä½•ç”¨äºè§£å†³é—®ç­”ä»»åŠ¡ã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼š[Devlin et al., 2018](https://arxiv.org/abs/1810.04805)ï¼‰
- en: 'The key difference of the BERTserini reader from the original BERT is: to allow
    comparison and aggregation of results from different segments, the final softmax
    layer over different answer spans is removed. The pre-trained BERT model is fine-tuned
    on the training set of SQuAD, where all inputs to the reader are padded to 384
    tokens with the learning rate 3e-5.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: BERTserinié˜…è¯»å™¨ä¸åŸå§‹BERTçš„å…³é”®åŒºåˆ«åœ¨äºï¼šä¸ºäº†å…è®¸å¯¹ä¸åŒæ®µè½çš„ç»“æœè¿›è¡Œæ¯”è¾ƒå’Œèšåˆï¼Œç§»é™¤äº†ä¸åŒç­”æ¡ˆèŒƒå›´ä¸Šçš„æœ€ç»ˆsoftmaxå±‚ã€‚é¢„è®­ç»ƒçš„BERTæ¨¡å‹åœ¨SQuADçš„è®­ç»ƒé›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå…¶ä¸­æ‰€æœ‰è¾“å…¥éƒ½è¢«å¡«å……åˆ°384ä¸ªæ ‡è®°ï¼Œå¹¶ä¸”å­¦ä¹ ç‡ä¸º3e-5ã€‚
- en: When ranking all the extracted answer spans, the retriever score (BM25) and
    the reader score (probability of token being the start position $\times$ probability
    of the same token being the end position ) are combined via linear interpolation.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¯¹æ‰€æœ‰æå–çš„ç­”æ¡ˆèŒƒå›´è¿›è¡Œæ’åæ—¶ï¼Œæ£€ç´¢å™¨åˆ†æ•°ï¼ˆBM25ï¼‰å’Œé˜…è¯»å™¨åˆ†æ•°ï¼ˆtokenæ˜¯èµ·å§‹ä½ç½®çš„æ¦‚ç‡ $\times$ ç›¸åŒtokenæ˜¯ç»“æŸä½ç½®çš„æ¦‚ç‡ï¼‰é€šè¿‡çº¿æ€§æ’å€¼è¿›è¡Œç»„åˆã€‚
- en: The original BERT normalizes the probability distributions of start and end
    position per token for every passage independently. Differently, the **Multi-passage
    BERT** ([Wang et al., 2019](https://arxiv.org/abs/1908.08167)) normalizes answer
    scores across all the retrieved passages of one question [globally](https://arxiv.org/abs/1710.10723).
    Precisely, multi-passage BERT removes the final normalization layer per passage
    in BERT for QA (same as in BERTserini) and then adds a global `softmax` over all
    the word positions of all the passages. Global normalization makes the reader
    model more stable while pin-pointing answers from a large number of passages.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹çš„BERTç‹¬ç«‹åœ°ä¸ºæ¯ä¸ªæ®µè½çš„æ¯ä¸ªæ ‡è®°è§„èŒƒåŒ–èµ·å§‹å’Œç»“æŸä½ç½®çš„æ¦‚ç‡åˆ†å¸ƒã€‚ä¸ä¹‹ä¸åŒçš„æ˜¯ï¼Œ**å¤šæ®µBERT**ï¼ˆ[Wangç­‰ï¼Œ2019](https://arxiv.org/abs/1908.08167)ï¼‰åœ¨ä¸€ä¸ªé—®é¢˜çš„æ‰€æœ‰æ£€ç´¢åˆ°çš„æ®µè½ä¸­å…¨å±€åœ°è§„èŒƒåŒ–ç­”æ¡ˆåˆ†æ•°ã€‚å‡†ç¡®åœ°è¯´ï¼Œå¤šæ®µBERTå»é™¤äº†BERTä¸­ç”¨äºQAçš„æ¯ä¸ªæ®µè½çš„æœ€ç»ˆè§„èŒƒåŒ–å±‚ï¼ˆä¸BERTseriniä¸­ç›¸åŒï¼‰ï¼Œç„¶ååœ¨æ‰€æœ‰æ®µè½çš„æ‰€æœ‰å•è¯ä½ç½®ä¸Šæ·»åŠ ä¸€ä¸ªå…¨å±€çš„`softmax`ã€‚å…¨å±€è§„èŒƒåŒ–ä½¿é˜…è¯»å™¨æ¨¡å‹åœ¨ä»å¤§é‡æ®µè½ä¸­æ‰¾åˆ°ç­”æ¡ˆæ—¶æ›´åŠ ç¨³å®šã€‚
- en: In addition, multi-passage BERT implemented an independent *passage ranker*
    model via another BERT model and the rank score for $(x, z)$ is generated by a
    `softmax` over the representation vectors of the first `[CLS]` token. The passage
    ranker brings in extra 2% improvements. Similar idea of re-ranking passages with
    BERT was discussed in [Nogueira & Cho, 2019](https://arxiv.org/abs/1901.04085),
    too.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œå¤šæ®µBERTé€šè¿‡å¦ä¸€ä¸ªBERTæ¨¡å‹å®ç°äº†ç‹¬ç«‹çš„*æ®µè½æ’åå™¨*æ¨¡å‹ï¼Œå¹¶ä¸”$(x, z)$çš„æ’ååˆ†æ•°æ˜¯é€šè¿‡å¯¹ç¬¬ä¸€ä¸ª`[CLS]`æ ‡è®°çš„è¡¨ç¤ºå‘é‡è¿›è¡Œ`softmax`ç”Ÿæˆçš„ã€‚æ®µè½æ’åå™¨å¸¦æ¥é¢å¤–çš„2%æ”¹è¿›ã€‚åœ¨[Nogueira
    & Choï¼Œ2019](https://arxiv.org/abs/1901.04085)ä¸­ä¹Ÿè®¨è®ºäº†ä½¿ç”¨BERTé‡æ–°æ’åˆ—æ®µè½çš„ç±»ä¼¼æƒ³æ³•ã€‚
- en: Interestingly, [Wang et al., 2019](https://arxiv.org/abs/1908.08167) found that
    *explicit inter-sentence matching* does not seem to be critical for RC tasks with
    BERT; check the original paper for how the experiments were designed. One possible
    reason is that the multi-head self-attention layers in BERT has already embedded
    the inter-sentence matching.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰è¶£çš„æ˜¯ï¼Œ[Wangç­‰ï¼Œ2019](https://arxiv.org/abs/1908.08167)å‘ç°ï¼Œåœ¨ä½¿ç”¨BERTè¿›è¡ŒRCä»»åŠ¡æ—¶ï¼Œ*æ˜¾å¼çš„å¥é—´åŒ¹é…*ä¼¼ä¹å¹¶ä¸æ˜¯å…³é”®å› ç´ ï¼›è¯·æŸ¥çœ‹åŸå§‹è®ºæ–‡ä»¥äº†è§£å®éªŒæ˜¯å¦‚ä½•è®¾è®¡çš„ã€‚ä¸€ä¸ªå¯èƒ½çš„åŸå› æ˜¯BERTä¸­çš„å¤šå¤´è‡ªæ³¨æ„åŠ›å±‚å·²ç»åµŒå…¥äº†å¥é—´åŒ¹é…ã€‚
- en: End-to-end Joint Training
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç«¯åˆ°ç«¯è”åˆè®­ç»ƒ
- en: The retriever and reader components can be jointly trained. This section covers
    R^3, ORQA, REALM and DPR. There are a lot of common designs, such as BERT-based
    dense vectors for retrieval and the loss function on maximizing the marginal likelihood
    of obtaining true answers.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€ç´¢å™¨å’Œé˜…è¯»å™¨ç»„ä»¶å¯ä»¥è”åˆè®­ç»ƒã€‚æœ¬èŠ‚æ¶µç›–äº†R^3ã€ORQAã€REALMå’ŒDPRã€‚æœ‰è®¸å¤šå…±åŒçš„è®¾è®¡ï¼Œä¾‹å¦‚åŸºäºBERTçš„å¯†é›†å‘é‡ç”¨äºæ£€ç´¢å’Œæœ€å¤§åŒ–è·å¾—çœŸå®ç­”æ¡ˆçš„è¾¹é™…ä¼¼ç„¶çš„æŸå¤±å‡½æ•°ã€‚
- en: The retriever and reader models in the **R^3** (â€œReinforced Ranker-Readerâ€;
    [Wang, et al., 2017](https://arxiv.org/abs/1709.00023)) QA system are jointly
    trained via [reinforcement learning](https://lilianweng.github.io/posts/2018-02-19-rl-overview/).
    (Note that to keep the term consistent between papers in this section, the â€œrankerâ€
    model in the original R^3 paper is referred to as the â€œretrieverâ€ model here.)
    Both components are variants of [Match-LSTM](https://arxiv.org/abs/1512.08849),
    which relies on an attention mechanism to compute word similarities between the
    passage and question sequences.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**R^3**ï¼ˆâ€œå¼ºåŒ–æ’å-é˜…è¯»å™¨â€ï¼›[Wangç­‰ï¼Œ2017](https://arxiv.org/abs/1709.00023)ï¼‰QAç³»ç»Ÿä¸­çš„æ£€ç´¢å™¨å’Œé˜…è¯»å™¨æ¨¡å‹é€šè¿‡[å¼ºåŒ–å­¦ä¹ ](https://lilianweng.github.io/posts/2018-02-19-rl-overview/)è”åˆè®­ç»ƒã€‚ï¼ˆè¯·æ³¨æ„ï¼Œåœ¨æœ¬èŠ‚ä¸­ä¿æŒæœ¯è¯­ä¸€è‡´ï¼ŒåŸå§‹R^3è®ºæ–‡ä¸­çš„â€œæ’åå™¨â€æ¨¡å‹åœ¨è¿™é‡Œè¢«ç§°ä¸ºâ€œæ£€ç´¢å™¨â€æ¨¡å‹ã€‚ï¼‰è¿™ä¸¤ä¸ªç»„ä»¶éƒ½æ˜¯[Match-LSTM](https://arxiv.org/abs/1512.08849)çš„å˜ä½“ï¼Œä¾èµ–äºæ³¨æ„æœºåˆ¶æ¥è®¡ç®—æ®µè½å’Œé—®é¢˜åºåˆ—ä¹‹é—´çš„å•è¯ç›¸ä¼¼æ€§ã€‚'
- en: '**How does the Match-LSTM module work?** Given a question $\mathbf{X}$ of $d_x$
    words and a passage $\mathbf{Z}$ of $d_z$ words, both representations use fixed
    [Glove](https://lilianweng.github.io/posts/2017-10-15-word-embedding/#glove-global-vectors)
    word embeddings,'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**Match-LSTMæ¨¡å—æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ** ç»™å®šä¸€ä¸ªåŒ…å«$d_x$ä¸ªå•è¯çš„é—®é¢˜$\mathbf{X}$å’Œä¸€ä¸ªåŒ…å«$d_z$ä¸ªå•è¯çš„æ®µè½$\mathbf{Z}$ï¼Œä¸¤ä¸ªè¡¨ç¤ºéƒ½ä½¿ç”¨å›ºå®šçš„[Glove](https://lilianweng.github.io/posts/2017-10-15-word-embedding/#glove-global-vectors)è¯åµŒå…¥ï¼Œ'
- en: $$ \begin{aligned} \mathbf{H}^x &= \text{BiLSTM}(\mathbf{X}) \in \mathbb{R}^{l
    \times d_x} \\ \mathbf{H}^z &= \text{BiLSTM}(\mathbf{Z}) \in \mathbb{R}^{l \times
    d_z} \\ \mathbf{G} &= \text{softmax}((\mathbf{W}^g \mathbf{H}^x + \mathbf{b}^g
    \otimes \mathbf{e}_{d_x})^\top \mathbf{H}^z) \in \mathbb{R}^{d_x \times d_z} &
    \text{; an attention matrix}\\ \bar{\mathbf{H}}^x &= \mathbf{H}^x \mathbf{G} \in
    \mathbb{R}^{l \times d_z} \\ \mathbf{M} &= \text{ReLU} \Big( \mathbf{W}^m \begin{bmatrix}
    \mathbf{H}^z \\ \bar{\mathbf{H}}^x \\ \mathbf{H}^z \odot \bar{\mathbf{H}}^x \\
    \mathbf{H}^z - \bar{\mathbf{H}}^x \end{bmatrix} \Big) \in \mathbb{R}^{2l \times
    d_z} \\ \mathbf{H}^m &= \text{BiLSTM}(M) \in \mathbb{R}^{l \times d_z} \end{aligned}
    $$
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \mathbf{H}^x &= \text{BiLSTM}(\mathbf{X}) \in \mathbb{R}^{l
    \times d_x} \\ \mathbf{H}^z &= \text{BiLSTM}(\mathbf{Z}) \in \mathbb{R}^{l \times
    d_z} \\ \mathbf{G} &= \text{softmax}((\mathbf{W}^g \mathbf{H}^x + \mathbf{b}^g
    \otimes \mathbf{e}_{d_x})^\top \mathbf{H}^z) \in \mathbb{R}^{d_x \times d_z} &
    \text{; ä¸€ä¸ªæ³¨æ„åŠ›çŸ©é˜µ}\\ \bar{\mathbf{H}}^x &= \mathbf{H}^x \mathbf{G} \in \mathbb{R}^{l
    \times d_z} \\ \mathbf{M} &= \text{ReLU} \Big( \mathbf{W}^m \begin{bmatrix} \mathbf{H}^z
    \\ \bar{\mathbf{H}}^x \\ \mathbf{H}^z \odot \bar{\mathbf{H}}^x \\ \mathbf{H}^z
    - \bar{\mathbf{H}}^x \end{bmatrix} \Big) \in \mathbb{R}^{2l \times d_z} \\ \mathbf{H}^m
    &= \text{BiLSTM}(M) \in \mathbb{R}^{l \times d_z} \end{aligned} $$
- en: where $l$ is the hidden dimension of the bidirectional LSTM module. $\mathbf{W}^g
    \in \mathbb{R}^{l\times l}$, $\mathbf{b}^g \in \mathbb{R}^l$, and $\mathbf{W}^m
    \in \mathbb{R}^{2l \times 4l}$ are parameters to learn. The operator $\otimes
    \mathbf{e}_{d_x}$ is the outer product to repeat the column vector $\mathbf{b}^g$
    $d_x$ times.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­$l$æ˜¯åŒå‘LSTMæ¨¡å—çš„éšè—ç»´åº¦ã€‚$\mathbf{W}^g \in \mathbb{R}^{l\times l}$ï¼Œ$\mathbf{b}^g
    \in \mathbb{R}^l$ï¼Œ$\mathbf{W}^m \in \mathbb{R}^{2l \times 4l}$æ˜¯è¦å­¦ä¹ çš„å‚æ•°ã€‚è¿ç®—ç¬¦$\otimes
    \mathbf{e}_{d_x}$æ˜¯å¤–ç§¯ï¼Œé‡å¤åˆ—å‘é‡$\mathbf{b}^g$ $d_x$æ¬¡ã€‚
- en: The ranker and reader components share the same Match-LSTM module with two separate
    prediction heads in the last layer, resulting in $\mathbf{H}^\text{rank}$ and
    $\mathbf{H}^\text{reader}$.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: æ’åºå™¨å’Œé˜…è¯»å™¨ç»„ä»¶åœ¨æœ€åä¸€å±‚ä¸­å…±äº«ç›¸åŒçš„Match-LSTMæ¨¡å—ï¼Œå¯¼è‡´$\mathbf{H}^\text{rank}$å’Œ$\mathbf{H}^\text{reader}$ã€‚
- en: '![](../Images/77a18c1ac97832793216d3fe33ae3eae.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77a18c1ac97832793216d3fe33ae3eae.png)'
- en: 'Fig. 6\. The overview of R^3 (reinforced ranker-reader) architecture. Both
    components share the same Match-LSTM module. (Image source: [Wang, et al., 2017](https://arxiv.org/abs/1709.00023))'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾6ã€‚R^3ï¼ˆå¼ºåŒ–æ’åºå™¨-é˜…è¯»å™¨ï¼‰æ¶æ„æ¦‚è¿°ã€‚ä¸¤ä¸ªç»„ä»¶å…±äº«ç›¸åŒçš„Match-LSTMæ¨¡å—ã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼š[Wang, et al., 2017](https://arxiv.org/abs/1709.00023)ï¼‰
- en: The retriever runs a max-pooling operation per passage and then aggregates to
    output a probability of each passage entailing the answer.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ä¿¡æ¯æ£€ç´¢å™¨å¯¹æ¯ä¸ªæ®µè½è¿›è¡Œæœ€å¤§æ± åŒ–æ“ä½œï¼Œç„¶åèšåˆè¾“å‡ºæ¯ä¸ªæ®µè½åŒ…å«ç­”æ¡ˆçš„æ¦‚ç‡ã€‚
- en: $$ \begin{aligned} \mathbf{u}_i &= \text{max-pooling}(\mathbf{H}^\text{rank}_i)
    \in \mathbb{R}^l \\ \mathbf{C} &= \text{tanh}(\mathbf{W}^c[\mathbf{u}_1;\dots;\mathbf{u}_N]
    + \mathbf{b}^c \otimes \mathbf{e}_N) \in \mathbb{R}^{l \times n} \\ \gamma &=
    \text{softmax}(\mathbf{w}^c \mathbf{C}) \in \mathbb{R}^n \end{aligned} $$
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \mathbf{u}_i &= \text{max-pooling}(\mathbf{H}^\text{rank}_i)
    \in \mathbb{R}^l \\ \mathbf{C} &= \text{tanh}(\mathbf{W}^c[\mathbf{u}_1;\dots;\mathbf{u}_N]
    + \mathbf{b}^c \otimes \mathbf{e}_N) \in \mathbb{R}^{l \times n} \\ \gamma &=
    \text{softmax}(\mathbf{w}^c \mathbf{C}) \in \mathbb{R}^n \end{aligned} $$
- en: Finally, the retriever is viewed as a *policy* to output action to sample a
    passage according to predicted $\gamma$,
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œä¿¡æ¯æ£€ç´¢å™¨è¢«è§†ä¸ºä¸€ä¸ª*ç­–ç•¥*ï¼Œè¾“å‡ºåŠ¨ä½œä»¥æ ¹æ®é¢„æµ‹çš„$\gamma$æŠ½å–æ®µè½ï¼Œ
- en: $$ \pi(z \vert x; \theta^\gamma) = \gamma_z $$
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \pi(z \vert x; \theta^\gamma) = \gamma_z $$
- en: The reader predicts the start position $\beta^s$ and the end position $\beta^e$
    of the answer span. Two positions are computed in the same way, with independent
    parameters to learn. There are $V$ words in all the passages involved.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: é˜…è¯»å™¨é¢„æµ‹ç­”æ¡ˆè·¨åº¦çš„èµ·å§‹ä½ç½®$\beta^s$å’Œç»“æŸä½ç½®$\beta^e$ã€‚ä¸¤ä¸ªä½ç½®ä»¥ç›¸åŒæ–¹å¼è®¡ç®—ï¼Œå…·æœ‰ç‹¬ç«‹çš„å­¦ä¹ å‚æ•°ã€‚æ‰€æœ‰æ¶‰åŠçš„æ®µè½ä¸­å…±æœ‰$V$ä¸ªå•è¯ã€‚
- en: $$ \begin{aligned} \mathbf{H}^\text{read} &= [\mathbf{H}^\text{read}_\tau; \mathbf{H}^\text{read}_{\text{neg}_1};
    \dots; \mathbf{H}^\text{read}_{\text{neg}_n}] \\ \mathbf{F}^s &= \text{tanh}(\mathbf{W}^s
    \mathbf{H}^\text{read} + \mathbf{b}^s \otimes \mathbf{e}_V) \quad \beta^s = \text{softmax}(\mathbf{w}^s
    \mathbf{F}^s) \in \mathbb{R}^V \\ \mathbf{F}^e &= \text{tanh}(\mathbf{W}^e \mathbf{H}^\text{read}
    + \mathbf{b}^e \otimes \mathbf{e}_V) \quad \beta^e = \text{softmax}(\mathbf{w}^e
    \mathbf{F}^e) \in \mathbb{R}^V \\ L(y \vert z, x) &= -\log(\beta^s_{y_z^s})-\log(\beta^e_{y_z^e})
    \end{aligned} $$
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \mathbf{H}^\text{read} &= [\mathbf{H}^\text{read}_\tau; \mathbf{H}^\text{read}_{\text{neg}_1};
    \dots; \mathbf{H}^\text{read}_{\text{neg}_n}] \\ \mathbf{F}^s &= \text{tanh}(\mathbf{W}^s
    \mathbf{H}^\text{read} + \mathbf{b}^s \otimes \mathbf{e}_V) \quad \beta^s = \text{softmax}(\mathbf{w}^s
    \mathbf{F}^s) \in \mathbb{R}^V \\ \mathbf{F}^e &= \text{tanh}(\mathbf{W}^e \mathbf{H}^\text{read}
    + \mathbf{b}^e \otimes \mathbf{e}_V) \quad \beta^e = \text{softmax}(\mathbf{w}^e
    \mathbf{F}^e) \in \mathbb{R}^V \\ L(y \vert z, x) &= -\log(\beta^s_{y_z^s})-\log(\beta^e_{y_z^e})
    \end{aligned} $$
- en: where $y$ is the ground-truth answer and the passage $z$ is sampled by the retriever.
    $\beta^s_{y_z^s}$ and $\beta^s_{y_z^e}$ represent the probabilities of the start
    and end positions of $y$ in passage $z$.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­$y$æ˜¯åœ°é¢çœŸç›¸ç­”æ¡ˆï¼Œæ®µè½$z$ç”±æ£€ç´¢å™¨é‡‡æ ·ã€‚$\beta^s_{y_z^s}$å’Œ$\beta^s_{y_z^e}$è¡¨ç¤º$y$åœ¨æ®µè½$z$ä¸­èµ·å§‹å’Œç»“æŸä½ç½®çš„æ¦‚ç‡ã€‚
- en: The training objective for the end-to-end R^3 QA system is to minimize the negative
    log-likelihood of obtaining the correct answer $y$ given a question $x$,
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ç«¯åˆ°ç«¯R^3 QAç³»ç»Ÿçš„è®­ç»ƒç›®æ ‡æ˜¯æœ€å°åŒ–ç»™å®šé—®é¢˜$x$çš„æ­£ç¡®ç­”æ¡ˆ$y$çš„è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼Œ
- en: $$ \begin{aligned} \mathcal{J}(\theta) &= -\mathbb{E}_{z\sim\pi(.\vert x)} [L(y
    \vert z, x)] \\ \nabla \mathcal{J}(\theta) &= - \nabla_\theta \sum_z \pi(z \vert
    x) L(y \vert z, x) \\ &= - \sum_z \big( L(y \vert z, x) \nabla_\theta\pi(z \vert
    x) + \pi(z \vert x) \nabla_\theta L(y \vert z, x) \big) \\ &= - \mathbb{E}_{z\sim\pi(.\vert
    x)} \big( \color{red}{L(y \vert z, x)\nabla_\theta\log\pi(z \vert x)} + \nabla_\theta
    L(y \vert z, x) \big) \\ &\approx - \mathbb{E}_{z\sim\pi(.\vert x)} \big( \underbrace{\color{red}{R(y
    \vert z, x)\nabla_\theta\log\pi(z \vert x)}}_\text{REINFORCE} + \nabla_\theta
    L(y \vert z, x) \big) \end{aligned} $$
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \mathcal{J}(\theta) &= -\mathbb{E}_{z\sim\pi(.\vert x)} [L(y
    \vert z, x)] \\ \nabla \mathcal{J}(\theta) &= - \nabla_\theta \sum_z \pi(z \vert
    x) L(y \vert z, x) \\ &= - \sum_z \big( L(y \vert z, x) \nabla_\theta\pi(z \vert
    x) + \pi(z \vert x) \nabla_\theta L(y \vert z, x) \big) \\ &= - \mathbb{E}_{z\sim\pi(.\vert
    x)} \big( \color{red}{L(y \vert z, x)\nabla_\theta\log\pi(z \vert x)} + \nabla_\theta
    L(y \vert z, x) \big) \\ &\approx - \mathbb{E}_{z\sim\pi(.\vert x)} \big( \underbrace{\color{red}{R(y
    \vert z, x)\nabla_\theta\log\pi(z \vert x)}}_\text{REINFORCE} + \nabla_\theta
    L(y \vert z, x) \big) \end{aligned} $$
- en: 'Essentially in training, given a passage $z$ sampled by the retriever, the
    reader is trained by gradient descent while the retriever is trained by [REINFORCE](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/#reinforce)
    using $L(y \vert z, x)$ as the reward function. However, $L(y \vert z, x)$ is
    not bounded and may introduce a lot of variance. The paper replaces the reward
    with a customized scoring function by comparing the ground truth $y$ and the answer
    extracted by the reader $\hat{y}$:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç»™å®šç”±æ£€ç´¢å™¨é‡‡æ ·çš„æ®µè½$z$ï¼Œé˜…è¯»å™¨é€šè¿‡æ¢¯åº¦ä¸‹é™è¿›è¡Œè®­ç»ƒï¼Œè€Œæ£€ç´¢å™¨åˆ™é€šè¿‡[REINFORCE](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/#reinforce)è¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨$L(y
    \vert z, x)$ä½œä¸ºå¥–åŠ±å‡½æ•°ã€‚ç„¶è€Œï¼Œ$L(y \vert z, x)$å¹¶éæœ‰ç•Œï¼Œå¯èƒ½å¼•å…¥å¾ˆå¤šæ–¹å·®ã€‚è¯¥è®ºæ–‡é€šè¿‡æ¯”è¾ƒåœ°é¢çœŸç›¸$y$å’Œé˜…è¯»å™¨æå–çš„ç­”æ¡ˆ$\hat{y}$ï¼Œç”¨è‡ªå®šä¹‰è¯„åˆ†å‡½æ•°æ›¿æ¢å¥–åŠ±ï¼š
- en: $$ R(y, \hat{y} \vert z) = \begin{cases} 2 & \text{if } y = \hat{y}\\ f1(y,
    \hat{y}) & \text{if } y \cap \hat{y} = \varnothing \\ -1 & \text{otherwise} \end{cases}
    $$![](../Images/8266be557a80072acd8a9ba12c582d15.png)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: $$ R(y, \hat{y} \vert z) = \begin{cases} 2 & \text{if } y = \hat{y}\\ f1(y,
    \hat{y}) & \text{if } y \cap \hat{y} = \varnothing \\ -1 & \text{otherwise} \end{cases}
    $$![](../Images/8266be557a80072acd8a9ba12c582d15.png)
- en: 'Fig. 7\. The workflow of R^3 training process. (Image source: [acl2020-openqa-tutorial/slides/part4](https://github.com/danqi/acl2020-openqa-tutorial/blob/master/slides/part4-retriever-reader.pdf))'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾7\. R^3è®­ç»ƒè¿‡ç¨‹çš„å·¥ä½œæµç¨‹ã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼š[acl2020-openqa-tutorial/slides/part4](https://github.com/danqi/acl2020-openqa-tutorial/blob/master/slides/part4-retriever-reader.pdf)ï¼‰
- en: '**ORQA** (â€œOpen-Retrieval Question-Answeringâ€; [Lee et al., 2019](https://arxiv.org/abs/1906.00300))
    jointly learns a retriever + reader QA model to optimize marginal log-likelihood
    of obtaining correct answers in a supervised manner. No explicit â€œblack-boxâ€ IR
    system is involved. Instead, it is capable of retrieving any text in an open corpus.
    During training, ORQA does not need ground-truth context passages (i.e. reading
    comprehension datasets) but only needs (question, answer) string pairs. Both retriever
    and reader components are based on BERT, but not shared.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**ORQA**ï¼ˆâ€œå¼€æ”¾å¼æ£€ç´¢é—®ç­”â€ï¼›[Leeç­‰ï¼Œ2019](https://arxiv.org/abs/1906.00300)ï¼‰å…±åŒå­¦ä¹ æ£€ç´¢å™¨+é˜…è¯»å™¨QAæ¨¡å‹ï¼Œä»¥ç›‘ç£æ–¹å¼ä¼˜åŒ–è·å¾—æ­£ç¡®ç­”æ¡ˆçš„è¾¹é™…å¯¹æ•°ä¼¼ç„¶ã€‚æ²¡æœ‰æ˜ç¡®çš„â€œé»‘ç›’â€IRç³»ç»Ÿå‚ä¸ã€‚ç›¸åï¼Œå®ƒèƒ½å¤Ÿæ£€ç´¢å¼€æ”¾è¯­æ–™åº“ä¸­çš„ä»»ä½•æ–‡æœ¬ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒORQAä¸éœ€è¦åœ°é¢çœŸç›¸ä¸Šä¸‹æ–‡æ®µè½ï¼ˆå³é˜…è¯»ç†è§£æ•°æ®é›†ï¼‰ï¼Œè€Œåªéœ€è¦ï¼ˆé—®é¢˜ï¼Œç­”æ¡ˆï¼‰å­—ç¬¦ä¸²å¯¹ã€‚æ£€ç´¢å™¨å’Œé˜…è¯»å™¨ç»„ä»¶éƒ½åŸºäºBERTï¼Œä½†ä¸å…±äº«ã€‚'
- en: '![](../Images/d1d494451a1b61d82bbbf05f30b1b6ff.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1d494451a1b61d82bbbf05f30b1b6ff.png)'
- en: 'Fig. 8\. An illustration of the retriever component in ORQA. (Image source:
    replotted based on one slide in [acl2020-openqa-tutorial/slides/part5](https://github.com/danqi/acl2020-openqa-tutorial/blob/master/slides/part5-dense-retriever-e2e-training.pdf))'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8\. ORQAä¸­æ£€ç´¢å™¨ç»„ä»¶çš„ç¤ºæ„å›¾ã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼šé‡æ–°ç»˜åˆ¶ï¼ŒåŸºäº[acl2020-openqa-tutorial/slides/part5](https://github.com/danqi/acl2020-openqa-tutorial/blob/master/slides/part5-dense-retriever-e2e-training.pdf)ä¸­çš„ä¸€å¼ å¹»ç¯ç‰‡ï¼‰
- en: All the evidence blocks are ranked by a retrieval score, defined as the inner
    product of BERT embedding vectors of the `[CLS]` token of the question $x$ and
    the evidence block $z$. Note that the encoders for questions and context are independent.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è¯æ®å—éƒ½æŒ‰æ£€ç´¢åˆ†æ•°æ’åï¼Œå®šä¹‰ä¸ºé—®é¢˜$x$çš„`[CLS]`æ ‡è®°å’Œè¯æ®å—$z$çš„BERTåµŒå…¥å‘é‡çš„å†…ç§¯ã€‚è¯·æ³¨æ„ï¼Œé—®é¢˜å’Œä¸Šä¸‹æ–‡çš„ç¼–ç å™¨æ˜¯ç‹¬ç«‹çš„ã€‚
- en: $$ \begin{aligned} h_x &= \mathbf{W}_x \text{BERT}_x(x)^{\mathtt{[CLS]}} \\
    h_z &= \mathbf{W}_z \text{BERT}_z(z)^{\mathtt{[CLS]}} \\ S_\text{retr}(z, x) &=
    h_x^\top h_z \end{aligned} $$
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} h_x &= \mathbf{W}_x \text{BERT}_x(x)^{\mathtt{[CLS]}} \\
    h_z &= \mathbf{W}_z \text{BERT}_z(z)^{\mathtt{[CLS]}} \\ S_\text{retr}(z, x) &=
    h_x^\top h_z \end{aligned} $$
- en: 'The retriever module is pretrained with *Inverse Cloze Task (ICT)*, which is
    to predict the context given a sentence, opposite to the standard [Cloze Task](https://en.wikipedia.org/wiki/Cloze_test).
    The ICT objective is to maximize the retrieval score of the correct context $z$
    given a random sentence $x$:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€ç´¢å™¨æ¨¡å—ä½¿ç”¨*Inverse Cloze Task (ICT)*è¿›è¡Œé¢„è®­ç»ƒï¼Œå³ç»™å®šä¸€ä¸ªå¥å­é¢„æµ‹ä¸Šä¸‹æ–‡ï¼Œä¸æ ‡å‡†çš„[Cloze Task](https://en.wikipedia.org/wiki/Cloze_test)ç›¸åã€‚ICTç›®æ ‡æ˜¯æœ€å¤§åŒ–ç»™å®šéšæœºå¥å­$x$çš„æ­£ç¡®ä¸Šä¸‹æ–‡$z$çš„æ£€ç´¢åˆ†æ•°ï¼š
- en: $$ L_\text{ICT} = p_\text{early}(z \vert x) = \frac{\exp(S_\text{retr}(z, x))}{\sum_{z'\in\text{BATCH}(\mathcal{Z})}
    \exp(S_\text{retr}(z', x))} $$
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: $$ L_\text{ICT} = p_\text{early}(z \vert x) = \frac{\exp(S_\text{retr}(z, x))}{\sum_{z'\in\text{BATCH}(\mathcal{Z})}
    \exp(S_\text{retr}(z', x))} $$
- en: where $\text{BATCH}(\mathcal{Z})$ is the set of evidence blocks in the same
    batch used as sampled negatives.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­$\text{BATCH}(\mathcal{Z})$æ˜¯åŒä¸€æ‰¹æ¬¡ä¸­ç”¨ä½œè´Ÿæ ·æœ¬çš„è¯æ®å—é›†åˆã€‚
- en: After such pretraining, the BERT retriever is expected to have representations
    good enough for evidence retrieval. Only the question encoder needs to be fine-tuned
    for answer extraction. In other words, the evidence block encoder (i.e., $\mathbf{W}_z$
    and $\text{BERT}_z$) is fixed and thus all the evidence block encodings can be
    pre-computed with support for [fast Maximum Inner Product Search (MIPS)](#fast-maximum-inner-product-search-mips).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ç»è¿‡è¿™æ ·çš„é¢„è®­ç»ƒï¼Œé¢„æœŸBERTæ£€ç´¢å™¨çš„è¡¨ç¤ºè¶³å¤Ÿå¥½ä»¥è¿›è¡Œè¯æ®æ£€ç´¢ã€‚åªéœ€å¯¹é—®é¢˜ç¼–ç å™¨è¿›è¡Œå¾®è°ƒä»¥è¿›è¡Œç­”æ¡ˆæå–ã€‚æ¢å¥è¯è¯´ï¼Œè¯æ®å—ç¼–ç å™¨ï¼ˆå³$\mathbf{W}_z$å’Œ$\text{BERT}_z$ï¼‰æ˜¯å›ºå®šçš„ï¼Œå› æ­¤æ‰€æœ‰è¯æ®å—ç¼–ç éƒ½å¯ä»¥é¢„å…ˆè®¡ç®—ï¼Œå¹¶æ”¯æŒ[å¿«é€Ÿæœ€å¤§å†…ç§¯æœç´¢ï¼ˆMIPSï¼‰](#fast-maximum-inner-product-search-mips)ã€‚
- en: '![](../Images/07848864f28b51f46371ee5e738d6a60.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07848864f28b51f46371ee5e738d6a60.png)'
- en: 'Fig. 9\. An illustration of the reader component in ORQA. (Image source: [acl2020-openqa-tutorial/slides/part5](https://github.com/danqi/acl2020-openqa-tutorial/blob/master/slides/part5-dense-retriever-e2e-training.pdf))'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9\. ORQAä¸­é˜…è¯»å™¨ç»„ä»¶çš„ç¤ºæ„å›¾ï¼ˆå›¾ç‰‡æ¥æºï¼š[acl2020-openqa-tutorial/slides/part5](https://github.com/danqi/acl2020-openqa-tutorial/blob/master/slides/part5-dense-retriever-e2e-training.pdf)ï¼‰
- en: 'The reader follows the same design as in the original [BERT RC](https://lilianweng.github.io/posts/2019-01-31-lm/#use-bert-in-downstream-tasks)
    experiments. It learns in a supervised manner, while the parameters of the evidence
    block encoder are fixed and all other parameters are fine-tuned. Given a question
    $x$ and a gold answer string $y$, the reader loss contains two parts:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: é˜…è¯»å™¨éµå¾ªåŸå§‹[BERT RC](https://lilianweng.github.io/posts/2019-01-31-lm/#use-bert-in-downstream-tasks)å®éªŒä¸­çš„ç›¸åŒè®¾è®¡ã€‚å®ƒä»¥ç›‘ç£æ–¹å¼å­¦ä¹ ï¼Œè€Œè¯æ®å—ç¼–ç å™¨çš„å‚æ•°æ˜¯å›ºå®šçš„ï¼Œæ‰€æœ‰å…¶ä»–å‚æ•°éƒ½è¿›è¡Œå¾®è°ƒã€‚ç»™å®šé—®é¢˜$x$å’Œé»„é‡‘ç­”æ¡ˆå­—ç¬¦ä¸²$y$ï¼Œé˜…è¯»å™¨æŸå¤±åŒ…å«ä¸¤éƒ¨åˆ†ï¼š
- en: $$ \mathcal{L}(x, y) = \mathcal{L}_\text{early}(x, y) + \mathcal{L}_\text{full}(x,
    y) $$
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathcal{L}(x, y) = \mathcal{L}_\text{early}(x, y) + \mathcal{L}_\text{full}(x,
    y) $$
- en: '(1) Find all correct text spans within top $k$ evidence blocks and optimize
    for the marginal likelihood of a text span $s$ that matches the true answer $y$:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: (1) åœ¨å‰$k$ä¸ªè¯æ®å—ä¸­æ‰¾åˆ°æ‰€æœ‰æ­£ç¡®çš„æ–‡æœ¬è·¨åº¦ï¼Œå¹¶ä¼˜åŒ–åŒ¹é…çœŸå®ç­”æ¡ˆ$y$çš„æ–‡æœ¬è·¨åº¦$s$çš„è¾¹é™…ä¼¼ç„¶ï¼š
- en: $$ \begin{aligned} h_s &= \text{BERT}_R(x, y)^{(\text{START}(s))} \\ h_e &=
    \text{BERT}_R(x, y)^{(\text{END}(s))} \\ S_\text{read}(z, s, x) &= \text{MLP}([h_s;
    h_e]) \\ p(z, s \vert x) &= \frac{\exp(S_\text{read}(z, s, x))}{\sum_{z'\in\text{TOP}(k)}
    \sum_{s'\in z'} \exp(S_\text{read}(z', s', x))} \\ L_\text{full}(x, y) &= - \log
    \sum_{\substack{z \in \text{TOP}(k)\\ s \in z}} \sum_{y=\text{TEXT}(s)} p(z, s
    \vert x) \end{aligned} $$
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} h_s &= \text{BERT}_R(x, y)^{(\text{START}(s))} \\ h_e &=
    \text{BERT}_R(x, y)^{(\text{END}(s))} \\ S_\text{read}(z, s, x) &= \text{MLP}([h_s;
    h_e]) \\ p(z, s \vert x) &= \frac{\exp(S_\text{read}(z, s, x))}{\sum_{z'\in\text{TOP}(k)}
    \sum_{s'\in z'} \exp(S_\text{read}(z', s', x))} \\ L_\text{full}(x, y) &= - \log
    \sum_{\substack{z \in \text{TOP}(k)\\ s \in z}} \sum_{y=\text{TEXT}(s)} p(z, s
    \vert x) \end{aligned} $$
- en: where $y=\text{TEXT}(s)$ indicates whether the answer $y$ matches the text span
    $s$. $\text{TOP}(k)$ is the top $k$ retrieved blocks according to $S_\text{retr}(z,
    x)$. The paper sets $k=5$.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­$y=\text{TEXT}(s)$è¡¨ç¤ºç­”æ¡ˆ$y$æ˜¯å¦ä¸æ–‡æœ¬è·¨åº¦$s$åŒ¹é…ã€‚$\text{TOP}(k)$æ˜¯æ ¹æ®$S_\text{retr}(z,
    x)$æ£€ç´¢åˆ°çš„å‰$k$ä¸ªå—ã€‚è¯¥è®ºæ–‡è®¾ç½®$k=5$ã€‚
- en: (2) At the early stage of learning, when the retriever is not strong enough,
    it is possible none of the top $k$ blocks contains the answer. To avoid such sparse
    learning signals, ORQA considers a larger set of $c$ evidence blocks for more
    aggressive learning. The paper has $c=5000$.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: (2) åœ¨å­¦ä¹ çš„æ—©æœŸé˜¶æ®µï¼Œå½“æ£€ç´¢å™¨è¿˜ä¸å¤Ÿå¼ºå¤§æ—¶ï¼Œå¯èƒ½æ²¡æœ‰ä¸€ä¸ªé¡¶éƒ¨çš„$k$å—åŒ…å«ç­”æ¡ˆã€‚ä¸ºäº†é¿å…è¿™ç§ç¨€ç–çš„å­¦ä¹ ä¿¡å·ï¼ŒORQAè€ƒè™‘äº†æ›´å¤§çš„$c$è¯æ®å—é›†åˆï¼Œä»¥è¿›è¡Œæ›´ç§¯æçš„å­¦ä¹ ã€‚è®ºæ–‡ä¸­$c=5000$ã€‚
- en: $$ L_\text{early}(x, y) = -\log \sum_{\substack{z\in \text{TOP}(c)\\y\in\text{TEXT}(z)}}
    p_\text{early}(z\vert x) = -\log \sum_{\substack{z\in \text{TOP}(c)\\y\in\text{TEXT}(z)}}
    \frac{\exp(S_\text{retr}(z, x)}{\sum_{z'\in\text{TOP}(c)} \exp(S_\text{retr}(z',
    x)} $$
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: $$ L_\text{early}(x, y) = -\log \sum_{\substack{z\in \text{TOP}(c)\\y\in\text{TEXT}(z)}}
    p_\text{early}(z\vert x) = -\log \sum_{\substack{z\in \text{TOP}(c)\\y\in\text{TEXT}(z)}}
    \frac{\exp(S_\text{retr}(z, x)}{\sum_{z'\in\text{TOP}(c)} \exp(S_\text{retr}(z',
    x)} $$
- en: 'Some issues in SQuAD dataset were discussed in the ORQA paper:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ORQAè®ºæ–‡ä¸­è®¨è®ºäº†SQuADæ•°æ®é›†ä¸­çš„ä¸€äº›é—®é¢˜ï¼š
- en: '" The notable drop between development and test accuracy for SQuAD is a reflection
    of an artifact in the datasetâ€”its 100k questions are derived from only 536 documents.
    Therefore, good retrieval targets are highly correlated between training examples,
    violating the IID assumption, and making it unsuitable for learned retrieval.
    We strongly suggest that those who are interested in end-to-end open-domain QA
    models no longer train and evaluate with SQuAD for this reason."'
  id: totrans-138
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œSQuADçš„å¼€å‘å’Œæµ‹è¯•å‡†ç¡®ç‡ä¹‹é—´çš„æ˜¾ç€ä¸‹é™åæ˜ äº†æ•°æ®é›†ä¸­çš„ä¸€ä¸ªé—®é¢˜â€”â€”å…¶10ä¸‡ä¸ªé—®é¢˜ä»…æ¥è‡ª536ä¸ªæ–‡æ¡£ã€‚å› æ­¤ï¼Œè®­ç»ƒç¤ºä¾‹ä¹‹é—´çš„å¥½çš„æ£€ç´¢ç›®æ ‡é«˜åº¦ç›¸å…³ï¼Œè¿åäº†IIDå‡è®¾ï¼Œä½¿å…¶ä¸é€‚ç”¨äºå­¦ä¹ æ£€ç´¢ã€‚å‡ºäºè¿™ä¸ªåŸå› ï¼Œæˆ‘ä»¬å¼ºçƒˆå»ºè®®é‚£äº›å¯¹ç«¯åˆ°ç«¯å¼€æ”¾åŸŸQAæ¨¡å‹æ„Ÿå…´è¶£çš„äººä¸å†ä½¿ç”¨SQuADè¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ã€‚â€
- en: '**REALM** (â€œRetrieval-Augmented Language Model pre-trainingâ€; [Guu et al.,
    2020](https://arxiv.org/abs/2002.08909)) also jointly trains retriever + reader
    by optimizing the marginal likelihood of obtaining the true answer:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**REALM**ï¼ˆâ€œæ£€ç´¢å¢å¼ºè¯­è¨€æ¨¡å‹é¢„è®­ç»ƒâ€ï¼›[Guuç­‰äººï¼Œ2020å¹´](https://arxiv.org/abs/2002.08909)ï¼‰è¿˜é€šè¿‡ä¼˜åŒ–è·å¾—çœŸå®ç­”æ¡ˆçš„è¾¹é™…ä¼¼ç„¶æ¥è”åˆè®­ç»ƒæ£€ç´¢å™¨+è¯»è€…ï¼š'
- en: $$ p(y \vert x) = \sum_{z \in \mathcal{Z}} \underbrace{p(y \vert x, z)}_\text{reader}
    \underbrace{p(z \vert x)}_\text{retriever} \approx \sum_{z \in \text{TOP}_k(\mathcal{Z})}
    p(y \vert x, z) p(z \vert x) $$![](../Images/09a2aca893d76d02388dc2e33a0aad7d.png)
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: $$ p(y \vert x) = \sum_{z \in \mathcal{Z}} \underbrace{p(y \vert x, z)}_\text{è¯»è€…}
    \underbrace{p(z \vert x)}_\text{æ£€ç´¢å™¨} \approx \sum_{z \in \text{TOP}_k(\mathcal{Z})}
    p(y \vert x, z) p(z \vert x) $$![](../Images/09a2aca893d76d02388dc2e33a0aad7d.png)
- en: 'Fig. 10\. REALM is first unsupervised pre-trained with salient spans masking
    and then fine-tuned with QA data. (Image source: [Guu et al., 2020](https://arxiv.org/abs/2002.08909)).'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10ã€‚REALMé¦–å…ˆä½¿ç”¨æ˜¾è‘—è·¨åº¦å±è”½è¿›è¡Œæ— ç›‘ç£é¢„è®­ç»ƒï¼Œç„¶åå†ç”¨QAæ•°æ®è¿›è¡Œå¾®è°ƒã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼š[Guuç­‰äººï¼Œ2020å¹´](https://arxiv.org/abs/2002.08909)ï¼‰ã€‚
- en: REALM computes two probabilities, $p(z \vert x)$ and $p(y \vert x, z)$, same
    as ORQA. However, different from ICT in ORQA, REALM upgrades the unsupervised
    pre-training step with several new design decisions, leading towards better retrievals.
    REALM pre-trains the model with Wikipedia or CC-News corpus.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: REALMè®¡ç®—ä¸¤ä¸ªæ¦‚ç‡ï¼Œ$p(z \vert x)$ å’Œ $p(y \vert x, z)$ï¼Œä¸ORQAç›¸åŒã€‚ç„¶è€Œï¼Œä¸ORQAä¸­çš„ICTä¸åŒï¼ŒREALMé€šè¿‡å‡ ä¸ªæ–°çš„è®¾è®¡å†³ç­–å‡çº§äº†æ— ç›‘ç£é¢„è®­ç»ƒæ­¥éª¤ï¼Œä»è€Œå®ç°æ›´å¥½çš„æ£€ç´¢ã€‚REALMä½¿ç”¨ç»´åŸºç™¾ç§‘æˆ–CC-Newsè¯­æ–™åº“å¯¹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒã€‚
- en: Use *salient span masking*. Named entities and dates are identified. Then one
    of these â€œsalient spansâ€ is selected and masked. Salient span masking is a special
    case of MLM and works out well for QA tasks.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨*æ˜¾è‘—è·¨åº¦å±è”½*ã€‚å‘½åå®ä½“å’Œæ—¥æœŸè¢«è¯†åˆ«ã€‚ç„¶åé€‰æ‹©å¹¶å±è”½å…¶ä¸­ä¸€ä¸ªâ€œæ˜¾è‘—è·¨åº¦â€ã€‚æ˜¾è‘—è·¨åº¦å±è”½æ˜¯MLMçš„ä¸€ä¸ªç‰¹ä¾‹ï¼Œå¯¹QAä»»åŠ¡æ•ˆæœå¾ˆå¥½ã€‚
- en: Add an *empty null document*. Because not every question demands a context document.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ·»åŠ ä¸€ä¸ª*ç©ºçš„ç©ºæ–‡æ¡£*ã€‚å› ä¸ºå¹¶éæ¯ä¸ªé—®é¢˜éƒ½éœ€è¦ä¸€ä¸ªä¸Šä¸‹æ–‡æ–‡æ¡£ã€‚
- en: No trivial retrieval. The context document should not be same as the selected
    sentence with a masked span.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸è¦è¿›è¡Œå¹³å‡¡çš„æ£€ç´¢ã€‚ä¸Šä¸‹æ–‡æ–‡æ¡£ä¸åº”ä¸é€‰æ‹©çš„å¸¦æœ‰å±è”½è·¨åº¦çš„å¥å­ç›¸åŒã€‚
- en: Apply the same ICT loss as in ORQA to encourage learning when the retrieval
    quality is still poor at the early stage of training.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒçš„æ—©æœŸé˜¶æ®µï¼Œå½“æ£€ç´¢è´¨é‡ä»ç„¶è¾ƒå·®æ—¶ï¼Œåº”ç”¨ä¸ORQAä¸­ç›¸åŒçš„ICTæŸå¤±ä»¥é¼“åŠ±å­¦ä¹ ã€‚
- en: â€œAmong all systems, the most direct comparison with REALM is ORQA (Lee et al.,
    2019), where the fine-tuning setup, hyperparameters and training data are identical.
    The improvement of REALM over ORQA is purely due to better pre-training methods.â€
    â€” from REALM paper.
  id: totrans-147
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œåœ¨æ‰€æœ‰ç³»ç»Ÿä¸­ï¼Œä¸REALMæœ€ç›´æ¥çš„æ¯”è¾ƒæ˜¯ORQAï¼ˆLeeç­‰äººï¼Œ2019å¹´ï¼‰ï¼Œå…¶ä¸­å¾®è°ƒè®¾ç½®ã€è¶…å‚æ•°å’Œè®­ç»ƒæ•°æ®æ˜¯ç›¸åŒçš„ã€‚REALMç›¸å¯¹äºORQAçš„æ”¹è¿›çº¯ç²¹æ˜¯ç”±äºæ›´å¥½çš„é¢„è®­ç»ƒæ–¹æ³•ã€‚â€
    â€”â€”æ¥è‡ªREALMè®ºæ–‡ã€‚
- en: Both unsupervised pre-training and supervised fine-tuning optimize the same
    log-likelihood $\log p(y \vert x)$. Because the parameters of the retriever encoder
    for evidence documents are also updated in the process, the index for MIPS is
    changing. REALM asynchronously refreshes the index with the updated encoder parameters
    every several hundred training steps.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: æ— ç›‘ç£é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒéƒ½ä¼˜åŒ–ç›¸åŒçš„å¯¹æ•°ä¼¼ç„¶$\log p(y \vert x)$ã€‚ç”±äºè¯æ®æ–‡æ¡£çš„æ£€ç´¢å™¨ç¼–ç å™¨çš„å‚æ•°ä¹Ÿåœ¨æ­¤è¿‡ç¨‹ä¸­æ›´æ–°ï¼ŒMIPSçš„ç´¢å¼•æ­£åœ¨å˜åŒ–ã€‚REALMæ¯éš”å‡ ç™¾ä¸ªè®­ç»ƒæ­¥éª¤å¼‚æ­¥ä½¿ç”¨æ›´æ–°çš„ç¼–ç å™¨å‚æ•°åˆ·æ–°ç´¢å¼•ã€‚
- en: '[Balachandran, et al. (2021)](https://arxiv.org/abs/2104.08710) found that
    REALM is significantly undertrained and REALM++ achieves great EM accuracy improvement
    (3-5%) by scaling up the model training with larger batch size and more retrieved
    documents for the reader to process.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[Balachandranç­‰äººï¼ˆ2021ï¼‰](https://arxiv.org/abs/2104.08710)å‘ç°REALMæ˜æ˜¾è®­ç»ƒä¸è¶³ï¼Œè€ŒREALM++é€šè¿‡å¢åŠ æ¨¡å‹è®­ç»ƒçš„æ‰¹é‡å¤§å°å’Œæ›´å¤šæ£€ç´¢æ–‡æ¡£ï¼Œä½¿è¯»è€…å¤„ç†çš„EMå‡†ç¡®æ€§å¾—åˆ°äº†å¾ˆå¤§çš„æé«˜ï¼ˆ3-5%ï¼‰ã€‚'
- en: '**DPR** (â€œDense Passage Retrieverâ€; [Karpukhin et al., 2020](https://arxiv.org/abs/2004.04906),
    [code](https://github.com/facebookresearch/DPR)) argues that ICT pre-training
    could be too computationally expensive and the ORQAâ€™s context encoder might be
    sub-optimal because it is not fine-tuned with question-answer pairs. DPR aims
    to resolve these two issues by only training a dense dual-encoder architecture
    for retrieval only from a small number of Q/A pairs, without any pre-training.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**DPR**ï¼ˆâ€œå¯†é›†æ®µè½æ£€ç´¢å™¨â€ï¼›[Karpukhinç­‰äººï¼Œ2020](https://arxiv.org/abs/2004.04906)ï¼Œ[code](https://github.com/facebookresearch/DPR)ï¼‰è®¤ä¸ºICTé¢„è®­ç»ƒå¯èƒ½è¿‡äºè®¡ç®—æ˜‚è´µï¼Œè€ŒORQAçš„ä¸Šä¸‹æ–‡ç¼–ç å™¨å¯èƒ½ä¸å¤Ÿä¼˜åŒ–ï¼Œå› ä¸ºå®ƒæ²¡æœ‰ä¸é—®é¢˜-ç­”æ¡ˆå¯¹è¿›è¡Œå¾®è°ƒã€‚DPRæ—¨åœ¨é€šè¿‡ä»…è®­ç»ƒä¸€ä¸ªå¯†é›†åŒç¼–ç å™¨æ¶æ„ï¼Œä»…ä»å°‘é‡Q/Aå¯¹ä¸­æ£€ç´¢ï¼Œè€Œæ— éœ€ä»»ä½•é¢„è®­ç»ƒæ¥è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ã€‚'
- en: Same as previous work, DPR uses the dot-product (L2 distance or cosine similarity
    also works) of BERT representations as retrieval score. The loss function for
    training the dual-encoder is the NLL of the positive passage, which essentially
    takes the same formulation as [ICT loss](#ICT-loss) of ORQA. Note that both of
    them consider other passages in the same batch as the negative samples, named
    *in-batch negative sampling*. The main difference is that DPR relies on supervised
    QA data, while ORQA trains with ICT on unsupervised corpus. At the inference time,
    DPR uses [FAISS](https://github.com/facebookresearch/faiss) to run fast MIPS.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å…ˆå‰çš„å·¥ä½œç›¸åŒï¼ŒDPRä½¿ç”¨BERTè¡¨ç¤ºçš„ç‚¹ç§¯ï¼ˆL2è·ç¦»æˆ–ä½™å¼¦ç›¸ä¼¼åº¦ä¹Ÿé€‚ç”¨ï¼‰ä½œä¸ºæ£€ç´¢åˆ†æ•°ã€‚ç”¨äºè®­ç»ƒåŒç¼–ç å™¨çš„æŸå¤±å‡½æ•°æ˜¯æ­£é¢æ®µè½çš„NLLï¼Œå…¶æœ¬è´¨ä¸Šä¸[ICTæŸå¤±](#ICT-loss)çš„ORQAç›¸åŒã€‚è¯·æ³¨æ„ï¼Œå®ƒä»¬éƒ½å°†åŒä¸€æ‰¹æ¬¡ä¸­çš„å…¶ä»–æ®µè½è§†ä¸ºè´Ÿæ ·æœ¬ï¼Œç§°ä¸º*æ‰¹å†…è´Ÿé‡‡æ ·*ã€‚ä¸»è¦åŒºåˆ«åœ¨äºDPRä¾èµ–äºç›‘ç£QAæ•°æ®ï¼Œè€ŒORQAåœ¨æ— ç›‘ç£è¯­æ–™åº“ä¸Šè¿›è¡ŒICTè®­ç»ƒã€‚åœ¨æ¨æ–­æ—¶ï¼ŒDPRä½¿ç”¨[FAISS](https://github.com/facebookresearch/faiss)æ¥å¿«é€Ÿè¿è¡ŒMIPSã€‚
- en: 'DPR did a set of comparison experiments involving several different types of
    negatives:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: DPRè¿›è¡Œäº†ä¸€ç³»åˆ—æ¯”è¾ƒå®éªŒï¼Œæ¶‰åŠå‡ ç§ä¸åŒç±»å‹çš„è´Ÿä¾‹ï¼š
- en: 'Random: any random passage from the corpus;'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: éšæœºï¼šæ¥è‡ªè¯­æ–™åº“çš„ä»»æ„éšæœºæ®µè½ï¼›
- en: 'BM25: top passages returned by BM25 which donâ€™t contain the answer but match
    most question tokens;'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BM25ï¼šç”±BM25è¿”å›çš„é¡¶çº§æ®µè½ï¼Œä¸åŒ…å«ç­”æ¡ˆä½†åŒ¹é…å¤§å¤šæ•°é—®é¢˜æ ‡è®°ï¼›
- en: 'In-batch negative sampling (â€œgoldâ€): positive passages paired with other questions
    which appear in the training set.'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ‰¹å†…è´Ÿé‡‡æ ·ï¼ˆâ€œé‡‘æ ‡â€ï¼‰ï¼šä¸è®­ç»ƒé›†ä¸­å‡ºç°çš„å…¶ä»–é—®é¢˜é…å¯¹çš„æ­£é¢æ®µè½ã€‚
- en: DPR found that using gold passages from the same mini-batch and one negative
    passage with high BM25 score works the best. To further improve the retrieval
    results, DPR also explored a setting where a BM25 score and a dense embedding
    retrieval score are linearly combined to serve as a new ranking function.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: DPRå‘ç°ï¼Œä½¿ç”¨æ¥è‡ªåŒä¸€å°æ‰¹æ¬¡çš„é‡‘æ ‡æ®µè½å’Œä¸€ä¸ªå…·æœ‰è¾ƒé«˜BM25åˆ†æ•°çš„è´Ÿé¢æ®µè½æ•ˆæœæœ€å¥½ã€‚ä¸ºäº†è¿›ä¸€æ­¥æ”¹å–„æ£€ç´¢ç»“æœï¼ŒDPRè¿˜æ¢ç´¢äº†ä¸€ç§è®¾ç½®ï¼Œå…¶ä¸­BM25åˆ†æ•°å’Œå¯†é›†åµŒå…¥æ£€ç´¢åˆ†æ•°è¢«çº¿æ€§ç»„åˆä»¥ä½œä¸ºæ–°çš„æ’åå‡½æ•°ã€‚
- en: 'Open-book QA: Retriever-Generator'
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¼€æ”¾ä¹¦é—®ç­”ï¼šæ£€ç´¢å™¨-ç”Ÿæˆå™¨
- en: Compared to the retriever-reader approach, the retriever-generator also has
    2 stages but the second stage is to generate free text directly to answer the
    question rather than to extract start/end position in a retrieved passage. Some
    paper also refer to this as *Generative question answering*.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ£€ç´¢å™¨-é˜…è¯»å™¨æ–¹æ³•ç›¸æ¯”ï¼Œæ£€ç´¢å™¨-ç”Ÿæˆå™¨ä¹Ÿæœ‰2ä¸ªé˜¶æ®µï¼Œä½†ç¬¬äºŒé˜¶æ®µæ˜¯ç›´æ¥ç”Ÿæˆè‡ªç”±æ–‡æœ¬æ¥å›ç­”é—®é¢˜ï¼Œè€Œä¸æ˜¯åœ¨æ£€ç´¢åˆ°çš„æ®µè½ä¸­æå–èµ·å§‹/ç»“æŸä½ç½®ã€‚ä¸€äº›è®ºæ–‡ä¹Ÿå°†æ­¤ç§°ä¸º*ç”Ÿæˆå¼é—®ç­”*ã€‚
- en: '![](../Images/b4b9253390c26d5cc09e1668d48bb9a8.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4b9253390c26d5cc09e1668d48bb9a8.png)'
- en: Fig. 11\. The retriever + generator QA framework combines a document retrieval
    system with a general language model.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾11. æ£€ç´¢å™¨+ç”Ÿæˆå™¨QAæ¡†æ¶å°†æ–‡æ¡£æ£€ç´¢ç³»ç»Ÿä¸é€šç”¨è¯­è¨€æ¨¡å‹ç»“åˆåœ¨ä¸€èµ·ã€‚
- en: A pretrained LM has a great capacity of memorizing knowledge in its parameters,
    as shown above. However, they cannot easily modify or expand their memory, cannot
    straightforwardly provide insights into their predictions, and may produce non-existent
    illusion.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„è®­ç»ƒçš„LMåœ¨å…¶å‚æ•°ä¸­å…·æœ‰è®°å¿†çŸ¥è¯†çš„å·¨å¤§èƒ½åŠ›ï¼Œå¦‚ä¸Šæ‰€ç¤ºã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¸èƒ½è½»æ¾ä¿®æ”¹æˆ–æ‰©å±•å…¶è®°å¿†ï¼Œä¸èƒ½ç›´æ¥æä¾›å¯¹å…¶é¢„æµ‹çš„è§è§£ï¼Œå¹¶å¯èƒ½äº§ç”Ÿä¸å­˜åœ¨çš„å¹»è§‰ã€‚
- en: '[Petroni et al. (2020)](https://arxiv.org/abs/2005.04611) studied how the retrieved
    relevant context can help a generative language model produce better answers.
    They found:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[Petroniç­‰äººï¼ˆ2020ï¼‰](https://arxiv.org/abs/2005.04611)ç ”ç©¶äº†æ£€ç´¢åˆ°çš„ç›¸å…³ä¸Šä¸‹æ–‡å¦‚ä½•å¸®åŠ©ç”Ÿæˆå¼è¯­è¨€æ¨¡å‹äº§ç”Ÿæ›´å¥½çš„ç­”æ¡ˆã€‚ä»–ä»¬å‘ç°ï¼š'
- en: Augmenting queries with relevant contexts dramatically improves the pretrained
    LM on unsupervised machine reading capabilities.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€šè¿‡ç›¸å…³ä¸Šä¸‹æ–‡å¢å¼ºæŸ¥è¯¢ï¼Œæ˜¾è‘—æé«˜äº†é¢„è®­ç»ƒçš„LMåœ¨æ— ç›‘ç£æœºå™¨é˜…è¯»èƒ½åŠ›ä¸Šçš„è¡¨ç°ã€‚
- en: An off-the-shelf IR system is sufficient for BERT to match the performance of
    a supervised ODQA baseline;
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç°æˆçš„IRç³»ç»Ÿè¶³ä»¥ä½¿BERTè¾¾åˆ°ç›‘ç£ODQAåŸºçº¿çš„æ€§èƒ½ï¼›
- en: BERTâ€™s [NSP](https://lilianweng.github.io/posts/2019-01-31-lm/#pre-training-tasks)
    pre-training strategy is a highly effective unsupervised mechanism in dealing
    with noisy and irrelevant contexts.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BERTçš„[NSP](https://lilianweng.github.io/posts/2019-01-31-lm/#pre-training-tasks)é¢„è®­ç»ƒç­–ç•¥æ˜¯ä¸€ç§é«˜æ•ˆçš„æ— ç›‘ç£æœºåˆ¶ï¼Œç”¨äºå¤„ç†å˜ˆæ‚å’Œæ— å…³çš„ä¸Šä¸‹æ–‡ã€‚
- en: 'They pair the BERT model with different types of context, including adversarial
    (unrelated context), retrieved (by BM25), and generative (by an autoregressive
    language model of 1.4N parameters, trained on CC-NEWS). The model is found to
    be robust to adversarial context, but only when the question and the context are
    provided as two segments (e.g. separated by `[SEP]`). One hypothesis is related
    to NSP task: â€œBERT might learn to not condition across segments for masked token
    prediction if the NSP score is low, thereby implicitly detecting irrelevant and
    noisy contexts.â€'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: å°†BERTæ¨¡å‹ä¸ä¸åŒç±»å‹çš„ä¸Šä¸‹æ–‡é…å¯¹ï¼ŒåŒ…æ‹¬å¯¹æŠ—æ€§ï¼ˆæ— å…³ä¸Šä¸‹æ–‡ï¼‰ã€æ£€ç´¢ï¼ˆé€šè¿‡BM25æ£€ç´¢ï¼‰å’Œç”Ÿæˆæ€§ï¼ˆé€šè¿‡ä¸€ä¸ªåŒ…å«1.4Nå‚æ•°çš„è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼Œåœ¨CC-NEWSä¸Šè®­ç»ƒï¼‰ã€‚å‘ç°è¯¥æ¨¡å‹å¯¹å¯¹æŠ—æ€§ä¸Šä¸‹æ–‡å…·æœ‰é²æ£’æ€§ï¼Œä½†ä»…å½“é—®é¢˜å’Œä¸Šä¸‹æ–‡ä½œä¸ºä¸¤ä¸ªç‰‡æ®µæä¾›æ—¶ï¼ˆä¾‹å¦‚ï¼Œç”±`[SEP]`åˆ†éš”ï¼‰ã€‚ä¸€ä¸ªå‡è®¾ä¸NSPä»»åŠ¡æœ‰å…³ï¼šâ€œå¦‚æœNSPå¾—åˆ†ä½ï¼ŒBERTå¯èƒ½ä¼šå­¦ä¹ ä¸è·¨ç‰‡æ®µè¿›è¡Œæ©ç æ ‡è®°é¢„æµ‹ï¼Œä»è€Œéšå¼æ£€æµ‹åˆ°æ— å…³å’Œå˜ˆæ‚çš„ä¸Šä¸‹æ–‡ã€‚â€
- en: '**RAG** (â€œRetrieval-Augmented Generationâ€; [Lewis et al., 2020](https://arxiv.org/abs/2005.11401))
    combines pre-trained parametric (language model) and non-parametric memory (external
    knowledge index) together for language generation. RAG can be fine-tuned on any
    seq2seq task, whereby both the retriever and the sequence generator are jointly
    learned. They found that unconstrained generation outperforms previous extractive
    approaches.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**RAG**ï¼ˆâ€œæ£€ç´¢å¢å¼ºç”Ÿæˆâ€ï¼›[Lewisç­‰äººï¼Œ2020](https://arxiv.org/abs/2005.11401)ï¼‰å°†é¢„è®­ç»ƒçš„å‚æ•°åŒ–ï¼ˆè¯­è¨€æ¨¡å‹ï¼‰å’Œéå‚æ•°åŒ–è®°å¿†ï¼ˆå¤–éƒ¨çŸ¥è¯†ç´¢å¼•ï¼‰ç»“åˆåœ¨ä¸€èµ·è¿›è¡Œè¯­è¨€ç”Ÿæˆã€‚RAGå¯ä»¥åœ¨ä»»ä½•seq2seqä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»è€ŒåŒæ—¶å­¦ä¹ æ£€ç´¢å™¨å’Œåºåˆ—ç”Ÿæˆå™¨ã€‚ä»–ä»¬å‘ç°ï¼Œæ— çº¦æŸçš„ç”Ÿæˆä¼˜äºå…ˆå‰çš„æŠ½å–å¼æ–¹æ³•ã€‚'
- en: 'RAG consists of a retriever model $p_\eta(z \vert x)$ and a generator model
    $p_\theta(y_i \vert x, z, y_{1:i-1})$:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: RAGç”±ä¸€ä¸ªæ£€ç´¢å™¨æ¨¡å‹$p_\eta(z \vert x)$å’Œä¸€ä¸ªç”Ÿæˆå™¨æ¨¡å‹$p_\theta(y_i \vert x, z, y_{1:i-1})$ç»„æˆï¼š
- en: The retriever uses the input sequence $x$ to retrieve text passages $z$, implemented
    as a [DPR](#DPR) retriever. $\log p_\eta(z \vert x) \propto E_z(z)^\top E_x(x)$.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ£€ç´¢å™¨ä½¿ç”¨è¾“å…¥åºåˆ—$x$æ£€ç´¢æ–‡æœ¬æ®µè½$z$ï¼Œå®ç°ä¸º[DPR](#DPR)æ£€ç´¢å™¨ã€‚$\log p_\eta(z \vert x) \propto E_z(z)^\top
    E_x(x)$ã€‚
- en: The generator uses $z$ as additional context when generating the target sequence
    $y$, where the context and the question are simply concatenated.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå™¨åœ¨ç”Ÿæˆç›®æ ‡åºåˆ—$y$æ—¶ä½¿ç”¨$z$ä½œä¸ºé¢å¤–ä¸Šä¸‹æ–‡ï¼Œå…¶ä¸­ä¸Šä¸‹æ–‡å’Œé—®é¢˜ç®€å•åœ°è¿æ¥åœ¨ä¸€èµ·ã€‚
- en: 'Depending on whether using the same or different retrieved documents for each
    token generation, there are two versions of RAG:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®æ¯ä¸ªæ ‡è®°ç”Ÿæˆæ—¶æ˜¯å¦ä½¿ç”¨ç›¸åŒæˆ–ä¸åŒçš„æ£€ç´¢æ–‡æ¡£ï¼ŒRAGæœ‰ä¸¤ä¸ªç‰ˆæœ¬ï¼š
- en: $$ \begin{aligned} p_\text{RAG-seq}(y \vert x) &= \sum_{z \in \text{TOP}_k(p_\eta(.\vert
    x))} p_\eta(z \vert x) \prod_i^N p_\theta(y_i \vert x, z, y_{1:i-1}) \\ p_\text{RAG-token}(y
    \vert x) &= \prod_i^N \sum_{z \in \text{TOP}_k(p_\eta(.\vert x))} p_\eta(z_i\vert
    x) p_\theta(y_i \vert x, z_i, y_{1:i-1}) \end{aligned} $$
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} p_\text{RAG-seq}(y \vert x) &= \sum_{z \in \text{TOP}_k(p_\eta(.\vert
    x))} p_\eta(z \vert x) \prod_i^N p_\theta(y_i \vert x, z, y_{1:i-1}) \\ p_\text{RAG-token}(y
    \vert x) &= \prod_i^N \sum_{z \in \text{TOP}_k(p_\eta(.\vert x))} p_\eta(z_i\vert
    x) p_\theta(y_i \vert x, z_i, y_{1:i-1}) \end{aligned} $$
- en: The retriever + generator in RAG is jointly trained to minimize the NLL loss,
    $\mathcal{L}_\text{RAG} = \sum_j -\log p(y_j \vert x_j)$. Updating the passage
    encoder $E_z(.)$ is expensive as it requires the model to re-index the documents
    for fast MIPS. RAG does not find fine-tuning $E_z(.)$ necessary (like in [ORQA](#ORQA))
    and only updates the query encoder + generator.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: RAGä¸­çš„æ£€ç´¢å™¨+ç”Ÿæˆå™¨æ˜¯è”åˆè®­ç»ƒçš„ï¼Œä»¥æœ€å°åŒ–NLLæŸå¤±ï¼Œ$\mathcal{L}_\text{RAG} = \sum_j -\log p(y_j \vert
    x_j)$ã€‚æ›´æ–°æ®µè½ç¼–ç å™¨$E_z(.)$æ˜¯æ˜‚è´µçš„ï¼Œå› ä¸ºå®ƒè¦æ±‚æ¨¡å‹é‡æ–°ç´¢å¼•æ–‡æ¡£ä»¥è¿›è¡Œå¿«é€ŸMIPSã€‚RAGè®¤ä¸ºä¸éœ€è¦å¯¹$E_z(.)$è¿›è¡Œå¾®è°ƒï¼ˆå°±åƒåœ¨[ORQA](#ORQA)ä¸­ä¸€æ ·ï¼‰ï¼Œåªæ›´æ–°æŸ¥è¯¢ç¼–ç å™¨+ç”Ÿæˆå™¨ã€‚
- en: '![](../Images/7c8fb0446c4e7648be17acea83e33c8f.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c8fb0446c4e7648be17acea83e33c8f.png)'
- en: 'Fig. 12\. An illustration of retrieval-augmented generation (RAG) architecture.
    (Image source: [Lewis et al., 2020](https://arxiv.org/abs/2005.11401))'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾12ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¶æ„çš„ç¤ºæ„å›¾ã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼š[Lewisç­‰äººï¼Œ2020](https://arxiv.org/abs/2005.11401)ï¼‰
- en: At decoding/test time, RAG-token can be evaluated via a [beam search](https://d2l.ai/chapter_recurrent-modern/beam-search.html#id1).
    RAG-seq cannot be broken down into a set of per-token likelihood, so it runs beam
    search for each candidate document $z$ and picks the one with optimal $p_\theta(y_i
    \vert x, z, y_{1:i-1})$.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è§£ç /æµ‹è¯•æ—¶ï¼ŒRAG-tokenå¯ä»¥é€šè¿‡[æŸæœç´¢](https://d2l.ai/chapter_recurrent-modern/beam-search.html#id1)è¿›è¡Œè¯„ä¼°ã€‚RAG-seqæ— æ³•åˆ†è§£ä¸ºä¸€ç»„æ¯ä¸ªæ ‡è®°çš„å¯èƒ½æ€§ï¼Œå› æ­¤å®ƒä¸ºæ¯ä¸ªå€™é€‰æ–‡æ¡£$z$è¿è¡ŒæŸæœç´¢ï¼Œå¹¶é€‰æ‹©å…·æœ‰æœ€ä½³$p_\theta(y_i
    \vert x, z, y_{1:i-1})$çš„æ–‡æ¡£ã€‚
- en: The *Fusion-in-Decoder* approach, proposed by [Izacard & Grave (2020)](https://arxiv.org/abs/2007.01282)
    is also based on a pre-trained T5\. It works similar to RAG but differently for
    how the context is integrated into the decoder.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '*Fusion-in-Decoder* æ–¹æ³•ï¼Œç”±[Izacard & Grave (2020)](https://arxiv.org/abs/2007.01282)æå‡ºï¼Œä¹ŸåŸºäºé¢„è®­ç»ƒçš„T5ã€‚å®ƒçš„å·¥ä½œæ–¹å¼ç±»ä¼¼äºRAGï¼Œä½†åœ¨ä¸Šä¸‹æ–‡å¦‚ä½•æ•´åˆåˆ°è§£ç å™¨ä¸­æ–¹é¢æœ‰æ‰€ä¸åŒã€‚'
- en: Retrieve top $k$ related passage of 100 words each, using BM25 or DPR.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ£€ç´¢å‰$k$ä¸ªç›¸å…³æ®µè½ï¼Œæ¯ä¸ªæ®µè½100ä¸ªè¯ï¼Œä½¿ç”¨BM25æˆ–DPRã€‚
- en: Each retrieved passage and its title are concatenated with the question using
    special tokens like `question:`, `title:` and `context:` to indicate the content
    differences.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¯ä¸ªæ£€ç´¢åˆ°çš„æ®µè½åŠå…¶æ ‡é¢˜éƒ½ä½¿ç”¨ç‰¹æ®Šæ ‡è®°ï¼ˆå¦‚`question:`ã€`title:`å’Œ`context:`ï¼‰ä¸é—®é¢˜è¿æ¥ï¼Œä»¥æŒ‡ç¤ºå†…å®¹å·®å¼‚ã€‚
- en: Each retrieved passage is processed independently and later combined in the
    decoder. Processing passages independently in the encoder allows us to parallelize
    the computation. OTOH, processing them jointly encourages better aggregation of
    multiple pieces of evidence. The aggregation part is missing in extractive approaches.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¯ä¸ªæ£€ç´¢åˆ°çš„æ®µè½éƒ½ä¼šè¢«ç‹¬ç«‹å¤„ç†ï¼Œç„¶ååœ¨è§£ç å™¨ä¸­åˆå¹¶ã€‚åœ¨ç¼–ç å™¨ä¸­ç‹¬ç«‹å¤„ç†æ®µè½å…è®¸æˆ‘ä»¬å¹¶è¡Œè®¡ç®—ã€‚å¦ä¸€æ–¹é¢ï¼Œåœ¨è”åˆå¤„ç†å®ƒä»¬æ—¶é¼“åŠ±æ›´å¥½åœ°èšåˆå¤šä¸ªè¯æ®ç‰‡æ®µã€‚åœ¨æŠ½å–å¼æ–¹æ³•ä¸­ç¼ºå°‘èšåˆéƒ¨åˆ†ã€‚
- en: Note that they did fine-tune the pretrained LM independently for each dataset.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œä»–ä»¬å¯¹é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ç‹¬ç«‹è¿›è¡Œäº†å¾®è°ƒï¼Œé’ˆå¯¹æ¯ä¸ªæ•°æ®é›†ã€‚
- en: 'Closed-book QA: Generative Language Model'
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é—­å·é—®ç­”ï¼šç”Ÿæˆå¼è¯­è¨€æ¨¡å‹
- en: Big language models have been pre-trained on a large collection of unsupervised
    textual corpus. Given enough parameters, these models are able to memorize some
    factual knowledge within parameter weights. Therefore, we can use these models
    to do question-answering without explicit context, just like in a closed-book
    exam. The pre-trained language models produce *free text* to respond to questions,
    no explicit reading comprehension.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å‹è¯­è¨€æ¨¡å‹å·²åœ¨å¤§é‡æ— ç›‘ç£æ–‡æœ¬è¯­æ–™åº“ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒã€‚åœ¨æœ‰è¶³å¤Ÿå‚æ•°çš„æƒ…å†µä¸‹ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿè®°å¿†ä¸€äº›äº‹å®çŸ¥è¯†åœ¨å‚æ•°æƒé‡ä¸­ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™äº›æ¨¡å‹è¿›è¡Œæ— éœ€æ˜ç¡®ä¸Šä¸‹æ–‡çš„é—®ç­”ï¼Œå°±åƒåœ¨é—­å·è€ƒè¯•ä¸­ä¸€æ ·ã€‚é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ç”Ÿæˆ*è‡ªç”±æ–‡æœ¬*ä»¥å›ç­”é—®é¢˜ï¼Œæ²¡æœ‰æ˜ç¡®çš„é˜…è¯»ç†è§£ã€‚
- en: '![](../Images/d4c3b32d9caf8584475b3c5117ad2c4f.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d4c3b32d9caf8584475b3c5117ad2c4f.png)'
- en: 'Fig. 13\. The amount of computation used for training big language models of
    different sizes is getting big. (Image source: [Brown et al., 2020](https://arxiv.org/abs/2005.14165)).'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾13ã€‚ç”¨äºè®­ç»ƒä¸åŒè§„æ¨¡çš„å¤§å‹è¯­è¨€æ¨¡å‹æ‰€ä½¿ç”¨çš„è®¡ç®—é‡æ­£åœ¨å¢åŠ ã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼š[Brownç­‰äººï¼Œ2020](https://arxiv.org/abs/2005.14165)ï¼‰ã€‚
- en: '[Roberts et al. (2020)](https://arxiv.org/abs/2002.08910) measured the practical
    utility of a language model by fine-tuning a pre-trained model to answer questions
    without access to any external context or knowledge. They fine-tuned the [T5](https://arxiv.org/abs/1910.10683)
    language model (same architecture as the original Transformer) to answer questions
    without inputting any additional information or context. Such setup enforces the
    language model to answer questions based on â€œknowledgeâ€ that it internalized during
    pre-training.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[Robertsç­‰äººï¼ˆ2020ï¼‰](https://arxiv.org/abs/2002.08910)é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹æ¥è¡¡é‡è¯­è¨€æ¨¡å‹çš„å®ç”¨æ€§ï¼Œä»¥å›ç­”é—®é¢˜è€Œæ— éœ€è®¿é—®ä»»ä½•å¤–éƒ¨ä¸Šä¸‹æ–‡æˆ–çŸ¥è¯†ã€‚ä»–ä»¬å°†[T5](https://arxiv.org/abs/1910.10683)è¯­è¨€æ¨¡å‹ï¼ˆä¸åŸå§‹Transformerç›¸åŒçš„æ¶æ„ï¼‰å¾®è°ƒä¸ºå›ç­”é—®é¢˜ï¼Œè€Œæ— éœ€è¾“å…¥ä»»ä½•é¢å¤–ä¿¡æ¯æˆ–ä¸Šä¸‹æ–‡ã€‚è¿™ç§è®¾ç½®è¿«ä½¿è¯­è¨€æ¨¡å‹æ ¹æ®åœ¨é¢„è®­ç»ƒæœŸé—´å†…åŒ–çš„â€œçŸ¥è¯†â€æ¥å›ç­”é—®é¢˜ã€‚'
- en: '![](../Images/8018e92fe405f6335004919ac5f4671d.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8018e92fe405f6335004919ac5f4671d.png)'
- en: 'Fig. 14\. T5 is first pre-trained with salient span masking and then fine-tuned
    for each QA dataset to produce answers in free text. (Image source: [Roberts et
    al. 2020](https://arxiv.org/abs/2002.08910))'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾14. T5é¦–å…ˆé€šè¿‡æ˜¾è‘—çš„é®è”½é¢„è®­ç»ƒï¼Œç„¶åé’ˆå¯¹æ¯ä¸ªQAæ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œä»¥ç”Ÿæˆè‡ªç”±æ–‡æœ¬ä¸­çš„ç­”æ¡ˆã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼š[Robertsç­‰äººï¼Œ2020](https://arxiv.org/abs/2002.08910)ï¼‰
- en: The original T5 models were pre-trained on a multi-task mixture including an
    unsupervised [â€œmasked language modelingâ€](https://lilianweng.github.io/posts/2019-01-31-lm/#use-bert-in-downstream-tasks)
    (MLM) tasks on the C4 (â€œColossal Clean Crawled Corpusâ€) dataset as well as fine-tuned
    altogether with supervised translation, summarization, classification, and reading
    comprehension tasks. [Roberts, et al. (2020)](https://arxiv.org/abs/2002.08910)
    took a pre-trained T5 model and continued pre-training with [salient span masking](#ssm)
    over Wikipedia corpus, which has been found to substantially boost the performance
    for ODQA. Then they fine-tuned the model for each QA datasets independently.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹çš„T5æ¨¡å‹åœ¨å¤šä»»åŠ¡æ··åˆä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼ŒåŒ…æ‹¬åœ¨C4ï¼ˆâ€œå·¨å¤§å¹²å‡€çˆ¬å–è¯­æ–™åº“â€ï¼‰æ•°æ®é›†ä¸Šçš„æ— ç›‘ç£[â€œé®è”½è¯­è¨€å»ºæ¨¡â€](https://lilianweng.github.io/posts/2019-01-31-lm/#use-bert-in-downstream-tasks)ï¼ˆMLMï¼‰ä»»åŠ¡ï¼Œä»¥åŠä¸ç›‘ç£ç¿»è¯‘ã€æ‘˜è¦ã€åˆ†ç±»å’Œé˜…è¯»ç†è§£ä»»åŠ¡ä¸€èµ·è¿›è¡Œäº†å¾®è°ƒã€‚[Robertsç­‰äººï¼ˆ2020ï¼‰](https://arxiv.org/abs/2002.08910)é‡‡ç”¨äº†ä¸€ä¸ªé¢„è®­ç»ƒçš„T5æ¨¡å‹ï¼Œå¹¶ç»§ç»­ä½¿ç”¨[æ˜¾è‘—é®è”½](#ssm)åœ¨ç»´åŸºç™¾ç§‘è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œè¿™å·²è¢«å‘ç°å¯ä»¥å¤§å¹…æå‡å¼€æ”¾é¢†åŸŸé—®ç­”çš„æ€§èƒ½ã€‚ç„¶åä»–ä»¬ä¸ºæ¯ä¸ªQAæ•°æ®é›†ç‹¬ç«‹åœ°å¾®è°ƒæ¨¡å‹ã€‚
- en: With a pre-trained T5 language model + continue pre-training with salient spans
    masking + fine-tuning for each QA dataset,
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨é¢„è®­ç»ƒçš„T5è¯­è¨€æ¨¡å‹+ç»§ç»­ä½¿ç”¨æ˜¾è‘—é®è”½è¿›è¡Œé¢„è®­ç»ƒ+é’ˆå¯¹æ¯ä¸ªQAæ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œ
- en: It can attain competitive results in open-domain question answering without
    access to external knowledge.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ƒå¯ä»¥åœ¨æ²¡æœ‰è®¿é—®å¤–éƒ¨çŸ¥è¯†çš„æƒ…å†µä¸‹è·å¾—å¼€æ”¾é¢†åŸŸé—®ç­”çš„ç«äº‰æ€§ç»“æœã€‚
- en: A larger model can obtain better performance. For example, a T5 with 11B parameters
    is able to match the performance with [DPR](#DPR) with 3 BERT-base models, each
    with 330M parameters.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ›´å¤§çš„æ¨¡å‹å¯ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œå…·æœ‰11Bå‚æ•°çš„T5èƒ½å¤Ÿä¸[DPR](#DPR)çš„3ä¸ªBERT-baseæ¨¡å‹ï¼ˆæ¯ä¸ªæ¨¡å‹æœ‰330Må‚æ•°ï¼‰çš„æ€§èƒ½åŒ¹æ•Œã€‚
- en: 'Interestingly, fine-tuning is not strictly necessary. GPT3 ([Brown et al.,
    2020](https://arxiv.org/abs/2005.14165)) has been evaluated on the closed book
    question answering task *without any gradient updates or fine-tuning*. During
    evaluation, the few-shot, one-shot and zero-shot settings here only refer to how
    many demonstrations are provided as context in the text input:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰è¶£çš„æ˜¯ï¼Œå¾®è°ƒå¹¶éç»å¯¹å¿…è¦ã€‚GPT3ï¼ˆ[Brownç­‰äººï¼Œ2020](https://arxiv.org/abs/2005.14165)ï¼‰åœ¨é—­å·é—®ç­”ä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œ*æ²¡æœ‰è¿›è¡Œä»»ä½•æ¢¯åº¦æ›´æ–°æˆ–å¾®è°ƒ*ã€‚åœ¨è¯„ä¼°è¿‡ç¨‹ä¸­ï¼Œè¿™é‡Œçš„å°‘æ¬¡å­¦ä¹ ã€ä¸€æ¬¡å­¦ä¹ å’Œé›¶æ¬¡å­¦ä¹ è®¾ç½®ä»…æŒ‡æä¾›äº†å¤šå°‘æ¼”ç¤ºä½œä¸ºæ–‡æœ¬è¾“å…¥ä¸­çš„ä¸Šä¸‹æ–‡ï¼š
- en: 'â€œfew-shot learningâ€: GPT3 is allowed to take as many demonstrations as what
    can fit into the modelâ€™s context window (typically 10 to 100).'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: â€œå°‘æ¬¡å­¦ä¹ â€ï¼šGPT3å¯ä»¥è·å–å°½å¯èƒ½å¤šçš„æ¼”ç¤ºï¼Œä»¥é€‚åº”æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£ï¼ˆé€šå¸¸ä¸º10åˆ°100ï¼‰ã€‚
- en: 'â€œone-shot learningâ€: only one demonstration is provided.'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: â€œä¸€æ¬¡æ€§å­¦ä¹ â€ï¼šåªæä¾›ä¸€æ¬¡æ¼”ç¤ºã€‚
- en: 'â€œzero-shot learningâ€: no demonstrations are allowed and only an instruction
    in natural language is given to the model.'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: â€œé›¶æ¬¡å­¦ä¹ â€ï¼šä¸å…è®¸æ¼”ç¤ºï¼Œåªå‘æ¨¡å‹æä¾›è‡ªç„¶è¯­è¨€çš„æŒ‡ä»¤ã€‚
- en: The performance grows with the model size. On the TriviaQA dataset, GPT3 evaluation
    with demonstrations can match or exceed the performance of SOTA baseline with
    fine-tuning.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€æ¨¡å‹è§„æ¨¡çš„å¢é•¿ï¼Œæ€§èƒ½ä¹Ÿåœ¨æå‡ã€‚åœ¨TriviaQAæ•°æ®é›†ä¸Šï¼ŒGPT3åœ¨æ¼”ç¤ºçš„æƒ…å†µä¸‹å¯ä»¥è¾¾åˆ°æˆ–è¶…è¿‡é€šè¿‡å¾®è°ƒå¾—åˆ°çš„SOTAåŸºçº¿çš„æ€§èƒ½ã€‚
- en: '![](../Images/492b7ccdac1e5a920e93db076cd87437.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/492b7ccdac1e5a920e93db076cd87437.png)'
- en: 'Fig. 15\. GPT3''s performance on TriviaQA grows smoothly with the model size.
    More demonstrations lead to better performance. (Image source: [Brown et al.,
    2020](https://arxiv.org/abs/2005.14165)).'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾15\. GPT3åœ¨TriviaQAä¸Šçš„è¡¨ç°éšç€æ¨¡å‹è§„æ¨¡çš„å¢é•¿è€Œå¹³ç¨³å¢é•¿ã€‚æ›´å¤šçš„æ¼”ç¤ºä¼šå¯¼è‡´æ›´å¥½çš„è¡¨ç°ã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼š[Brownç­‰äººï¼Œ2020](https://arxiv.org/abs/2005.14165)ï¼‰ã€‚
- en: Check out this cool example in OpenAI API [playground viewer](https://beta.openai.com/playground/p/HMoho4552EHXrPLbmOIxpX4X).
    The model is able to answer factal questions in short answer and not to make up
    things when the model does not know the answer. I added the last two questions
    and asked the model to respond with `A:`. The API is still in beta version, so
    you might need to [apply](https://beta.openai.com/) to get on the wait list.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹OpenAI APIçš„è¿™ä¸ªé…·ä¾‹å­[playground viewer](https://beta.openai.com/playground/p/HMoho4552EHXrPLbmOIxpX4X)ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿä»¥ç®€çŸ­å›ç­”å½¢å¼å›ç­”äº‹å®æ€§é—®é¢˜ï¼Œå¹¶ä¸”åœ¨æ¨¡å‹ä¸çŸ¥é“ç­”æ¡ˆæ—¶ä¸ä¼šå‡­ç©ºæé€ ã€‚æˆ‘æ·»åŠ äº†æœ€åä¸¤ä¸ªé—®é¢˜ï¼Œå¹¶è¦æ±‚æ¨¡å‹ä»¥`A:`å›ç­”ã€‚è¯¥APIä»å¤„äºæµ‹è¯•ç‰ˆï¼Œå› æ­¤æ‚¨å¯èƒ½éœ€è¦[ç”³è¯·](https://beta.openai.com/)åŠ å…¥ç­‰å¾…åˆ—è¡¨ã€‚
- en: '[PRE1]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Related Techniques
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç›¸å…³æŠ€æœ¯
- en: Fast Maximum Inner Product Search (MIPS)
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¿«é€Ÿæœ€å¤§å†…ç§¯æœç´¢ï¼ˆMIPSï¼‰
- en: MIPS (maximum inner product search) is a crucial component in many open-domain
    question answering models. In retriever + reader/generator framework, a large
    number of passages from the knowledge source are encoded and stored in a memory.
    A retrieval model is able to query the memory to identify the top relevant passages
    which have the maximum inner product with the questionâ€™s embedding.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: MIPSï¼ˆæœ€å¤§å†…ç§¯æœç´¢ï¼‰æ˜¯è®¸å¤šå¼€æ”¾é¢†åŸŸé—®ç­”æ¨¡å‹ä¸­çš„å…³é”®ç»„ä»¶ã€‚åœ¨æ£€ç´¢å™¨ + è¯»è€…/ç”Ÿæˆå™¨æ¡†æ¶ä¸­ï¼Œæ¥è‡ªçŸ¥è¯†æºçš„å¤§é‡æ®µè½è¢«ç¼–ç å¹¶å­˜å‚¨åœ¨å†…å­˜ä¸­ã€‚æ£€ç´¢æ¨¡å‹èƒ½å¤ŸæŸ¥è¯¢å†…å­˜ä»¥è¯†åˆ«ä¸é—®é¢˜åµŒå…¥å…·æœ‰æœ€å¤§å†…ç§¯çš„é¡¶çº§ç›¸å…³æ®µè½ã€‚
- en: We need fast MIPS because the number of precomputed passage representations
    can be gigantic. There are several ways to achieve fast MIPS at run time, such
    as [asymmetric LSH](https://papers.nips.cc/paper/5329-asymmetric-lsh-alsh-for-sublinear-time-maximum-inner-product-search-mips.pdf),
    [data-dependent hashing](https://arxiv.org/abs/1501.01062), and [FAISS](https://github.com/facebookresearch/faiss).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éœ€è¦å¿«é€Ÿçš„MIPSï¼Œå› ä¸ºé¢„å…ˆè®¡ç®—çš„æ®µè½è¡¨ç¤ºæ•°é‡å¯èƒ½æ˜¯å·¨å¤§çš„ã€‚åœ¨è¿è¡Œæ—¶å®ç°å¿«é€ŸMIPSæœ‰å‡ ç§æ–¹æ³•ï¼Œä¾‹å¦‚[éå¯¹ç§°LSH](https://papers.nips.cc/paper/5329-asymmetric-lsh-alsh-for-sublinear-time-maximum-inner-product-search-mips.pdf)ã€[æ•°æ®ç›¸å…³å“ˆå¸Œ](https://arxiv.org/abs/1501.01062)å’Œ[FAISS](https://github.com/facebookresearch/faiss)ã€‚
- en: Language Model Pre-training
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒ
- en: Two pre-training tasks are especially helpful for QA tasks, as we have discussed
    above.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¤ä¸ªé¢„è®­ç»ƒä»»åŠ¡å¯¹QAä»»åŠ¡ç‰¹åˆ«æœ‰å¸®åŠ©ï¼Œæ­£å¦‚æˆ‘ä»¬ä¸Šé¢æ‰€è®¨è®ºçš„ã€‚
- en: '**Inverse Cloze Task** (proposed by [ORQA](#ORQA)): The goal of [Cloze Task](https://en.wikipedia.org/wiki/Cloze_test)
    is to predict masked-out text based on its context. The prediction of Inverse
    Cloze Task (ICT) is in the reverse direction, aiming to predict the context given
    a sentence. In the context of QA tasks, a random sentence can be treated as a
    pseudo-question, and its context can be treated as pseudo-evidence.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é€†å‘å¡«ç©ºä»»åŠ¡**ï¼ˆç”±[ORQA](#ORQA)æå‡ºï¼‰ï¼š[å¡«ç©ºä»»åŠ¡](https://en.wikipedia.org/wiki/Cloze_test)çš„ç›®æ ‡æ˜¯æ ¹æ®ä¸Šä¸‹æ–‡é¢„æµ‹è¢«å±è”½çš„æ–‡æœ¬ã€‚é€†å‘å¡«ç©ºä»»åŠ¡ï¼ˆICTï¼‰çš„é¢„æµ‹æ–¹å‘ç›¸åï¼Œæ—¨åœ¨æ ¹æ®ä¸€ä¸ªå¥å­é¢„æµ‹ä¸Šä¸‹æ–‡ã€‚åœ¨QAä»»åŠ¡çš„èƒŒæ™¯ä¸‹ï¼Œä¸€ä¸ªéšæœºå¥å­å¯ä»¥è¢«è§†ä¸ºä¼ªé—®é¢˜ï¼Œå…¶ä¸Šä¸‹æ–‡å¯ä»¥è¢«è§†ä¸ºä¼ªè¯æ®ã€‚'
- en: '**Salient Spans Masking** (proposed by [REALM](#REALM)): Salient span masking
    is a special case for MLM task in language model training. First, we find *salient
    spans* by using a tagger to identify named entities and a regular expression to
    identify dates. Then one of the detected salient spans is selected and masked.
    The task is to predict this masked salient span.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ˜¾è‘—è·¨åº¦å±è”½**ï¼ˆç”±[REALM](#REALM)æå‡ºï¼‰ï¼šæ˜¾è‘—è·¨åº¦å±è”½æ˜¯è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­MLMä»»åŠ¡çš„ä¸€ä¸ªç‰¹æ®Šæƒ…å†µã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨æ ‡è®°å™¨è¯†åˆ«å‘½åå®ä½“å’Œæ­£åˆ™è¡¨è¾¾å¼è¯†åˆ«æ—¥æœŸæ¥æ‰¾åˆ°*æ˜¾è‘—è·¨åº¦*ã€‚ç„¶åé€‰æ‹©å¹¶å±è”½æ£€æµ‹åˆ°çš„ä¸€ä¸ªæ˜¾è‘—è·¨åº¦ã€‚ä»»åŠ¡æ˜¯é¢„æµ‹è¿™ä¸ªè¢«å±è”½çš„æ˜¾è‘—è·¨åº¦ã€‚'
- en: Summary
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: '| Model | Retriever | Reader / Generator | Pre-training / Fine-tuning | End2end
    |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| æ¨¡å‹ | æ£€ç´¢å™¨ | è¯»è€… / ç”Ÿæˆå™¨ | é¢„è®­ç»ƒ / å¾®è°ƒ | ç«¯åˆ°ç«¯ |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| DrQA | TF-IDF | Bi-directional LSTM | â€“ | No |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| DrQA | TF-IDF | åŒå‘LSTM | â€“ | å¦ |'
- en: '| BERTserini | Aserini + BM25 | BERT without softmax layer | Fine-tune with
    SQuAD | No |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| BERTserini | Aserini + BM25 | æ²¡æœ‰softmaxå±‚çš„BERT | ä¸SQuADå¾®è°ƒ | å¦ |'
- en: '| Multi-passage BERT | ElasticSearch + BM25 | Multi-passage BERT + Passage
    ranker |  | No |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| å¤šæ®µBERT | ElasticSearch + BM25 | å¤šæ®µBERT + æ®µè½æ’åºå™¨ |  | å¦ |'
- en: '| R^3 | Classic IR + Match-LSTM | Match-LSTM |  | Yes |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| R^3 | ç»å…¸IR + Match-LSTM | Match-LSTM |  | æ˜¯ |'
- en: '| ORQA | Dot product of BERT embeddings | BERT-RC | Inverse cloze task | Yes
    |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| ORQA | BERTåµŒå…¥çš„ç‚¹ç§¯ | BERT-RC | é€†å‘å¡«ç©ºä»»åŠ¡ | æ˜¯ |'
- en: '| REALM | Dot product of BERT embeddings | BERT-RC | Salient span masking |
    Yes |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| REALM | BERTåµŒå…¥çš„ç‚¹ç§¯ | BERT-RC | æ˜¾è‘—è·¨åº¦å±è”½ | æ˜¯ |'
- en: '| DPR | Dot product of BERT embeddings | BERT-RC | supervised training with
    QA pairs | Yes |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| DPR | BERTåµŒå…¥çš„ç‚¹ç§¯ | BERT-RC | é€šè¿‡QAå¯¹è¿›è¡Œç›‘ç£è®­ç»ƒ | æ˜¯ |'
- en: '| DenSPI | Classic + Neural IR | â€“ |  | Yes |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| DenSPI | ç»å…¸ + ç¥ç»ä¿¡æ¯æ£€ç´¢ | â€“ |  | æ˜¯ |'
- en: '| T5 + SSM | â€“ | T5 | SSM on [CommonCrawl](https://commoncrawl.org/the-data/get-started/)
    data + Fine-tuning on QA data | Yes |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| T5 + SSM | â€“ | T5 | SSMåœ¨[CommonCrawl](https://commoncrawl.org/the-data/get-started/)æ•°æ®ä¸Š
    + åœ¨QAæ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒ | æ˜¯ |'
- en: '| GPT3 | â€“ | GPT3 | NSP on [CommonCrawl](https://commoncrawl.org/the-data/get-started/)
    data | Yes |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| GPT3 | â€“ | GPT3 | NSPåœ¨[CommonCrawl](https://commoncrawl.org/the-data/get-started/)æ•°æ®ä¸Š
    | æ˜¯ |'
- en: '| RAG | DPR retriever | [BART](https://arxiv.org/abs/1910.13461) |  | Yes |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| RAG | DPRæ£€ç´¢å™¨ | [BART](https://arxiv.org/abs/1910.13461) |  | æ˜¯ |'
- en: '| Fusion-in-Decoder | BM25 / DPR retriever | Tranformer |  | No |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| Fusion-in-Decoder | BM25 / DPRæ£€ç´¢å™¨ | Tranformer |  | å¦ |'
- en: '![](../Images/c272a31e9b28bd10d951b6e49a223503.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c272a31e9b28bd10d951b6e49a223503.png)'
- en: 'Fig. 16\. A comparison of performance of several QA models on common QA datasets.
    On TriviaQA, two columns of results are reported, on the open domain test set
    (left) and on the hidden test set (right). (Image source: [Izacard & Grave, 2020](https://arxiv.org/abs/2007.01282)).'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾16. å‡ ä¸ªé—®ç­”æ¨¡å‹åœ¨å¸¸è§é—®ç­”æ•°æ®é›†ä¸Šçš„æ€§èƒ½æ¯”è¾ƒã€‚åœ¨TriviaQAä¸Šï¼ŒæŠ¥å‘Šäº†ä¸¤åˆ—ç»“æœï¼Œåˆ†åˆ«æ˜¯å¼€æ”¾é¢†åŸŸæµ‹è¯•é›†ï¼ˆå·¦ï¼‰å’Œéšè—æµ‹è¯•é›†ï¼ˆå³ï¼‰ã€‚ (å›¾ç‰‡æ¥æºï¼š[Izacard
    & Grave, 2020](https://arxiv.org/abs/2007.01282))ã€‚
- en: Citation
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¼•ç”¨
- en: 'Cited as:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: è¢«å¼•ç”¨ä¸ºï¼š
- en: Weng, Lilian. (Oct 2020). How to build an open-domain question answering system?
    Lilâ€™Log. https://lilianweng.github.io/posts/2020-10-29-odqa/.
  id: totrans-229
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Weng, Lilian. (2020å¹´10æœˆ). å¦‚ä½•æ„å»ºä¸€ä¸ªå¼€æ”¾é¢†åŸŸçš„é—®ç­”ç³»ç»Ÿï¼ŸLilâ€™Log. https://lilianweng.github.io/posts/2020-10-29-odqa/.
- en: Or
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–
- en: '[PRE2]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Appendix: QA Datasets'
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é™„å½•ï¼šé—®ç­”æ•°æ®é›†
- en: '[SQuAD 2.0](https://rajpurkar.github.io/SQuAD-explorer/): the Stanford QA dataset.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SQuAD 2.0](https://rajpurkar.github.io/SQuAD-explorer/)ï¼šæ–¯å¦ç¦é—®ç­”æ•°æ®é›†ã€‚'
- en: '[RACE](http://www.qizhexie.com/data/RACE_leaderboard): a reading comprehension
    dataset collected from English Examinations that are created for middle school
    and high school students.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[RACE](http://www.qizhexie.com/data/RACE_leaderboard)ï¼šä»ä¸ºä¸­å­¦å’Œé«˜ä¸­å­¦ç”Ÿåˆ›å»ºçš„è‹±è¯­è€ƒè¯•ä¸­æ”¶é›†çš„é˜…è¯»ç†è§£æ•°æ®é›†ã€‚'
- en: '[TREC QA](https://trec.nist.gov/data/qa.html): the TREC QA collections.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TRECé—®ç­”](https://trec.nist.gov/data/qa.html)ï¼šTRECé—®ç­”é›†åˆã€‚'
- en: '[MS MARCO](https://microsoft.github.io/msmarco/): a QA dataset featuring 100,000
    real Bing questions and a human generated answer.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MS MARCO](https://microsoft.github.io/msmarco/)ï¼šä¸€ä¸ªåŒ…å«10ä¸‡ä¸ªçœŸå®å¿…åº”é—®é¢˜å’Œäººå·¥ç”Ÿæˆç­”æ¡ˆçš„é—®ç­”æ•°æ®é›†ã€‚'
- en: '[CuratedTREC](https://github.com/brmson/dataset-factoid-curated): based on
    the benchmarks from the TREC QA tasks that have been curated by [Baudis & Sedivy
    (2015)](https://link.springer.com/chapter/10.1007%2F978-3-319-24027-5_20).'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CuratedTREC](https://github.com/brmson/dataset-factoid-curated)ï¼šåŸºäºTRECé—®ç­”ä»»åŠ¡çš„åŸºå‡†ï¼Œç”±[Baudis
    & Sedivy (2015)](https://link.springer.com/chapter/10.1007%2F978-3-319-24027-5_20)ç­–åˆ’ã€‚'
- en: '[Google Natural Questions](https://ai.google.com/research/NaturalQuestions/dataset):
    contains real user questions issued to Google search, and answers found from Wikipedia
    by annotators.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Googleè‡ªç„¶é—®é¢˜](https://ai.google.com/research/NaturalQuestions/dataset)ï¼šåŒ…å«å‘Googleæœç´¢å‘å‡ºçš„çœŸå®ç”¨æˆ·é—®é¢˜ï¼Œä»¥åŠç”±æ³¨é‡Šè€…ä»ç»´åŸºç™¾ç§‘æ‰¾åˆ°çš„ç­”æ¡ˆã€‚'
- en: '[WebQuestions](https://github.com/brmson/dataset-factoid-webquestions): designed
    for knowledge-base QA with answers restricted to Freebase entities.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[WebQuestions](https://github.com/brmson/dataset-factoid-webquestions)ï¼šè®¾è®¡ç”¨äºåŸºäºçŸ¥è¯†åº“çš„é—®ç­”ï¼Œç­”æ¡ˆé™åˆ¶åœ¨Freebaseå®ä½“ä¸Šã€‚'
- en: '[WikiQA](https://www.microsoft.com/en-us/research/publication/wikiqa-a-challenge-dataset-for-open-domain-question-answering/):
    Bing query logs were used as the source of questions. Each question is then linked
    to a Wikipedia page that potentially contains the answer.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[WikiQA](https://www.microsoft.com/en-us/research/publication/wikiqa-a-challenge-dataset-for-open-domain-question-answering/)ï¼šå¿…åº”æŸ¥è¯¢æ—¥å¿—è¢«ç”¨ä½œé—®é¢˜çš„æ¥æºã€‚ç„¶åå°†æ¯ä¸ªé—®é¢˜é“¾æ¥åˆ°æ½œåœ¨åŒ…å«ç­”æ¡ˆçš„ç»´åŸºç™¾ç§‘é¡µé¢ã€‚'
- en: '[WikiMovies](https://research.fb.com/downloads/babi/): contains movie-related
    questions from the OMDb and MovieLens databases and where the questions can be
    answered using Wikipedia pages.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[WikiMovies](https://research.fb.com/downloads/babi/)ï¼šåŒ…å«æ¥è‡ªOMDbå’ŒMovieLensæ•°æ®åº“çš„ä¸ç”µå½±ç›¸å…³çš„é—®é¢˜ï¼Œè¿™äº›é—®é¢˜å¯ä»¥ä½¿ç”¨ç»´åŸºç™¾ç§‘é¡µé¢å›ç­”ã€‚'
- en: '[WikiReading](https://github.com/google-research-datasets/wiki-reading): to
    predict textual values from the structured knowledge base Wikidata by reading
    the text of the corresponding Wikipedia articles.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[WikiReading](https://github.com/google-research-datasets/wiki-reading)ï¼šé€šè¿‡é˜…è¯»ç›¸åº”ç»´åŸºç™¾ç§‘æ–‡ç« çš„æ–‡æœ¬æ¥é¢„æµ‹ç»“æ„åŒ–çŸ¥è¯†åº“Wikidataä¸­çš„æ–‡æœ¬å€¼ã€‚'
- en: '[TriviaQA](https://nlp.cs.washington.edu/triviaqa/): a reading comprehension
    dataset containing 95K question-answer pairs authored by trivia enthusiasts and
    independently gathered multiple evidence documents per question.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TriviaQA](https://nlp.cs.washington.edu/triviaqa/)ï¼šä¸€ä¸ªåŒ…å«95Ké—®é¢˜-ç­”æ¡ˆå¯¹çš„é˜…è¯»ç†è§£æ•°æ®é›†ï¼Œç”±çˆ±å¥½è€…ç¼–å†™ï¼Œå¹¶ç‹¬ç«‹æ”¶é›†æ¯ä¸ªé—®é¢˜çš„å¤šä¸ªè¯æ®æ–‡æ¡£ã€‚'
- en: '[Jeopardy! Questions](https://www.kaggle.com/tunguz/200000-jeopardy-questions):
    contains 200,000+ [Jeopardy!](https://en.wikipedia.org/wiki/Jeopardy!) questions.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Jeopardy! é—®é¢˜](https://www.kaggle.com/tunguz/200000-jeopardy-questions)ï¼šåŒ…å«
    200,000+ [Jeopardy!](https://en.wikipedia.org/wiki/Jeopardy!) é—®é¢˜ã€‚'
- en: '[DeepMind Q&A Dataset](https://cs.nyu.edu/~kcho/DMQA/): question/answer pairs
    from CNN and Daily Mail articles.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DeepMind Q&A æ•°æ®é›†](https://cs.nyu.edu/~kcho/DMQA/)ï¼šæ¥è‡ª CNN å’Œ Daily Mail æ–‡ç« çš„é—®é¢˜/ç­”æ¡ˆå¯¹ã€‚'
- en: '[bAbi](https://research.fb.com/downloads/babi/): a rich collection of datasets
    for text understanding by Facebook.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[bAbi](https://research.fb.com/downloads/babi/)ï¼šFacebook ç”¨äºæ–‡æœ¬ç†è§£çš„ä¸°å¯Œæ•°æ®é›†é›†åˆã€‚'
- en: '[FEVER](https://fever.ai/data.html): for fact extraction and verification.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[FEVER](https://fever.ai/data.html)ï¼šç”¨äºäº‹å®æå–å’ŒéªŒè¯ã€‚'
- en: '[SearchQA](https://github.com/nyu-dl/dl4ir-searchQA): question-answer pairs
    were crawled from from [J! Archive](https://j-archive.com/), and then augmented
    with text snippets from Google.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SearchQA](https://github.com/nyu-dl/dl4ir-searchQA)ï¼šé—®é¢˜-ç­”æ¡ˆå¯¹ä» [J! Archive](https://j-archive.com/)
    çˆ¬å–ï¼Œç„¶åç”¨ Google çš„æ–‡æœ¬ç‰‡æ®µè¿›è¡Œå¢å¼ºã€‚'
- en: '[Quasar-T](https://github.com/bdhingra/quasar): a collection of open-domain
    trivia questions and their answers obtained from various internet sources.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Quasar-T](https://github.com/bdhingra/quasar)ï¼šä»å„ç§äº’è”ç½‘æ¥æºè·å–çš„å¼€æ”¾é¢†åŸŸçäº‹é—®é¢˜åŠå…¶ç­”æ¡ˆçš„é›†åˆã€‚'
- en: '[Quiz bowl](https://people.cs.umass.edu/~miyyer/qblearn/index.html): contains
    data from a trivia competition called quiz bowl.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Quiz bowl](https://people.cs.umass.edu/~miyyer/qblearn/index.html)ï¼šåŒ…å«æ¥è‡ªåä¸º
    quiz bowl çš„çäº‹ç«èµ›çš„æ•°æ®ã€‚'
- en: '[AmbigNQ](https://nlp.cs.washington.edu/ambigqa/): ambiguous questions selected
    from NQ-OPEN dataset.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AmbigNQ](https://nlp.cs.washington.edu/ambigqa/)ï¼šä» NQ-OPEN æ•°æ®é›†ä¸­é€‰å‡ºçš„æ¨¡ç³Šé—®é¢˜ã€‚'
- en: '[QA-Overlap](https://github.com/facebookresearch/QA-Overlap): a collections
    of overlapped answers/questions between train and test set for Natural Questions,
    TriviaQA, and WebQuestions.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[QA-Overlap](https://github.com/facebookresearch/QA-Overlap)ï¼šè‡ªç„¶é—®é¢˜ã€TriviaQAå’ŒWebQuestionsè®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¹‹é—´é‡å ç­”æ¡ˆ/é—®é¢˜çš„é›†åˆã€‚'
- en: References
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] Danqi Chen & Scott Yih. [â€œACL2020 Tutorial: Open-Domain Question Answeringâ€](https://github.com/danqi/acl2020-openqa-tutorial)
    July 2020.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] é™ˆä¸¹ç¦å’Œæ–¯ç§‘ç‰¹Â·ä¼Šã€‚[â€œACL2020 æ•™ç¨‹ï¼šå¼€æ”¾é¢†åŸŸé—®ç­”â€](https://github.com/danqi/acl2020-openqa-tutorial)
    2020å¹´7æœˆã€‚'
- en: '[2] Danqi Chen, et al. [â€œReading Wikipedia to Answer Open-Domain Questionsâ€](https://arxiv.org/abs/1704.00051)
    ACL 2017\. | [code](https://github.com/facebookresearch/DrQA)'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] é™ˆä¸¹ç¦ç­‰äººã€‚[â€œé˜…è¯»ç»´åŸºç™¾ç§‘ä»¥å›ç­”å¼€æ”¾é¢†åŸŸé—®é¢˜â€](https://arxiv.org/abs/1704.00051) ACL 2017ã€‚|
    [code](https://github.com/facebookresearch/DrQA)'
- en: '[3] Shuohang Wang, et al. [â€œR^3: Reinforced Ranker-Reader for Open-Domain Question
    Answeringâ€](https://arxiv.org/abs/1709.00023) AAAI 2018.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] ç‹ç¡•èˆªç­‰äººã€‚[â€œR^3ï¼šå¼ºåŒ–æ’åº-é˜…è¯»å™¨ç”¨äºå¼€æ”¾é¢†åŸŸé—®ç­”â€](https://arxiv.org/abs/1709.00023) AAAI 2018ã€‚'
- en: '[4] Jimmy Lin. [â€œThe neural hype and comparisons against weak baselines.â€](https://sigir.org/wp-content/uploads/2019/01/p040.pdf)
    ACM SIGIR Forum. Vol. 52\. No. 2\. 2019.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] å‰ç±³Â·æ—ã€‚[â€œç¥ç»ç‚’ä½œå’Œä¸å¼±åŸºçº¿çš„æ¯”è¾ƒã€‚â€](https://sigir.org/wp-content/uploads/2019/01/p040.pdf)
    ACM SIGIR è®ºå›ã€‚Vol. 52ã€‚No. 2ã€‚2019å¹´ã€‚'
- en: '[5] Wei Yang, et al. [â€œEnd-to-End Open-Domain Question Answering with BERTseriniâ€](https://arxiv.org/abs/1902.01718)
    NAACL 2019.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] æ¨ä¼Ÿç­‰äººã€‚[â€œä½¿ç”¨ BERTserini è¿›è¡Œç«¯åˆ°ç«¯å¼€æ”¾é¢†åŸŸé—®ç­”â€](https://arxiv.org/abs/1902.01718) NAACL
    2019ã€‚'
- en: '[6] Christopher Clark & Matt Gardner. [â€œSimple and Effective Multi-Paragraph
    Reading Comprehension.â€](https://arxiv.org/abs/1710.10723) arXiv:1710.10723 (2017).'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] å…‹é‡Œæ–¯æ‰˜å¼—Â·å…‹æ‹‰å…‹å’Œé©¬ç‰¹Â·åŠ å¾·çº³ã€‚[â€œç®€å•è€Œæœ‰æ•ˆçš„å¤šæ®µé˜…è¯»ç†è§£ã€‚â€](https://arxiv.org/abs/1710.10723) arXiv:1710.10723
    (2017)ã€‚'
- en: '[7] Rodrigo Nogueira & Kyunghyun Cho. [â€œPassage Re-ranking with BERT.â€](https://arxiv.org/abs/1901.04085)
    arXiv preprint arXiv:1901.04085 (2019). | [code](https://github.com/nyu-dl/dl4marco-bert)'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Rodrigo Nogueira å’Œ Kyunghyun Choã€‚[â€œä½¿ç”¨ BERT è¿›è¡Œæ®µè½é‡æ–°æ’åºã€‚â€](https://arxiv.org/abs/1901.04085)
    arXiv é¢„å°æœ¬ arXiv:1901.04085 (2019)ã€‚| [code](https://github.com/nyu-dl/dl4marco-bert)'
- en: '[8] Zhiguo Wang, et al. [â€œMulti-passage BERT: A globally normalized BERT model
    for open-domain question answering.â€](https://arxiv.org/abs/1908.08167) EMNLP
    2019.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] ç‹å¿—å›½ç­‰äººã€‚[â€œå¤šæ®µ BERTï¼šç”¨äºå¼€æ”¾é¢†åŸŸé—®ç­”çš„å…¨å±€å½’ä¸€åŒ– BERT æ¨¡å‹ã€‚â€](https://arxiv.org/abs/1908.08167)
    EMNLP 2019ã€‚'
- en: '[9] Minjoon Seo et al. [â€œReal-time open-domain question answering with dense-sparse
    phrase index.â€](https://arxiv.org/abs/1906.05807) ACL 2019.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] å¾æ•ä¿Šç­‰äººã€‚[â€œå…·æœ‰ç¨ å¯†-ç¨€ç–çŸ­è¯­ç´¢å¼•çš„å®æ—¶å¼€æ”¾é¢†åŸŸé—®ç­”ã€‚â€](https://arxiv.org/abs/1906.05807) ACL 2019ã€‚'
- en: '[10] Kenton Lee, et al. [â€œLatent Retrieval for Weakly Supervised Open Domain
    Question Answeringâ€](https://arxiv.org/abs/1906.00300) ACL 2019.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Kenton Lee ç­‰äººã€‚[â€œå¼±ç›‘ç£å¼€æ”¾é¢†åŸŸé—®ç­”çš„æ½œåœ¨æ£€ç´¢â€](https://arxiv.org/abs/1906.00300) ACL
    2019ã€‚'
- en: '[11] Kelvin Guu, et al. [â€œREALM: Retrieval-Augmented Language Model Pre-Trainingâ€](https://arxiv.org/abs/2002.08909)
    arXiv:2002.08909 (2020).'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Kelvin Guu ç­‰äººã€‚[â€œREALMï¼šæ£€ç´¢å¢å¼ºè¯­è¨€æ¨¡å‹é¢„è®­ç»ƒâ€](https://arxiv.org/abs/2002.08909)
    arXiv:2002.08909 (2020)ã€‚'
- en: '[12] Vladimir Karpukhin et al. [â€œDense passage retrieval for open-domain question
    answering.â€](https://arxiv.org/abs/2004.04906). EMNLP 2020\. | [code](https://github.com/facebookresearch/DPR)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Vladimir Karpukhinç­‰äººã€‚[â€œç”¨äºå¼€æ”¾é¢†åŸŸé—®ç­”çš„å¯†é›†æ®µè½æ£€ç´¢ã€‚â€](https://arxiv.org/abs/2004.04906)
    EMNLP 2020ã€‚ | [ä»£ç ](https://github.com/facebookresearch/DPR)'
- en: '[13] Patrick Lewis et al. [â€œRetrieval-Augmented Generation for Knowledge-Intensive
    NLP Tasksâ€](https://arxiv.org/abs/2005.11401) arXiv:2005.11401 (2020).'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Patrick Lewisç­‰äººã€‚[â€œç”¨äºçŸ¥è¯†å¯†é›†å‹è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„æ£€ç´¢å¢å¼ºç”Ÿæˆâ€](https://arxiv.org/abs/2005.11401)
    arXiv:2005.11401 (2020)ã€‚'
- en: '[14] Adam Roberts, et al. [â€œHow Much Knowledge Can You Pack Into the Parameters
    of a Language Model?â€](https://arxiv.org/abs/2002.08910) EMNLP 2020.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Adam Robertsç­‰äººã€‚[â€œè¯­è¨€æ¨¡å‹å‚æ•°ä¸­å¯ä»¥åŒ…å«å¤šå°‘çŸ¥è¯†ï¼Ÿâ€](https://arxiv.org/abs/2002.08910)
    EMNLP 2020ã€‚'
- en: '[15] Tom Brown, et al. [â€œLanguage models are few-shot learners.â€](https://arxiv.org/abs/2005.14165)
    arXiv:2005.14165 (2020).'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Tom Brownç­‰äººã€‚[â€œè¯­è¨€æ¨¡å‹æ˜¯å°‘æ ·æœ¬å­¦ä¹ è€…ã€‚â€](https://arxiv.org/abs/2005.14165) arXiv:2005.14165
    (2020)ã€‚'
- en: '[16] Fabio Petroni, et al. [â€œHow Context Affects Language Modelsâ€™ Factual Predictionsâ€](https://arxiv.org/abs/2005.04611)
    AKBC 2020.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Fabio Petroniç­‰äººã€‚[â€œä¸Šä¸‹æ–‡å¦‚ä½•å½±å“è¯­è¨€æ¨¡å‹çš„äº‹å®é¢„æµ‹â€](https://arxiv.org/abs/2005.04611)
    AKBC 2020ã€‚'
- en: '[17] Gautier Izacard & Edouard Grave. [â€œLeveraging passage retrieval with generative
    models for open domain question answering.â€](https://arxiv.org/abs/2007.01282)
    arXiv:2007.01282 (2020).'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Gautier Izacardå’ŒEdouard Graveã€‚[â€œåˆ©ç”¨ç”Ÿæˆæ¨¡å‹çš„æ®µè½æ£€ç´¢è¿›è¡Œå¼€æ”¾é¢†åŸŸé—®ç­”ã€‚â€](https://arxiv.org/abs/2007.01282)
    arXiv:2007.01282 (2020)ã€‚'
- en: '[18] [â€œDive into deep learning: Beam searchâ€](https://d2l.ai/chapter_recurrent-modern/beam-search.html)'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] [â€œæ·±å…¥æ·±åº¦å­¦ä¹ ï¼šæ³¢æŸæœç´¢â€](https://d2l.ai/chapter_recurrent-modern/beam-search.html)'
- en: '[19] Patrick Lewis, et al. [â€œQuestion and Answer Test-Train Overlap in Open-Domain
    Question Answering Datasetsâ€](https://arxiv.org/abs/2008.02637) arXiv:2008.02637
    (2020). | [data](https://github.com/facebookresearch/QA-Overlap)'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Patrick Lewisç­‰äººã€‚[â€œå¼€æ”¾é¢†åŸŸé—®ç­”æ•°æ®é›†ä¸­çš„é—®é¢˜å’Œç­”æ¡ˆæµ‹è¯•è®­ç»ƒé‡å â€](https://arxiv.org/abs/2008.02637)
    arXiv:2008.02637 (2020)ã€‚ | [æ•°æ®](https://github.com/facebookresearch/QA-Overlap)'
- en: '[20] HervÃ© Jegou, et al. [â€œFaiss: A library for efficient similarity searchâ€](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/)
    Mar 2017.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] HervÃ© Jegouç­‰äººã€‚[â€œFaissï¼šé«˜æ•ˆç›¸ä¼¼æ€§æœç´¢çš„åº“â€](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/)
    2017å¹´3æœˆã€‚'
- en: '[21] Vidhisha Balachandran, et al. [â€œSimple and Efficient ways to Improve REALM.â€](https://arxiv.org/abs/2104.08710)
    arXiv:2104.08710 (2021).'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] Vidhisha Balachandranç­‰äººã€‚[â€œæ”¹è¿›REALMçš„ç®€å•æœ‰æ•ˆæ–¹æ³•ã€‚â€](https://arxiv.org/abs/2104.08710)
    arXiv:2104.08710 (2021)ã€‚'
