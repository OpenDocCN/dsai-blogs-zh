- en: Large Transformer Model Inference Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://lilianweng.github.io/posts/2023-01-10-inference-optimization/](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Updated on 2023-01-24: add a small section on [Distillation](#distillation).]'
  prefs: []
  type: TYPE_NORMAL
- en: Large transformer models are mainstream nowadays, creating SoTA results for
    a variety of tasks. They are powerful but very expensive to train and use. The
    extremely high inference cost, in both time and memory, is a big bottleneck for
    adopting a powerful transformer for solving real-world tasks at scale.
  prefs: []
  type: TYPE_NORMAL
- en: '**Why is it hard to run inference for large transformer models?** Besides the
    increasing size of SoTA models, there are two main factors contributing to the
    inference challenge ([Pope et al. 2022](https://arxiv.org/abs/2211.05102)):'
  prefs: []
  type: TYPE_NORMAL
- en: '*Large memory footprint*. Both model parameters and intermediate states are
    needed in memory at inference time. For example,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The KV cache should be stored in memory during decoding time; E.g. For a batch
    size of 512 and context length of 2048, the KV cache totals 3TB, that is 3x the
    model size (!).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Inference cost from the attention mechanism scales quadratically with input
    sequence length.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Low parallelizability.* Inference generation is executed in an autoregressive
    fashion, making the decoding process hard to parallel.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this post, we will look into several approaches for making transformer inference
    more efficient. Some are general network compression methods, while others are
    specific to transformer architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Methods Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We in general consider the following as goals for model inference optimization:'
  prefs: []
  type: TYPE_NORMAL
- en: Reduce the memory footprint of the model by using fewer GPU devices and less
    GPU memory;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce the desired computation complexity by lowering the number of FLOPs needed;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce the inference latency and make things run faster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Several methods can be used to make inference cheaper in memory or/and faster
    in time.
  prefs: []
  type: TYPE_NORMAL
- en: Apply various *parallelism* to scale up the model across a large number of GPUs.
    Smart parallelism of model components and data makes it possible to run a model
    of trillions of parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Memory *offloading* to offload temporarily unused data to the CPU and read them
    back when needed later. This helps with memory usage but causes higher latency.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Smart batching strategy; E.g. [EffectiveTransformer](https://github.com/bytedance/effective_transformer)
    packs consecutive sequences together to remove padding within one batch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Network *compression* techniques, such as *pruning, quantization, distillation*.
    A model of smaller size, in terms of parameter count or bitwidth, should demand
    less memory and run faster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Improvement specific to a target model architecture. Many *architectural changes*,
    especially those for attention layers, help with transformer decoding speed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check [the previous post on large model training](https://lilianweng.github.io/posts/2021-09-25-train-large/)
    on different types of training parallelism and memory saving designs including
    CPU memory offloading. This post focuses on network compression techniques and
    architecture-specific improvement for transformer models.
  prefs: []
  type: TYPE_NORMAL
- en: Distillation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Knowledge Distillation** (**KD**; [Hinton et al. 2015](https://arxiv.org/abs/1503.02531),
    [Gou et al. 2020](https://arxiv.org/abs/2006.05525)) is a straightforward way
    to build a smaller, cheaper model (*“student model”*) to speed up inference by
    transferring skills from a pre-trained expensive model (*“teacher model”*) into
    the student. There is no much restriction on how the student architecture should
    be constructed, except for a matched output space with the teacher in order to
    construct a proper learning objective.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/725a3661459727ebe97c6bb59776196d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 1\. The generic framework of teacher-student knowledge distillation training.
    (Image source: [Gou et al. 2020](”https://arxiv.org/abs/2006.05525”))'
  prefs: []
  type: TYPE_NORMAL
- en: Given a dataset, a student model is trained to mimic outputs of a teacher via
    distillation loss. Usually a neural network has a softmax layer; For example,
    a LLM outputs a probability distribution over tokens. Let’s denote the logits
    layer right before softmax as $\mathbf{z}_t$ and $\mathbf{z}_s$ for teacher and
    student models, respectively. The *distillation loss* minimizes the difference
    between two softmax outputs with a high temperature $T$. When ground truth labels
    $\mathbf{y}$ are known, we can combine it with a *supervised* learning objective
    between ground truth and the student’s soft logits using e.g. cross-entropy.
  prefs: []
  type: TYPE_NORMAL
- en: $$ \mathcal{L}_\text{KD} = \mathcal{L}_\text{distll}(\text{softmax}(\mathbf{z}_t,
    T), \text{softmax}(\mathbf{z}_s, T)) + \lambda\mathcal{L}_\text{CE}(\mathbf{y},
    \mathbf{z}_s) $$
  prefs: []
  type: TYPE_NORMAL
- en: where $\lambda$ is a hyperparameter to balance between soft and hard learning
    objectives. A common choice for $\mathcal{L}_\text{distll}$ is KL divergence /
    cross entropy.
  prefs: []
  type: TYPE_NORMAL
- en: A successful early trial is **DistilBERT** ([Sanh et al. 2019](https://arxiv.org/abs/1910.01108))
    that is able to reduce the parameters of a BERT by 40% while maintaining 97% performance
    of BERT on fine-tuned downstream tasks and running 71% faster. The loss of pre-training
    DistilBERT is a combination of soft distillation loss, supervised training loss
    (i.e. [Masked language modeling loss](https://lilianweng.github.io/posts/2019-01-31-lm/#MLM)
    $\mathcal{L}_\text{MLM}$ in the case of BERT) and a special *cosine embedding
    loss* to align the hidden state vectors between teacher and student.
  prefs: []
  type: TYPE_NORMAL
- en: Distillation can be easily combined with [quantization](#quantization), [pruning](#pruning)
    or [sparsification](#sparsity) techniques, where the teacher model is the original
    full-precision, dense model and the student is quantized, pruned, or trimmed to
    have higher sparsity level.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two common approaches for applying quantization on a deep neural
    network:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Post-Training Quantization (PTQ)*: A model is first trained to convergence
    and then we convert its weights to lower precision without more training. It is
    usually quite cheap to implement, in comparison to training.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Quantization-Aware Training (QAT)*: Quantization is applied during pre-training
    or further fine-tuning. QAT is able to attain better performance but requires
    extra computation resources and access to representative training data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We should be aware of the gap between theoretical optimal quantization strategy
    and the hardware kernel support. Due to the lack of GPU kernel support for certain
    types of matrix multiplication (e.g. INT4 x FP16), not all the methods below result
    in speedup for the actual inference.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges for Transformer Quantization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many studies on Transformer model quantization have the same observation: A
    simple low-precision (e.g. 8-bit) post-training quantization leads to significant
    performance drop mainly due to the high dynamic ranges of activation and a naive
    activation quantization strategy fails to maintain the capacity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9bba072c597c9c3e5bd136aa1bfdbbe5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 2\. Only quantizing model weights to 8-bit while keeping activation at
    full precision (`W8A32`) achieves much better results when activations are quantized
    to 8-bit irrespective of whether weights are in lower precision (`W8A8` and `W32A8`).
    (Image source: [Bondarenko et al. 2021](https://arxiv.org/abs/2109.12948))'
  prefs: []
  type: TYPE_NORMAL
- en: '[Bondarenko et al. (2021)](https://arxiv.org/abs/2109.12948) observed in a
    small BERT model that FFN’s input and output have very different dynamic ranges
    due to strong outliers in the output tensor. Therefore per-tensor quantization
    for the FFN’s residual sum is likely to cause a notable error.'
  prefs: []
  type: TYPE_NORMAL
- en: As the model size continues to grow to billions of parameters, outlier features
    of high magnitude start to emerge in *all* transformer layers, causing failure
    of simple low-bit quantization. [Dettmers et al. (2022)](https://arxiv.org/abs/2208.07339)
    observed such a phenomenon for [OPT](https://arxiv.org/abs/2205.01068) models
    larger than 6.7B parameters. Larger models have more layers with extreme outliers
    and these outlier features have a significant impact on the model performance.
    The scale of activation outliers in a few dimensions can be ~100× larger than
    most of the other values.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/138eae43c673ec786ed7f843aa719c0c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 3\. The mean zero-shot accuracy over a set of language tasks (WinoGrande,
    HellaSwag, PIQA, LAMBADA) of OPT models of increasing sizes. (Image source: [Dettmers
    et al. 2022](https://arxiv.org/abs/2208.07339))'
  prefs: []
  type: TYPE_NORMAL
- en: Post-training quantization (PTQ)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mixed-precision quantization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most straightforward approach for resolving the above quantization challenge
    is to implement quantization at different precision for weights vs activation.
  prefs: []
  type: TYPE_NORMAL
- en: GOBO ([Zadeh et al. 2020](https://arxiv.org/abs/2005.03842)) is one of the first
    models to apply post-training quantization on transformers (i.e. a small BERT
    model). It assumes that model weights of each layer follow a Gaussian distribution
    and therefore detects outliers by tracking mean and standard deviation per layer.
    Outlier features remain in original form, while other values are split into multiple
    bins and only corresponding bin indices of weights and the centroid values are
    stored.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the observation that only certain activation layers (e.g. residual
    connections after FFN) in BERT cause big performance drop, [Bondarenko et al.
    (2021)](https://arxiv.org/abs/2109.12948) adopted mixed-precision quantization
    by using 16-bit quantization on problematic activations but 8-bit on others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mixed-precision quantization in `LLM.int8()` ([Dettmers et al. 2022](https://arxiv.org/abs/2208.07339))
    is implemented via two mixed-precision decompositions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Because matrix multiplication contains a set of independent inner products
    between row and column vectors, we can impose independent quantization per inner
    product: Each row and column are scaled by the absolution maximum values and then
    quantized to INT8.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Outlier activation features (e.g. 20x larger than other dimensions) remain in
    FP16 but they represent only a tiny fraction of total weights. How to identify
    outliers is empirical.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/33f8a976159592458e948c450c4ac8a3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 4\. Two mixed-precision decompositions of `LLM.int8()`. (Image source:
    [Dettmers et al. 2022](https://arxiv.org/abs/2208.07339))'
  prefs: []
  type: TYPE_NORMAL
- en: Quantization at fine-grained granularity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/bd7f9ebc1d3a525adeb0320120da3aad.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 5\. Comparison of quantization at different granularity. $d$ is the model
    size / hidden state dimension and $h$ is the number of heads in one MHSA (multi-head
    self-attention) component.
  prefs: []
  type: TYPE_NORMAL
- en: Naively quantizing the entire weight matrix in one layer (“per-tensor” or “per-layer”
    quantization) is easiest to implement but does not lead to good granularity of
    quantization.
  prefs: []
  type: TYPE_NORMAL
- en: '**Q-BERT** ([Shen, Dong & Ye, et al. 2020](https://arxiv.org/abs/1909.05840))
    applied *group-wise quantization* to a fine-tuned BERT model, treating an individual
    matrix $W$ with respect to *each head* in MHSA (multi-head self-attention) as
    one group and then applies Hessian based mixed precision quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Per-embedding group (PEG)* activation quantization was motivated by the observation
    that outlier values only appear in a few out of $d$ (hidden state / model size)
    dimensions ([Bondarenko et al. 2021](https://arxiv.org/abs/2109.12948)). Per-embedding
    is pretty computationally expensive. In comparison, PEG quantization splits the
    activation tensor into several evenly sized groups along the embedding dimension
    where elements in the same group share quantization parameters. To ensure all
    outliers are grouped together, they apply a deterministic range-based permutation
    of embedding dimensions, where dimensions are sorted by their value ranges.'
  prefs: []
  type: TYPE_NORMAL
- en: '**ZeroQuant** ([Yao et al. 2022](https://arxiv.org/abs/2206.01861)) uses *group-wise
    quantization* for weights, same as in Q-BERT, and *token-wise quantization* for
    activation. To avoid expensive quantization and de-quantization computation, ZeroQuant
    built customized *kernel* to *fuse* quantization operation with its previous operator.'
  prefs: []
  type: TYPE_NORMAL
- en: Second order information for quantization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Q-BERT ([Shen, Dong & Ye, et al. 2020](https://arxiv.org/abs/1909.05840)) developed
    Hessian AWare Quantization (HAWQ) for its mixed-precision quantization. The motivation
    is that parameters with higher Hessian spectrum (i.e., larger top eigenvalues)
    are more sensitive to quantization and thus require higher precision. It is essentially
    a way to identify outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In another viewpoint, the problem of quantization is an optimization problem.
    Given a weight matrix $\mathbf{W}$ and an input matrix $\mathbf{X}$ , we want
    to find a quantized weight matrix $\hat{\mathbf{W}}$ to minimize the MSE:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \hat{\mathbf{W}}^* = {\arg\min}_{\hat{\mathbf{W}}} | \mathbf{W}\mathbf{X}
    - \hat{\mathbf{W}}\mathbf{X}| $$
  prefs: []
  type: TYPE_NORMAL
- en: '**GPTQ** ([Frantar et al. 2022](https://arxiv.org/abs/2210.17323)) treats the
    weight matrix $\mathbf{W}$ as a collection of row vectors ${\mathbf{w}}$ and applies
    quantization to each row independently. GPTQ iteratively quantizes more weights
    that are selected greedily to minimize the quantization error. The update on selected
    weights has a closed-form formula, utilizing Hessian matrices. Read more details
    in the paper and the OBQ (Optimal Brain Quantization; [Frantar & Alistarh 2022](https://arxiv.org/abs/2208.11580))
    method if interested. GPTQ can reduce the bitwidth of weights in OPT-175B down
    to 3 or 4 bits without much performance loss, but it only applies to model weights
    not activation.'
  prefs: []
  type: TYPE_NORMAL
- en: Outlier smoothing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is known that activations are harder to quantize than weights in transformer
    models. **SmoothQuant** ([Xiao & Lin 2022](https://arxiv.org/abs/2211.10438))
    proposed a smart solution to smooth outlier features from activations to weights
    via mathematically equivalent transformation and then enable quantization on both
    weights and activations (`W8A8`). Because of this, SmoothQuant has better hardware
    efficiency than mixed-precision quantization.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c95294b17a1f34dd697fe1302cfa0f46.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 6\. SmoothQuant migrates the scale variance from activations to weights
    offline to reduce the difficulty of activation quantization. Both the resulting
    new weight and activation matrices are easy to quantize. (Image source: [Xiao
    & Lin 2022](https://arxiv.org/abs/2211.10438))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering a per-channel smooth factor $\mathbf{s}$, SmoothQuant scales the
    weights according to:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \mathbf{Y} = (\mathbf{X} \text{diag}(\mathbf{s})^{-1}) \cdot (\text{diag}(\mathbf{s})\mathbf{W})
    = \hat{\mathbf{X}}\hat{\mathbf{W}} $$
  prefs: []
  type: TYPE_NORMAL
- en: 'The smoothing factor can be easily fused into previous layers’ parameters offline.
    A hyperparameter $\alpha$ controls how much we migrate the quantization difficulty
    from activations to weights: $\mathbf{s} = \max (\vert \mathbf{X}_j \vert)^\alpha
    / \max( \vert \mathbf{W}_j \vert )^{1-\alpha}$. The paper found that $\alpha=0.5$
    is a sweet spot for many LLMs in the experiments. For models with more significant
    outliers in activation, $\alpha$ can be adjusted to be larger.'
  prefs: []
  type: TYPE_NORMAL
- en: Quantization-aware training (QAT)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Quantization-aware training fuses the quantization operation into the pre-training
    or fine-tuning process. It learns model weights in low-bit representation directly
    and leads to better performance at the cost of additional training time and computation.
  prefs: []
  type: TYPE_NORMAL
- en: The most straightforward approach is to **fine-tune** the model after quantization
    on a training dataset that is the same as or representative of the pre-training
    dataset. The training objective can be the same as the one for pre-training (e.g.
    NLL/MLM in general language model training) or specific to a downstream task that
    we care about (e.g. Cross entropy for classification).
  prefs: []
  type: TYPE_NORMAL
- en: Another approach is to consider the full-precision model as the teacher and
    the lower-precision model as the student, and then optimize the low-precision
    model with **distillation** loss. Distillation usually doesn’t need to use the
    original dataset; E.g. Wikipedia dataset is a good choice and even random tokens
    can give decent performance gain. The *Layer-by-layer Knowledge Distillation*
    (*LKD*; [Yao et al. 2022](https://arxiv.org/abs/2206.01861)) method quantizes
    the network layer by layer and uses its original, unquantized version as the teacher
    model. Given the same inputs, LKD minimizes the MSE between the multiplication
    with layer weights and the multiplication of quantized layer weights.
  prefs: []
  type: TYPE_NORMAL
- en: Pruning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Network pruning is to reduce the model size by trimming unimportant model weights
    or connections while the model capacity remains. It may or may not require re-training.
    Pruning can be **unstructured** or **structured**.
  prefs: []
  type: TYPE_NORMAL
- en: '*Unstructured pruning* is allowed to drop any weight or connection, so it does
    not retain the original network architecture. Unstructured pruning often does
    not work well with modern hardware and doesn’t lead to actual inference speedup.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Structured pruning* aims to maintain the dense matrix multiplication form
    where some elements are zeros. They may need to follow certain pattern restrictions
    to work with what hardware kernel supports. Here we focus on structured pruning
    to achieve *high sparsity* in transformer models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A routine workflow to construct a pruned network has three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Train a dense network until convergence;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prune the network to remove unwanted structure;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Optionally retrain the network to recover the performance with new weights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The idea of discovering a sparse structure within a dense model via network
    pruning while the sparse network can still maintain similar performance is motivated
    by [**Lottery Ticket Hypothesis**](https://lilianweng.github.io/posts/2019-03-14-overfit/#the-lottery-ticket-hypothesis)
    (**LTH**): A randomly initialized, dense, feed-forward network contains a pool
    of subnetworks and among them only a subset (a sparse network) are *“winning tickets”*
    which can achieve the optimal performance when trained in isolation.'
  prefs: []
  type: TYPE_NORMAL
- en: How to prune?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Magnitude pruning** is simplest yet quite effective pruning method - weights
    with smallest absolute values are trimmed. In fact, some studies ([Gale et al.
    2019](https://arxiv.org/abs/1902.09574)) found that *simple magnitude pruning
    approaches can achieve comparable or better results than complicated pruning methods*,
    such as variational dropout ([Molchanov et al. 2017](https://arxiv.org/abs/1701.05369))
    and $l_0$ regularization ([Louizos et al. 2017](https://arxiv.org/abs/1712.01312)).
    Magnitude pruning is simple to apply to large models and achieves reasonably consistent
    performance across a wide range of hyperparameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Zhu & Gupta (2017)](https://arxiv.org/abs/1710.01878) found that *large sparse
    models were able to achieve better performance than their small but dense counterparts*.
    They proposed **Gradual Magnitude Pruning (GMP)** algorithm that increases the
    sparsity of a network gradually over the course of training. At each training
    step, weights with smallest absolute values are masked to be zeros to achieve
    a desired sparsity level $s$ and masked weights do not get gradient update during
    back-propagation. The desired sparsity level $s$ goes up with more training steps.
    The process of GMP is sensitive to the learning rate schedule, which should be
    higher than what’s used in dense network training, but not too high to prevent
    convergence.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Iterative pruning** ([Renda et al. 2020](https://arxiv.org/abs/2003.02389))
    iterates step 2 (prune) & step 3 (retrain) multiple times: Only a small fraction
    of weights are pruned and the model is retrained in each iteration. The process
    repeats until a desired sparsity level is reached.'
  prefs: []
  type: TYPE_NORMAL
- en: How to retrain?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The retraining step can be simple fine-tuning using the same pre-training data
    or other task-specific datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '[Lottery Ticket Hypothesis](https://lilianweng.github.io/posts/2019-03-14-overfit/#the-lottery-ticket-hypothesis)
    proposed a **weight rewinding** retraining technique: After pruning, the unpruned
    weights are *reinitialized back to original values* earlier in the training and
    then retrain with the same learning rate schedule.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Learning rate rewinding** ([Renda et al. 2020](https://arxiv.org/abs/2003.02389))
    only resets the learning rate back to its early value, while the unpruned weights
    stay unchanged since the end of the last train stage. They observed that (1) retraining
    with weight rewinding outperforms retraining with fine-tuning across networks
    and datasets and (2) learning rate rewinding matches or outperforms weight rewinding
    in all tested scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: Sparsity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sparsity is an effective way to scale up model capacity while keeping model
    inference computationally efficient. Here we consider two types of sparsity for
    transformers:'
  prefs: []
  type: TYPE_NORMAL
- en: Sparsified dense layers, including both self-attention and FFN layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sparse model architecture; i.e. via incorporating the Mixture-of-Experts (MoE)
    component.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: N:M Sparsity via Pruning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**N:M sparsity** is a structured sparsity pattern that works well with modern
    GPU hardware optimization, in which $N$ out of every $M$ consecutive elements
    are zeros. For example, the sparse tensor core of Nvidia A100 GPU has support
    for 2:4 sparsity for faster inference ([Nvidia 2020](https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9fab047f5925135271a2af85e9df0724.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 7\. A matrix of 2:4 structured sparsity and its compressed representation.
    (Image source: [Nvidia blog](https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/))'
  prefs: []
  type: TYPE_NORMAL
- en: 'To sparsify a dense neural network to follow a N:M structured sparsity pattern,
    [Nvidia (2020)](https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf)
    suggested using the three-step [routine workflow](#routine-workflow) for training
    a pruned network: train –> prune to satisfy 2:4 sparsity –> retrain.'
  prefs: []
  type: TYPE_NORMAL
- en: Permuting columns can provide more options in the pruning process to maintain
    parameters of large magnitude or to satisfy a special restriction like N:M sparsity
    ([Pool & Yu 2021](https://proceedings.neurips.cc/paper/2021/hash/6e8404c3b93a9527c8db241a1846599a-Abstract.html)).
    As long as paired axes of two matrices are permuted in the same order, the results
    of matrix multiplication would not change. For example,
  prefs: []
  type: TYPE_NORMAL
- en: (1) Within the self-attention module, if the same permutation order is applied
    on the axis 1 of query embedding matrix $\mathbf{Q}$ and the axis 0 of key embedding
    matrix $\mathbf{K}^\top$, the final result of matrix multiplication of $\mathbf{Q}\mathbf{K}^\top$
    would stay the same.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c40cc3244a551d240931b9d592ac318.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 8\. Illustration of same permutation on $\mathbf{Q}$ (axis 1) and $\mathbf{K}^\top$
    (axis 0) to keep the results of a self-attention module unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: (2) Within the FFN layer that contains two MLP layers and one ReLU non-linear
    layer, we can permute the first linear weight matrix $\mathbf{W}_1$ along the
    axis 1 and the second linear weight matrix $\mathbf{W}_2$ along the axis 0 in
    the same order.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7f552044e81f2e6288c15b1f1354b47d.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 9\. Illustration of the same permutation on $\mathbf{W}_1$ (axis 1) and
    $\mathbf{W}_2$ (axis 0) to keep the FFN layer's output unchanged. For simplicity,
    the bias terms are skipped but the same permutation should be applied on them
    too.
  prefs: []
  type: TYPE_NORMAL
- en: To enforce N:M structured sparsity, let’s split the columns of one matrix into
    multiple slides of $M$ columns (named “stripe”) and we can easily observe that
    both the order of columns within each stripe and the order of stripes have no
    effect on the N:M sparsity restriction.
  prefs: []
  type: TYPE_NORMAL
- en: '[Pool & Yu (2021)](https://proceedings.neurips.cc/paper/2021/hash/6e8404c3b93a9527c8db241a1846599a-Abstract.html)
    proposed an iterative greedy algorithm to find optimal permutation that maximizes
    the weight magnitude for N:M sparsity. All pairs of channels are speculatively
    swapped and only the swap that leads to the greatest increase in magnitude is
    adopted, generating a new permutation and concluding a single iteration. Greedy
    algorithm may only find local minima, so they introduced two techniques to escape
    local minima:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bounded regressions: In practice two random channels are swapped, up to a fixed
    number of times. The solution search is limited to a depth of only one channel
    swap to keep the search space broad and shallow.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Narrow, deep search: Choose multiple stripes and optimize them at the same
    time.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/6201035d10a8d3208d2da4d89d9a03c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 10\. Algorithm of finding the best permutation for N:M sparsity greedily
    and iteratively. (Image source: [Pool & Yu 2021](https://proceedings.neurips.cc/paper/2021/hash/6e8404c3b93a9527c8db241a1846599a-Abstract.html))'
  prefs: []
  type: TYPE_NORMAL
- en: The network can achieve better performance if it was permuted before pruning,
    compared to pruning the network in its default channel order.
  prefs: []
  type: TYPE_NORMAL
- en: To train a model with N:M sparsity from scratch, [Zhou & Ma, et al. (2021)](https://arxiv.org/abs/2102.04010)
    extended STE (Straight-Through Estimator; [Bengio et al. 2013](https://arxiv.org/abs/1308.3432)),
    which is commonly used for back-propagation update in model quantization, to work
    for magnitude pruning and sparse parameter update.
  prefs: []
  type: TYPE_NORMAL
- en: 'STE computes the gradients of dense parameters wrt the pruned network $\widetilde{W}$,
    $\partial \mathcal{L}/\partial \widetilde{W}$, and applies that to the dense network
    $W$ as an approximation:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ W_{t+1} \gets W_t - \gamma \frac{\partial\mathcal{L}}{\partial\widetilde{W}}
    $$
  prefs: []
  type: TYPE_NORMAL
- en: 'The extended version, **SR-STE** (Sparse-refined STE), updates the dense weights
    $W$ by:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ W_{t+1} \gets W_t - \gamma \frac{\partial\mathcal{L}}{\partial\widetilde{W}}
    + \lambda_W (\bar{\mathcal{E}} \odot W_t) $$ where $\bar{\mathcal{E}}$ is the
    mask matrix for $\widetilde{W}$ and $\odot$ is element-wise multiplication. SR-STE
    is proposed to prevent large change in the binary mask by (1) restricting the
    values of weights pruned in $\widetilde{W}_t$, and (2) promoting the non-pruned
    weights in $\widetilde{W}_t$.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c48d040c45d3faf10e894b208fe7a9ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 11\. Comparison of STE and SR-STE. $\odot$ is element-wise product; $\otimes$
    is matrix multiplication. (Image source: [Zhou & Ma, et al. 2021](https://arxiv.org/abs/2102.04010))'
  prefs: []
  type: TYPE_NORMAL
- en: Different from STE or SR-STE, the **Top-KAST** ([Jayakumar et al. 2021](https://arxiv.org/abs/2106.03517))
    method can preserve constant sparsity throughout training in both the forward
    and backward-passes but does not require forward passes with dense parameters
    or dense gradients.
  prefs: []
  type: TYPE_NORMAL
- en: 'At one training step $t$, Top-KAST processes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Sparse forward pass*: Select a subset of parameters $A^t \subset \Theta$,
    containing top-$K$ parameters by magnitude by each layer, restricted to top $D$-proportion
    of weights. The parameterization $\alpha^t$ at time $t$ has parameters zeroed
    out if it is not in $A^t$ (active weights).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: $$ \alpha^t_i = \begin{cases} \theta^t_i & \text{ if } i \in A^t = \{i \mid
    \theta^t_i \in \text{TopK}(\theta^t, D) \}\\ 0 & \text{ otherwise} \end{cases}
    $$
  prefs: []
  type: TYPE_NORMAL
- en: where $\text{TopK}(\theta, x)$ selected top $x$ proportion of weights from $\theta$
    based on magnitude.
  prefs: []
  type: TYPE_NORMAL
- en: '*Sparse backward pass*: Then apply gradients to a larger parameter subset $B
    \subset \Theta$ where $B$ contains $(D+M)$-proportion of weights and $A \subset
    B$. Updating a larger proportion of weights enables more effective exploration
    of different pruning masks, making it more likely to cause permutations in the
    top $D$-proportion active weights.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: $$ \Delta_{\theta^t_i} = \begin{cases} -\eta \nabla_{\alpha_t} \mathcal{L}(y,
    x, \alpha^t)_i & \text{ if } i\in B^t = \{i \mid \theta^t_i \in \text{TopK}(\theta^t,
    D+M) \} \\ 0 & \text{ otherwise } \end{cases} $$
  prefs: []
  type: TYPE_NORMAL
- en: Training is split into two stages and the additional coordinates in the set
    $B \setminus A$ controls how much exploration is brought in. The amount of exploration
    is expected to diminish gradually through the training process and the mask eventually
    stabilizes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/25fe93ad5b6a36fa095319a6119ac598.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 12\. The pruning mask of Top-KAST stabilizes in time. (Image source: [Jayakumar
    et al. 2021](https://arxiv.org/abs/2106.03517))'
  prefs: []
  type: TYPE_NORMAL
- en: To prevent rich-get-richer phenomenon, Top-KAST penalizes the magnitude of active
    weights via a L2 regularization loss to encourage more exploration of new items.
    Parameters in $B \setminus A$ are penalized more than $A$ for a higher selection
    bar during updates to stabilize the mask.
  prefs: []
  type: TYPE_NORMAL
- en: $$ L_\text{penalty}(\alpha^t_i) = \begin{cases} \vert \theta^t_i\vert & \text{
    if } i \in A^t \\ \vert \theta^t_i\vert / D & \text{ if } i \in B^t \setminus
    A^t \\ 0 & \text{ otherwise} \end{cases} $$
  prefs: []
  type: TYPE_NORMAL
- en: Sparsified Transformer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Scaling Transformer* ([Jaszczur et al. 2021](https://arxiv.org/abs/2111.12763))
    sparsifies both self-attention and FFN layers in transformer architecture, achieving
    37x speedup for single-example inference.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fdcae05c8e0aa6ac8ce88209b13e696d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 13\. The speed of decoding a single token (unbatched inference) by a transformer
    model when sparsification is applied on different layers. (Image source: [Jaszczur
    et al. 2021](https://arxiv.org/abs/2111.12763))'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sparse FFN layer**: Each FFN layer contains 2 MLP and one ReLU in-between.
    Because ReLU will introduce a lot of zeros, they implement a fixed structure on
    activations to enforce only 1 non-zero value in one block of $N$ elements. The
    sparsity pattern is dynamic, different for each token.'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} Y_\text{sparse} &= \max(0, xW_1 + b_1) \odot \text{Controller}(x)
    \\ \text{SparseFFN}(x) &= Y_\text{sparse} W_2 + b_2 \\ \text{Controller}(x) &=
    \arg\max(\text{Reshape}(x C_1 C_2, (-1, N))) \end{aligned} $$
  prefs: []
  type: TYPE_NORMAL
- en: where each activation in $Y_\text{sparse}$ corresponds to one column in $W_1$
    and one row in $W_2$. The controller is implemented as a low-rank bottleneck dense
    layer, $C_1 \in \mathbb{R}^{d_\text{model} \times d_\text{lowrank}}, C_2 \in \mathbb{R}^{d_\text{lowrank}
    \times d_\text{ff}}$ and $d_\text{lowrank} = d_\text{model} / N$. It uses $\arg\max$
    for inference to select which columns should be non-zero and Gumbel-softmax trick
    ([Jang et al. 2016](https://arxiv.org/abs/1611.01144)) during training. Because
    we can compute $\text{Controller}(x)$ before loading FFN weight matrices, we know
    which columns will be zeroed out and thus choose *not to load* them into memory
    for inference speedup.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5f949562872fd0982691b85d0c21d37c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 14\. (a) Sparse FFN layer; columns in red are not loaded in memory for
    faster inference. (b) Sparse FFN controller for 1:4 sparsity. (Image source: [Jaszczur
    et al. 2021](https://arxiv.org/abs/2111.12763)) *Lilian''s side note*: Fig (a)
    in the illustration from the paper is actually $Y_\text{sparse} = \max\big(0,
    (xW_1 + b_1) \odot \text{Controller}(x)\big)$, but it doesn''t change the results.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sparse QKV (attention) layer**: In the attention layer, the dimensionality
    $d_\text{model}$ is divided into $S$ modules, each of size $M=d_\text{model} /S$.
    To make sure each subdivision can access any part of the embedding, Scaling Transformer
    introduces a multiplicative layer (i.e., a multiplication layer multiplies inputs
    from multiple neural network layers element-wise) which can represent arbitrary
    permutation but contains fewer parameters than a dense layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given an input vector $x \in \mathbb{R}^{d_\text{model}}$, the multiplicative
    layer outputs $y \in \mathbb{R}^{S \times M}$:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ y_{s,m} = \sum_i x_i D_{i,s} E_{i,m} \quad\text{where }D \in \mathbb{R}^{d_\text{model}
    \times S}, D \in \mathbb{R}^{d_\text{model} \times M} $$
  prefs: []
  type: TYPE_NORMAL
- en: The output of the multiplicative layer is a tensor of size $\in \mathbb{R}^{\text{batch
    size}\times \text{length} \times S \times M}$. It then gets processed by a two-dimensional
    convolutional layer, where $\text{length}$ and $S$ are treated as the height and
    width of an image. Such a convolution layer further reduces the parameter count
    and computation time of attention layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/546997f555fcb662f035d86760caf224.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 15\. (a) A multiplicative layer is introduced to enable partitions to
    access any part of an embedding. (b) Combination of multiplicative dense layer
    and 2-D convolutional layer reduces the number of parameters and computation time
    of the attention layer. (Image source: [Jaszczur et al. 2021](https://arxiv.org/abs/2111.12763))'
  prefs: []
  type: TYPE_NORMAL
- en: To better work with long sequences, Scaling Transformer is further equipped
    with LSH (locality-sensitive hashing) attention from [Reformer](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/#locality-sensitive-hashing-reformer)
    ([Kitaev, et al. 2020](https://arxiv.org/abs/2001.04451)) and FFN block recurrence,
    resulting in *Terraformer*.
  prefs: []
  type: TYPE_NORMAL
- en: Mixture-of-Experts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mixture-of-experts (MoE) models depend on a collection of “expert” networks
    and each example only activates a subset of networks to get predictions. The idea
    originated back to the 1990s ([Jacobs et al. 1991](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf))
    and is strongly related to ensemble methods. For details on how to incorporate
    MoE module into transformer, please check my [previous post on large model training
    techniques](https://lilianweng.github.io/posts/2021-09-25-train-large/) and a
    survey paper on MoE by [Fedus et al. 2022](https://arxiv.org/abs/2209.01667).
  prefs: []
  type: TYPE_NORMAL
- en: 'With MoE architecture, only partial parameters are utilized at decoding time
    and therefore it saves inference cost. The capacity of each expert can be adjusted
    with a hyperparameter, capacity factor $C$, and the expert capacity is defined
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '$$ \text{Expert capacity} = \text{round}(C \cdot k \cdot \frac{\text{total
    # tokens in one batch}}{\text{# experts}}) $$'
  prefs: []
  type: TYPE_NORMAL
- en: where top-$k$ experts are selected per token. Larger $C$ leads to higher expert
    capacity and improved performance but more expensive computationally. When $C>1$,
    a slack capacity is added; otherwise, when $C<1$, the routing network needs to
    ignore some tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Routing Strategy Improvement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MoE layer has a routing network to assign a subset of experts for each input
    token. The routing strategy in vanilla MoE models is to route each token toward
    preferred experts differently as they come up in the natural order. If a token
    is routed to experts that have reached their capacity, the token would be marked
    *“overflowed” and skipped*.
  prefs: []
  type: TYPE_NORMAL
- en: '**V-MoE** (Vision MoE; [Riquelme et al. 2021](https://arxiv.org/abs/2106.05974))
    adds MoE layers into ViT (Vision Transformer). It matches the performance of previous
    SoTA but only requires *half* of inference compute. V-MoE can be scaled up to
    15B parameters. Their experiments used $k=2$, 32 experts and every-2 expert placement
    (meaning that MoEs are placed in every other layer).'
  prefs: []
  type: TYPE_NORMAL
- en: Since each expert has a limited capacity, some important and informative tokens
    may have to be discarded if they come up too late in the predefined sequence order
    (e.g. the order of words in a sentence, or the order of image patches). To avoid
    such a drawback in the vanilla routing scheme, V-MoE adopts **BPR (Batch Priority
    Routing)** to assign experts to tokens with a high priority score first. BPR computes
    a priority score (max or sum of top-$k$ router scores) per token before expert
    assignment and alters the order of tokens accordingly. This guarantees that the
    expert capacity buffer would be fulfilled with key tokens first.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c063b58b528994fd5ab986e8a0fed8b2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 16\. How image patches are discarded according to priority scores when
    $C < 1$. (Image source: [Riquelme et al. 2021](https://arxiv.org/abs/2106.05974))'
  prefs: []
  type: TYPE_NORMAL
- en: BPR works much better than vanilla routing when $C\leq 0.5$, where the model
    starts dropping a significant amount of tokens. It capacitates the model to be
    competitive with the dense network even at quite low capacities.
  prefs: []
  type: TYPE_NORMAL
- en: When looking into how to interpret image class-expert association, they observed
    that early MoE layers are more general, while later MoE layers could be specialized
    for a few image classes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Task MoE** (Task-level Mixture-of-Experts; [Kudugunta et al. 2021](https://arxiv.org/abs/2110.03742)
    ) takes the task information into consideration and routes tokens at the *task*
    level instead of the word or token level for machine translation. They used MNMT
    (multilingual neural machine translation) as an example and group translation
    tasks based on the target language or language pairs.'
  prefs: []
  type: TYPE_NORMAL
- en: Token level routing is dynamic and the routing decision for each token is made
    disjointly. Hence, at inference time, the server needs to preload all the experts.
    In comparison, task level routing is *static* given a fixed task, so the inference
    server for one task only needs to preload $k$ experts (assuming top-$k$ routing).
    According to their experiments, Task MoE can achieve similar performance gain
    as token MoE compared to dense model baseline with 2.6x higher peak throughput
    and 1.6% of the decoder size.
  prefs: []
  type: TYPE_NORMAL
- en: Task level MoE is essentially to categorize a distribution of tasks according
    to predefined *heuristics* and incorporate such human knowledge into the router.
    When such heuristics do not exist (e.g. consider a general sentence continuation
    task), it would not be straightforward how to utilize Task MoE.
  prefs: []
  type: TYPE_NORMAL
- en: '**PR-MoE** (Pyramid residual MoE; [Rajbhandari et al. 2022](https://arxiv.org/abs/2201.05596))
    has each token pass one fixed MLP and one chosen expert. Due to the observation
    that MoE at later layers is more beneficial, PR-MoE adopts more exports at later
    layers. DeepSpeed library implements a flexible multi-expert, multi-data parallelism
    to enable training PR-MoE with different numbers of experts across layers.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/103ff0dc26f6bcc7f8ed0067b9b49f4f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 17\. Illustration of PR-MoE architecture in comparison with a standard
    MoE. (Image source: [Rajbhandari et al. 2022](https://arxiv.org/abs/2201.05596))'
  prefs: []
  type: TYPE_NORMAL
- en: Kernel Improvement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Expert networks can be hosted on different devices. However, when the number
    of GPUs increases, the number of experts per GPU decreases and the communication
    between experts (“All-to-all”) grows to be more expensive. All-to-all communication
    between experts across a number of GPUs relies on P2P APIs of NCCL, which cannot
    saturate the bandwidth of high-speed links (e.g. NVLink, HDR InfiniBand) at a
    large scale, as individual chunk gets smaller with more nodes used. The existing
    all-to-all algorithm performs poorly at large scale with a small workload. There
    are a variety of kernel improvements to enable more efficient MoE computation,
    such as making all-to-all communication cheaper/faster.
  prefs: []
  type: TYPE_NORMAL
- en: Both the *DeepSpeed* library ([Rajbhandari et al. 2022](https://arxiv.org/abs/2201.05596))
    and TUTEL ([Hwang et al. 2022](https://arxiv.org/abs/2206.03382)) implemented
    a tree-based **hierarchical all-to-all** algorithm, which runs an intra-node all-to-all
    followed by an inter-node all-to-all. It reduces the communication hops from $O(G)$
    to $O(G_\text{node} + G / G_\text{node})$, where $G$ is the total number of GPU
    nodes and $G_\text{node}$ is the number of GPU cores per node. Although the communication
    volume is doubled in such implementation, it enables better scaling with small
    batches at large scale as the bottleneck is on latency instead of communication
    bandwidth when the batch size is small.
  prefs: []
  type: TYPE_NORMAL
- en: '*DynaMoE* ([Kossmann et al. 2022](https://arxiv.org/abs/2205.01848)) uses **dynamic
    recompilation** to adapt the computational resources to dynamic workloads among
    experts. The `RECOMPILE` mechanism compiles the computation graph from scratch
    and only reallocates resources when needed. It measures how many samples are assigned
    to each expert and adjusts their capacity factors $C$ dynamically, in order to
    reduce the memory and computation requirements at run time. Based on the observation
    that sample-expert assignments converge early in training, *sample assignment
    caching* is introduced after convergence and then `RECOMPILE` is used to eliminate
    the dependency between the gating network and experts.'
  prefs: []
  type: TYPE_NORMAL
- en: Architectural Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The survey paper on *Efficient Transformers* ([Tay et al. 2020](https://arxiv.org/abs/2009.06732))
    reviewed a collection of new transformer architectures with improvement for better
    *computational and memory efficiency*. Strongly recommend a read. You can also
    check out my post [“The Transformer Family Version 2.0”](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/)
    for introduction to a diverse set of transformer archiecture improvements in depth,
    including changes to make the model cheaper to run.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/064852b2a370a12b8495168c9ddc6f3d.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 18\. Categorization of efficient transformer models.
  prefs: []
  type: TYPE_NORMAL
- en: '(Image source: [Tay et al. 2020](https://arxiv.org/abs/2009.06732))'
  prefs: []
  type: TYPE_NORMAL
- en: Since the self-attention mechanism has quadratic time and memory complexity
    and that is the main bottleneck for better transformer decoding efficiency, all
    the efficient transformer models have applied some form of sparsity to the otherwise
    dense attention layer. Here only lists a high-level overview, several derived
    from [Tay et al. 2020](https://arxiv.org/abs/2009.06732).
  prefs: []
  type: TYPE_NORMAL
- en: Sparse Attention Patterns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Fixed Patterns* limit the field of view for the attention matrix, using predefined,
    fixed patterns.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chunk input sequences into fixed blocks, such as [Blockwise Attention](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/##strided-context);
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Image Transformer](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/##fixed-local-context)
    uses local attention;'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Sparse Transformer](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/##strided-context)
    uses strided attention patterns.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Combined Patterns* learn to sort/cluster the input tokens - enabling a more
    optimal global view of the sequence while maintaining the efficiency benefits
    of fixed patterns.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Sparse Transformer](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/#sparse-attention-matrix-factorization-sparse-transformers)
    combines strided and local attention;'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Given a high dimensional input tensor, instead of applying attention to the
    flattened version of the input, [Axial Transformer](https://arxiv.org/abs/1912.12180)
    applies multiple attentions, each along a single axis of the input tensor.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ETC, Longformer and Big Bird](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/#combination-of-local-and-global-context)
    combines local and global context, as well as strided or random attention.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Learnable Patterns* identify the optimal attention pattern via learning.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Reformer](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/#content-based-attention)
    clusters tokens into clusters based on hash-based similarity (LSH);'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Routing Transformer](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/#content-based-attention)
    runs $k$-means clustering on tokens;'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Sinkhorn Sorting Network](https://arxiv.org/abs/2002.11296) learns to sort
    blocks of input sequence.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Recurrence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recurrence mechanism connects multiple blocks/segments via recurrence.
  prefs: []
  type: TYPE_NORMAL
- en: '[Transformer-XL](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/#context-memory)
    makes use of longer context by reusing hidden states between segments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Universal Transformer](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/#make-it-recurrent)
    combines self-attention with the recurrent mechanism in RNN.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Compressive Transformer](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/#context-memory)
    is an extension of Transformer-XL with additional memory, containing a set of
    memory slots for past activiations and compressive memory slots for compressed
    activations. Whenever the model accepts a new input segment, the oldest activations
    in the primary memory are moved to the compressed memory where a compression function
    is applied.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory Saving Designs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Memory saving designs refer to changes of the architecture to use less memory.
  prefs: []
  type: TYPE_NORMAL
- en: '[Linformer](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/#low-rank-attention)
    projects the length dimension of keys and values to a lower-dimensional representation
    ($N \to k$) and thus the memory complexity is reduced from $N \times N$ to $N
    \times k$.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Shazeer (2019)](https://arxiv.org/abs/1911.02150) proposed *multi-query attention*
    which has the keys and values shared across different attention “heads”, greatly
    reducing the size of these tensors and the memory cost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Random feature attention and Performer](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/#low-rank-attention)
    use [kernel methods]((https://lilianweng.github.io/posts/2022-09-08-ntk/#kernel--kernel-methods))
    to achieve a cheaper mathematical format of the self-attention mechanism.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adaptive Attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Adaptive attention* enables the model to learn the optimal attention span
    or decide on when to do early exiting for different input tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Adaptive Attention Span](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/#adaptive-attention-span)
    trains the model to learn the optimal attention span per token per head via a
    soft mask between the token and other keys.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Universal Transformer](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/#make-it-recurrent)
    incorporates recurrent mechanism and uses [ACT (Adaptive computation time)](https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/#adaptive-computation-time-act)
    to dynamically decide the number of recurrent steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Depth-Adaptive Transformer and CALM](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/#depth-adaptive-transformer)
    learns when to early exit the computation layers per token using some confidence
    measures to achieve good performance-efficiency tradeoffs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Citation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Cited as:'
  prefs: []
  type: TYPE_NORMAL
- en: Weng, Lilian. (Jan 2023). Large Transformer Model Inference Optimization. Lil’Log.
    https://lilianweng.github.io/posts/2023-01-10-inference-optimization/.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Or
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Bondarenko et al. [“Understanding and overcoming the challenges of efficient
    transformer quantization”](https://arxiv.org/abs/2109.12948) ACL 2021.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Dettmers et al. [“LLM.int8(): 8-bit Matrix Multiplication for Transformers
    at Scale”](https://arxiv.org/abs/2208.07339) NeuriPS 2022'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Zadeh et al. [“Gobo: Quantizing attention-based NLP models for low latency
    and energy efficient inference.”](https://arxiv.org/abs/2005.03842) MICRO 2020'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Shen, Dong & Ye, et al. [“Q-BERT: Hessian based ultra low precision quantization
    of BERT”](https://arxiv.org/abs/1909.05840) AAAI 2020.'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Yao et al. [“ZeroQuant: Efficient and affordable post-training quantization
    for large-scale transformers”](https://arxiv.org/abs/2206.01861) arXiv preprint
    arXiv:2206.01861 (2022).'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Frantar et al. [“GPTQ: Accurate Quantization for Generative Pre-trained
    Transformers”](https://arxiv.org/abs/2210.17323) arXiv preprint arXiv:2210.17323
    (2022).'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Xiao & Lin [“SmoothQuant: Accelerated sparse neural training: A provable
    and efficient method to find N:M transposable masks.”](https://arxiv.org/abs/2211.10438)
    arXiv preprint arXiv:2211.10438 (2022). | [code](https://github.com/mit-han-lab/smoothquant)'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] Pool & Yu. [“Channel Permutations for N:M Sparsity.”](https://proceedings.neurips.cc/paper/2021/hash/6e8404c3b93a9527c8db241a1846599a-Abstract.html)
    NeuriPS 2021\. | [code](https://github.com/NVIDIA/apex/tree/master/apex/contrib/sparsity)'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] Zhou & Ma, et al. [“Learning N:M fine-grained structured sparse neural
    networks from scratch.”](https://arxiv.org/abs/2102.04010) arXiv preprint arXiv:2102.04010
    (2021).'
  prefs: []
  type: TYPE_NORMAL
- en: '[10] Jayakumar et al. [“Top-KAST: Top-K Always Sparse Training.”](https://arxiv.org/abs/2106.03517)
    NeuriPS 2020.'
  prefs: []
  type: TYPE_NORMAL
- en: '[11] Nvidia. [“Nvidia A100 tensor core GPU architecture.”](https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf)
    2020.'
  prefs: []
  type: TYPE_NORMAL
- en: '[12] Gale, Elsen & Hooker [“The State of Sparsity in Deep Neural Networks.”](https://arxiv.org/abs/1902.09574)
    arXiv preprint arXiv:1902.09574 (2019).'
  prefs: []
  type: TYPE_NORMAL
- en: '[13] Zhu & Gupta. [“To Prune, or Not to Prune: Exploring the Efficacy of Pruning
    for Model Compression.”](https://arxiv.org/abs/1710.01878) arXiv preprint arXiv:1710.01878
    (2017).'
  prefs: []
  type: TYPE_NORMAL
- en: '[14] Renda et al. [“Comparing rewinding and fine-tuning in neural network pruning.”](https://arxiv.org/abs/2003.02389)
    arXiv preprint arXiv:2003.02389 (2020).'
  prefs: []
  type: TYPE_NORMAL
- en: '[15] Zhou & Ma, et al. [“Learning N:M fine-grained structured sparse neural
    networks from scratch.”](https://arxiv.org/abs/2102.04010) arXiv preprint arXiv:2102.04010
    (2021).'
  prefs: []
  type: TYPE_NORMAL
- en: '[16] Pool & Yu. [“Channel Permutations for N:M Sparsity.”](https://proceedings.neurips.cc/paper/2021/hash/6e8404c3b93a9527c8db241a1846599a-Abstract.html)
    NeuriPS 2021\. | [code](https://github.com/NVIDIA/apex/tree/master/apex/contrib/sparsity)'
  prefs: []
  type: TYPE_NORMAL
- en: '[17] Jaszczur et al. [“Sparse is Enough in Scaling Transformers.”](https://arxiv.org/abs/2111.12763)
    NeuriPS 2021.'
  prefs: []
  type: TYPE_NORMAL
- en: '[18] Mishra et al. [“An Survey of Neural Network Compression.”](https://arxiv.org/abs/2010.03954)
    arXiv preprint arXiv:1710.09282 (2017).'
  prefs: []
  type: TYPE_NORMAL
- en: '[19] Fedus et al. [“A Review of Sparse Expert Models in Deep Learning.”](https://arxiv.org/abs/2209.01667)
    arXiv preprint arXiv:2209.01667 (2022)..'
  prefs: []
  type: TYPE_NORMAL
- en: '[20] Riquelme et al. [“Scaling vision with sparse mixture of experts.”](https://arxiv.org/abs/2106.05974)
    NeuriPS 2021.'
  prefs: []
  type: TYPE_NORMAL
- en: '[21] Kudugunta et al. [“Beyond Distillation: Task-level Mixture-of-Experts
    for Efficient Inference.”](https://arxiv.org/abs/2110.03742) arXiv preprint arXiv:2110.03742
    (2021).'
  prefs: []
  type: TYPE_NORMAL
- en: '[22] Rajbhandari et al. [“DeepSpeed-MoE: Advancing mixture-of-experts inference
    and training to power next-generation ai scale.”](https://arxiv.org/abs/2201.05596)
    arXiv preprint arXiv:2201.05596 (2022).'
  prefs: []
  type: TYPE_NORMAL
- en: '[23] Kossmann et al. [“Optimizing mixture of experts using dynamic recompilations.”](https://arxiv.org/abs/2205.01848)
    arXiv preprint arXiv:2205.01848 (2022).'
  prefs: []
  type: TYPE_NORMAL
- en: '[24] Hwang et al. [“Tutel: Adaptive mixture-of-experts at scale.”](https://arxiv.org/abs/2206.03382)
    arXiv preprint arXiv:2206.03382 (2022). | [code](https://github.com/microsoft/tutel)'
  prefs: []
  type: TYPE_NORMAL
- en: '[25] Noam Shazeer. [“Fast Transformer Decoding: One Write-Head is All You Need.”](https://arxiv.org/abs/1911.02150)
    arXiv preprint arXiv:1911.02150 (2019).'
  prefs: []
  type: TYPE_NORMAL
- en: '[26] Tay et al. [“Efficient Transformers: A Survey.”](https://arxiv.org/abs/2009.06732)
    ACM Computing Surveys 55.6 (2022): 1-28.'
  prefs: []
  type: TYPE_NORMAL
- en: '[27] Pope et al. [“Efficiently Scaling Transformer Inference.”](https://arxiv.org/abs/2211.05102)
    arXiv preprint arXiv:2211.05102 (2022).'
  prefs: []
  type: TYPE_NORMAL
- en: '[28] Frankle & Carbin. [“The Lottery Ticket Hypothesis: Finding Sparse, Trainable
    Neural Networks”](https://arxiv.org/abs/1803.03635) ICLR 2019.'
  prefs: []
  type: TYPE_NORMAL
- en: '[29] Elabyad et al. [“Depth-Adaptive Transformer”](https://arxiv.org/abs/1910.10073)
    ICLR 2020.'
  prefs: []
  type: TYPE_NORMAL
- en: '[30] Schuster et al. [“Confident Adaptive Language Modeling”](https://arxiv.org/abs/2207.07061)
    arXiv preprint arXiv:2207.07061 (2022).'
  prefs: []
  type: TYPE_NORMAL
- en: '[31] Gou et al. [“https://arxiv.org/abs/2006.05525”](https://arxiv.org/abs/2006.05525)
    arXiv preprint arXiv:2006.05525 (2020).'
  prefs: []
  type: TYPE_NORMAL
- en: '[32] Hinton et al. [“Distilling the Knowledge in a Neural Network”](https://arxiv.org/abs/1503.02531)
    NIPS 2014.'
  prefs: []
  type: TYPE_NORMAL
- en: '[33] Sanh et al. [“DistilBERT, a distilled version of BERT: smaller, faster,
    cheaper and lighter”](https://arxiv.org/abs/1910.01108) Workshop on Energy Efficient
    Machine Learning and Cognitive Computing @ NeuriPS 2019.'
  prefs: []
  type: TYPE_NORMAL
