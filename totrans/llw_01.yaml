- en: Thinking about High-Quality Human Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ€è€ƒé«˜è´¨é‡çš„äººç±»æ•°æ®
- en: åŽŸæ–‡ï¼š[https://lilianweng.github.io/posts/2024-02-05-human-data-quality/](https://lilianweng.github.io/posts/2024-02-05-human-data-quality/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŽŸæ–‡ï¼š[https://lilianweng.github.io/posts/2024-02-05-human-data-quality/](https://lilianweng.github.io/posts/2024-02-05-human-data-quality/)
- en: '[Special thank you to [Ian Kivlichan](https://scholar.google.com/citations?user=FRBObOwAAAAJ&hl=en)
    for many useful pointers (E.g. the 100+ year old Nature paper â€œVox populiâ€) and
    nice feedback. ðŸ™ ]'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[ç‰¹åˆ«æ„Ÿè°¢[Ian Kivlichan](https://scholar.google.com/citations?user=FRBObOwAAAAJ&hl=en)æä¾›äº†è®¸å¤šæœ‰ç”¨çš„æŒ‡å¼•ï¼ˆä¾‹å¦‚100å¤šå¹´å‰çš„ã€Šè‡ªç„¶ã€‹æ‚å¿—ä¸Šçš„â€œæ°‘æ„â€è®ºæ–‡ï¼‰å’Œå®è´µçš„åé¦ˆã€‚ðŸ™]'
- en: High-quality data is the fuel for modern data deep learning model training.
    Most of the task-specific labeled data comes from human annotation, such as classification
    task or [RLHF](https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/#rl-fine-tuning-with-human-preferences)
    labeling (which can be constructed as classification format) for LLM alignment
    training. Lots of ML techniques in the post can help with data quality, but fundamentally
    human data collection involves attention to details and careful execution. The
    community knows the value of high quality data, but somehow we have this subtle
    impression that â€œEveryone wants to do the model work, not the data workâ€ ([Sambasivan
    et al. 2021](https://dl.acm.org/doi/abs/10.1145/3411764.3445518)).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: é«˜è´¨é‡æ•°æ®æ˜¯çŽ°ä»£æ•°æ®æ·±åº¦å­¦ä¹ æ¨¡åž‹è®­ç»ƒçš„ç‡ƒæ–™ã€‚å¤§å¤šæ•°ä»»åŠ¡ç‰¹å®šçš„æ ‡è®°æ•°æ®æ¥è‡ªäººç±»æ ‡æ³¨ï¼Œä¾‹å¦‚åˆ†ç±»ä»»åŠ¡æˆ–[RLHF](https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/#rl-fine-tuning-with-human-preferences)æ ‡è®°ï¼ˆå¯ä»¥æž„å»ºä¸ºåˆ†ç±»æ ¼å¼ï¼‰ç”¨äºŽLLMå¯¹é½è®­ç»ƒã€‚åŽç»­çš„è®¸å¤šæœºå™¨å­¦ä¹ æŠ€æœ¯å¯ä»¥å¸®åŠ©æé«˜æ•°æ®è´¨é‡ï¼Œä½†ä»Žæ ¹æœ¬ä¸Šè¯´ï¼Œäººç±»æ•°æ®æ”¶é›†æ¶‰åŠå¯¹ç»†èŠ‚çš„å…³æ³¨å’Œè°¨æ…Žçš„æ‰§è¡Œã€‚ç¤¾åŒºçŸ¥é“é«˜è´¨é‡æ•°æ®çš„ä»·å€¼ï¼Œä½†ä¸çŸ¥ä½•æ•…æˆ‘ä»¬æœ‰è¿™ç§å¾®å¦™çš„å°è±¡ï¼šâ€œæ¯ä¸ªäººéƒ½æƒ³åšæ¨¡åž‹å·¥ä½œï¼Œè€Œä¸æ˜¯æ•°æ®å·¥ä½œâ€ï¼ˆ[Sambasivanç­‰äººï¼Œ2021](https://dl.acm.org/doi/abs/10.1145/3411764.3445518)ï¼‰ã€‚
- en: '![](../Images/fa16acb7631052a382be08ca5db95245.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa16acb7631052a382be08ca5db95245.png)'
- en: Fig. 1\. Two directions to approach high data quality.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1. æŽ¥è¿‘é«˜æ•°æ®è´¨é‡çš„ä¸¤ä¸ªæ–¹å‘ã€‚
- en: Human Raters â†” Data Quality
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: äººç±»è¯„åˆ†å‘˜ â†” æ•°æ®è´¨é‡
- en: 'Collecting human data involve a set of operation steps and every step contributes
    to the data quality:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¶é›†äººç±»æ•°æ®æ¶‰åŠä¸€ç³»åˆ—æ“ä½œæ­¥éª¤ï¼Œæ¯ä¸€æ­¥éƒ½å¯¹æ•°æ®è´¨é‡æœ‰æ‰€è´¡çŒ®ï¼š
- en: 'Task design: Design task workflow to improve clarity and reduce complexity.
    Detailed guidelines are helpful but very long and complicated guidelines demand
    a decent amount of training to be useful.'
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»»åŠ¡è®¾è®¡ï¼šè®¾è®¡ä»»åŠ¡å·¥ä½œæµç¨‹ä»¥æé«˜æ¸…æ™°åº¦å¹¶å‡å°‘å¤æ‚æ€§ã€‚è¯¦ç»†çš„æŒ‡å—æ˜¯æœ‰å¸®åŠ©çš„ï¼Œä½†éžå¸¸é•¿å’Œå¤æ‚çš„æŒ‡å—éœ€è¦ç›¸å½“å¤šçš„åŸ¹è®­æ‰èƒ½å‘æŒ¥ä½œç”¨ã€‚
- en: 'Select and train a pool of raters: Select annotators with matched skillset
    and consistency. Training sessions are necessary. After onboarding, regular feedback
    and calibration sessions are also needed.'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€‰æ‹©å’ŒåŸ¹è®­ä¸€ç»„è¯„åˆ†å‘˜ï¼šé€‰æ‹©å…·æœ‰åŒ¹é…æŠ€èƒ½å’Œä¸€è‡´æ€§çš„æ ‡æ³¨è€…ã€‚åŸ¹è®­è¯¾ç¨‹æ˜¯å¿…è¦çš„ã€‚å…¥èŒåŽï¼Œè¿˜éœ€è¦å®šæœŸåé¦ˆå’Œæ ¡å‡†ä¼šè®®ã€‚
- en: Collect and aggregate data. This is the stage where more ML techniques can be
    applied to clean, filter and smartly aggregate data to identify the true labels.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ”¶é›†å’Œæ±‡æ€»æ•°æ®ã€‚è¿™æ˜¯æ›´å¤šæœºå™¨å­¦ä¹ æŠ€æœ¯å¯ä»¥åº”ç”¨äºŽæ¸…æ´ã€è¿‡æ»¤å’Œæ™ºèƒ½èšåˆæ•°æ®ä»¥è¯†åˆ«çœŸå®žæ ‡ç­¾çš„é˜¶æ®µã€‚
- en: '![](../Images/5b506f613c8e7a99b7f95dc9085c7caf.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b506f613c8e7a99b7f95dc9085c7caf.png)'
- en: 'Fig. 2\. Quality assurance refers to a set of actions that allow one to improve
    quality by acting on the quality attributes identified in the quality model. (Image
    source: [Daniel et al. 2018](https://arxiv.org/abs/1801.02546))'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2. è´¨é‡ä¿è¯æŒ‡çš„æ˜¯é€šè¿‡å¯¹è´¨é‡æ¨¡åž‹ä¸­ç¡®å®šçš„è´¨é‡å±žæ€§é‡‡å–è¡ŒåŠ¨æ¥æé«˜è´¨é‡çš„ä¸€ç³»åˆ—è¡ŒåŠ¨ã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼š[Danielç­‰äººï¼Œ2018](https://arxiv.org/abs/1801.02546))
- en: The Wisdom of the Crowd
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¼—äººçš„æ™ºæ…§
- en: '[Vox populi](https://en.wikipedia.org/wiki/Vox_populi) (originally â€œVox populi,
    vox Deiâ€), a Latin phrase, means the voice of people. A short paper named was
    the same name was published in 1907 on Nature. It tracked an event at an annual
    exhibition where a fat ox was selected and people would guess the weight of the
    ox in order to win a prize if the guess is close to the real number. The middlemost
    estimate was treated as â€œthe vox populiâ€ and ended up being very close to the
    true value. The author concluded *â€œThis result is, I think, more creditable to
    the trustworthiness of a democratic judgment than might have been expected.â€*
    This is probably the earliest mention of how crowdsourcing (â€œthe wisdom of the
    crowdâ€) would work out.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[æ°‘æ„](https://en.wikipedia.org/wiki/Vox_populi)ï¼ˆæœ€åˆæ˜¯â€œVox populi, vox Deiâ€ï¼‰ï¼Œä¸€å¥æ‹‰ä¸çŸ­è¯­ï¼Œæ„ä¸ºäººæ°‘çš„å£°éŸ³ã€‚1907å¹´åœ¨ã€Šè‡ªç„¶ã€‹æ‚å¿—ä¸Šå‘è¡¨äº†ä¸€ç¯‡åŒåçŸ­æ–‡ã€‚å®ƒè¿½è¸ªäº†ä¸€æ¬¡å¹´åº¦å±•è§ˆä¸­çš„ä¸€ä¸ªäº‹ä»¶ï¼Œäººä»¬ä¼šçŒœæµ‹ä¸€å¤´è‚¥ç‰›çš„é‡é‡ï¼Œä»¥èµ¢å¾—å¥–å“ï¼Œå¦‚æžœçŒœæµ‹æŽ¥è¿‘çœŸå®žæ•°å­—ã€‚ä¸­é—´ä¼°è®¡è¢«è§†ä¸ºâ€œæ°‘æ„â€ï¼Œæœ€ç»ˆéžå¸¸æŽ¥è¿‘çœŸå®žå€¼ã€‚ä½œè€…æ€»ç»“é“ï¼šâ€œæˆ‘è®¤ä¸ºï¼Œè¿™ä¸ªç»“æžœæ›´å€¼å¾—ä¿¡èµ–çš„æ˜¯æ°‘ä¸»åˆ¤æ–­çš„å¯é æ€§ï¼Œè¿™å¯èƒ½è¶…å‡ºäº†é¢„æœŸã€‚â€è¿™å¯èƒ½æ˜¯å…³äºŽä¼—åŒ…ï¼ˆâ€œä¼—äººçš„æ™ºæ…§â€ï¼‰å¦‚ä½•è¿ä½œçš„æœ€æ—©æåŠã€‚'
- en: 'Almost 100 years later, [Callison-Burch (2009)](https://aclanthology.org/D09-1030/)
    did an early study on using Amazon Mechanical Turk (AMT) to run non-expert human
    evaluation on Machine Translation (MT) tasks and even to rely on non-experts to
    create new gold reference translations. The setup for human evaluation was simple:
    Each turker is shown a source sentence, a reference translation, and 5 translations
    from 5 MT systems. They are asked to rank 5 translations from best to worst. Each
    task is completed by 5 turkers.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§çº¦100å¹´åŽï¼Œ[Callison-Burch (2009)](https://aclanthology.org/D09-1030/) è¿›è¡Œäº†ä¸€é¡¹æ—©æœŸç ”ç©¶ï¼Œä½¿ç”¨äºšé©¬é€Š
    Mechanical Turkï¼ˆAMTï¼‰æ¥è¿›è¡Œéžä¸“å®¶äººå‘˜å¯¹æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰ä»»åŠ¡çš„è¯„ä¼°ï¼Œç”šè‡³ä¾èµ–éžä¸“å®¶äººå‘˜åˆ›å»ºæ–°çš„é»„é‡‘å‚è€ƒç¿»è¯‘ã€‚ äººç±»è¯„ä¼°çš„è®¾ç½®å¾ˆç®€å•ï¼šæ¯ä¸ªå·¥äººè¢«å±•ç¤ºä¸€ä¸ªæºå¥å­ï¼Œä¸€ä¸ªå‚è€ƒç¿»è¯‘ï¼Œä»¥åŠæ¥è‡ª5ä¸ªMTç³»ç»Ÿçš„5ä¸ªç¿»è¯‘ã€‚
    ä»–ä»¬è¢«è¦æ±‚å°†5ä¸ªç¿»è¯‘ä»Žæœ€å¥½åˆ°æœ€å·®è¿›è¡ŒæŽ’åã€‚ æ¯ä¸ªä»»åŠ¡ç”±5ä¸ªå·¥äººå®Œæˆã€‚
- en: 'Unsurprisingly, there are spammers producing low quality annotation to only
    optimize the volume. So when measuring the agreement between experts and non-experts,
    different weighting schemes need to be applied to downweight the contribution
    of spammers: (1) â€œweighted by expertsâ€: using agreement rate with experts on a
    gold set of 10 examples; (2) â€œweighted by non-expertsâ€: relying on agreement rate
    with the rest of turkers on the whole dataset.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯«ä¸å¥‡æ€ªï¼Œæœ‰äº›åžƒåœ¾é‚®ä»¶åˆ¶é€ å•†åªä¼˜åŒ–ä½“ç§¯è€Œäº§ç”Ÿä½Žè´¨é‡çš„æ³¨é‡Šã€‚ å› æ­¤ï¼Œåœ¨è¡¡é‡ä¸“å®¶å’Œéžä¸“å®¶ä¹‹é—´çš„ä¸€è‡´æ€§æ—¶ï¼Œéœ€è¦åº”ç”¨ä¸åŒçš„åŠ æƒæ–¹æ¡ˆæ¥é™ä½Žåžƒåœ¾é‚®ä»¶åˆ¶é€ å•†çš„è´¡çŒ®ï¼š(1)â€œç”±ä¸“å®¶åŠ æƒâ€ï¼šä½¿ç”¨ä¸Žä¸“å®¶åœ¨ä¸€ä¸ªåŒ…å«10ä¸ªç¤ºä¾‹çš„é»„é‡‘é›†ä¸Šçš„ä¸€è‡´æ€§çŽ‡;
    (2)â€œç”±éžä¸“å®¶åŠ æƒâ€ï¼šä¾èµ–äºŽä¸Žæ•´ä¸ªæ•°æ®é›†ä¸Šçš„å…¶ä»–å·¥äººçš„ä¸€è‡´æ€§çŽ‡ã€‚
- en: In a harder task, non-expert human annotators were asked to create new gold
    reference translations. Callison-Burch designed the task in two stages, where
    the first stage created new translations with reference to MT outputs and the
    second one filtered translations that may seem to be gerated by a MT system. The
    correlation between expertsâ€™ and crowdsourced translations is higher than that
    between expert and MT system outputs.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸€ä¸ªæ›´å›°éš¾çš„ä»»åŠ¡ä¸­ï¼Œéžä¸“å®¶äººç±»æ³¨é‡Šè€…è¢«è¦æ±‚åˆ›å»ºæ–°çš„é»„é‡‘å‚è€ƒç¿»è¯‘ã€‚ Callison-Burchè®¾è®¡äº†ä¸¤ä¸ªé˜¶æ®µçš„ä»»åŠ¡ï¼Œç¬¬ä¸€é˜¶æ®µæ ¹æ®MTè¾“å‡ºåˆ›å»ºæ–°çš„ç¿»è¯‘ï¼Œç¬¬äºŒé˜¶æ®µè¿‡æ»¤å¯èƒ½ç”±MTç³»ç»Ÿç”Ÿæˆçš„ç¿»è¯‘ã€‚
    ä¸“å®¶å’Œä¼—åŒ…ç¿»è¯‘ä¹‹é—´çš„ç›¸å…³æ€§é«˜äºŽä¸“å®¶å’ŒMTç³»ç»Ÿè¾“å‡ºä¹‹é—´çš„ç›¸å…³æ€§ã€‚
- en: '![](../Images/9652c419600380a5ca8f07332498b6d6.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9652c419600380a5ca8f07332498b6d6.png)'
- en: 'Fig. 3\. (Left) The agreement rate is measured by comparing each pair of translation
    sentences ("A > B", "A=B", "A < B") and thus chance agreement is 1/3\. The upper
    bound is set by the expert-expert agreement rate. (Right) Comparison of BLEU score
    between translations from different sources. LCD (Linguistic Data Consortium)
    translators provide expert translations. (Image source: [Callison-Burch 2009](https://aclanthology.org/D09-1030/))'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3.ï¼ˆå·¦ï¼‰é€šè¿‡æ¯”è¾ƒæ¯å¯¹ç¿»è¯‘å¥å­ï¼ˆ"A > B"ï¼Œ"A=B"ï¼Œ"A < B"ï¼‰æ¥è¡¡é‡ä¸€è‡´æ€§çŽ‡ï¼Œå› æ­¤å¶ç„¶ä¸€è‡´æ€§çŽ‡ä¸º1/3ã€‚ ä¸Šé™ç”±ä¸“å®¶-ä¸“å®¶ä¸€è‡´æ€§çŽ‡è®¾å®šã€‚
    ï¼ˆå³ï¼‰æ¯”è¾ƒæ¥è‡ªä¸åŒæ¥æºçš„ç¿»è¯‘ä¹‹é—´çš„BLEUåˆ†æ•°ã€‚ LCDï¼ˆè¯­è¨€æ•°æ®è”ç›Ÿï¼‰ç¿»è¯‘äººå‘˜æä¾›ä¸“å®¶ç¿»è¯‘ã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼š[Callison-Burch 2009](https://aclanthology.org/D09-1030/)ï¼‰
- en: Rater Agreement
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯„åˆ†è€…ä¸€è‡´æ€§
- en: We often think of annotation as targeting a single ground truth and try to evaluate
    quality against one gold answer with consistent standards. A common practice for
    finding reliable ground truth labels is to collect multiple labels from multiple
    raters. Assuming that each rater performs at a different level of quality, we
    can use a weighted average of annotations but weighted by a proficiency score.
    This score is often approximated by how often one rater agrees with others.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç»å¸¸è®¤ä¸ºæ³¨é‡Šæ˜¯é’ˆå¯¹å•ä¸€çœŸç›¸ï¼Œå¹¶å°è¯•æ ¹æ®ä¸€è‡´çš„æ ‡å‡†è¯„ä¼°è´¨é‡ã€‚ æ‰¾åˆ°å¯é çš„åŸºå‡†æ ‡ç­¾çš„å¸¸è§åšæ³•æ˜¯ä»Žå¤šä¸ªè¯„åˆ†è€…é‚£é‡Œæ”¶é›†å¤šä¸ªæ ‡ç­¾ã€‚ å‡è®¾æ¯ä¸ªè¯„åˆ†è€…çš„è´¨é‡æ°´å¹³ä¸åŒï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ³¨é‡Šçš„åŠ æƒå¹³å‡å€¼ï¼Œä½†æ˜¯æ ¹æ®ç†Ÿç»ƒåº¦å¾—åˆ†åŠ æƒã€‚
    è¿™ä¸ªåˆ†æ•°é€šå¸¸æ˜¯é€šè¿‡è¯„åˆ†è€…ä¸Žå…¶ä»–äººä¸€è‡´çš„é¢‘çŽ‡æ¥è¿‘ä¼¼çš„ã€‚
- en: '**Majority Voting**: Taking the majority vote is the simplest way of aggregation,
    equivalent to taking the [mode](https://en.wikipedia.org/wiki/Mode_(statistics))
    of a set of labels. In this setting, every annotator is contributing equally.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¤šæ•°æŠ•ç¥¨**ï¼šé‡‡å–å¤šæ•°æŠ•ç¥¨æ˜¯æœ€ç®€å•çš„èšåˆæ–¹å¼ï¼Œç›¸å½“äºŽå–ä¸€ç»„æ ‡ç­¾çš„[ä¼—æ•°](https://en.wikipedia.org/wiki/Mode_(statistics))ã€‚
    åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¯ä¸ªæ³¨é‡Šè€…éƒ½æ˜¯å¹³ç­‰è´¡çŒ®çš„ã€‚'
- en: '**Raw agreement** ([Tratz & Hovy, 2010](https://aclanthology.org/P10-1070/)):
    Raw agreement counts the percentage of other people agreeing with them. This is
    indirectly correlated to majority vote, because all members of the majority class
    are expected to get higher inter-annotator agreement rate.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**åŽŸå§‹ä¸€è‡´æ€§** ([Tratz & Hovy, 2010](https://aclanthology.org/P10-1070/))ï¼šåŽŸå§‹ä¸€è‡´æ€§è®¡ç®—å…¶ä»–äººåŒæ„çš„ç™¾åˆ†æ¯”ã€‚
    è¿™ä¸Žå¤šæ•°æŠ•ç¥¨é—´æŽ¥ç›¸å…³ï¼Œå› ä¸ºé¢„æœŸå¤§å¤šæ•°ç±»åˆ«çš„æ‰€æœ‰æˆå‘˜éƒ½ä¼šèŽ·å¾—æ›´é«˜çš„æ ‡æ³¨è€…é—´ä¸€è‡´æ€§çŽ‡ã€‚'
- en: '**Cohenâ€™s Kappa** ([Landis & Koch, 1977](https://www.jstor.org/stable/2529310)):
    Cohenâ€™s kappa measures the inter-rater agreement in the form of $\kappa = (p_o
    - p_e) / (1 - p_c)$, where $p_o$ is the raw agreement rate and $p_e$ is the agreement
    by chance. Cohenâ€™s kappa has a correction term for agreeing by chance, but this
    correction may be overestimated if one label is more prevalent.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**Cohen''s Kappa**ï¼ˆ[Landis & Koch, 1977](https://www.jstor.org/stable/2529310)ï¼‰ï¼šCohen''s
    Kappaä»¥$\kappa = (p_o - p_e) / (1 - p_c)$çš„å½¢å¼è¡¡é‡è¯„åˆ†è€…é—´çš„ä¸€è‡´æ€§ï¼Œå…¶ä¸­$p_o$æ˜¯åŽŸå§‹ä¸€è‡´çŽ‡ï¼Œ$p_e$æ˜¯éšæœºä¸€è‡´çŽ‡ã€‚Cohen''s
    Kappaå¯¹äºŽé€šè¿‡å¶ç„¶ä¸€è‡´æ€§æœ‰ä¸€ä¸ªæ ¡æ­£é¡¹ï¼Œä½†å¦‚æžœä¸€ä¸ªæ ‡ç­¾æ›´æ™®éï¼Œåˆ™è¿™ç§æ ¡æ­£å¯èƒ½è¢«é«˜ä¼°ã€‚'
- en: '**Probabilistic Graph Modeling**: There is a body of work relying on [probabilistic
    graph modeling](https://en.wikipedia.org/wiki/Graphical_model) to model different
    factors within annotation decisions, e.g. difficulty of the task, task latent
    topics, rater bias, rater confidence, and then predict the true labels accordingly.
    [Zheng et al. (2017)](https://dl.acm.org/doi/abs/10.14778/3055540.3055547) compared
    17 algorithms on truth inference in crowdsourcing and most of them are probabilistic
    graph models.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ¦‚çŽ‡å›¾å»ºæ¨¡**ï¼šæœ‰ä¸€ç³»åˆ—ä¾èµ–äºŽ[æ¦‚çŽ‡å›¾å»ºæ¨¡](https://en.wikipedia.org/wiki/Graphical_model)çš„å·¥ä½œï¼Œç”¨äºŽæ¨¡æ‹Ÿæ ‡æ³¨å†³ç­–ä¸­çš„ä¸åŒå› ç´ ï¼Œä¾‹å¦‚ä»»åŠ¡éš¾åº¦ã€ä»»åŠ¡æ½œåœ¨ä¸»é¢˜ã€è¯„åˆ†è€…åè§ã€è¯„åˆ†è€…ä¿¡å¿ƒï¼Œç„¶åŽç›¸åº”åœ°é¢„æµ‹çœŸå®žæ ‡ç­¾ã€‚[Zhengç­‰äººï¼ˆ2017ï¼‰](https://dl.acm.org/doi/abs/10.14778/3055540.3055547)æ¯”è¾ƒäº†17ç§ç®—æ³•åœ¨ä¼—åŒ…ä¸­çš„çœŸå®žæŽ¨æ–­ï¼Œå…¶ä¸­å¤§å¤šæ•°æ˜¯æ¦‚çŽ‡å›¾æ¨¡åž‹ã€‚'
- en: '**MACE** (Multi-Annotator Competence Estimation; [Hovy et al. 2013](https://aclanthology.org/N13-1132))
    is an early example of using graph modeling to estimate the likelihood of someone
    acting like a â€œspammerâ€ by providing random labels. Unsurprisingly in cases when
    the incentive is misaligned, some annotators may behave as â€œspammersâ€ to optimize
    the volume of tasks completed for higher pay. The goal of MACE is to identify
    spammers. Given a task $i$ and an annotator $j$, $T_i$ is the true label, $A_{ij}$
    is the assigned label and $S_{ij}$ models the probability of annotator $j$ spamming.
    Then the generative process can be represented as belows. The parameter $\theta_j$
    defines the trustworthiness of the annotator $j$ (probability of not spamming)
    and the parameter $\xi_j$ defines how an annotator behaves when they are spamming.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MACE**ï¼ˆå¤šæ³¨é‡Šè€…èƒ½åŠ›ä¼°è®¡ï¼›[Hovyç­‰äººï¼Œ2013](https://aclanthology.org/N13-1132)ï¼‰æ˜¯ä½¿ç”¨å›¾å»ºæ¨¡æ—©æœŸçš„ä¾‹å­ï¼Œç”¨äºŽä¼°è®¡æŸäººåƒâ€œåžƒåœ¾é‚®ä»¶å‘é€è€…â€ä¸€æ ·æä¾›éšæœºæ ‡ç­¾çš„å¯èƒ½æ€§ã€‚åœ¨æ¿€åŠ±ä¸ä¸€è‡´çš„æƒ…å†µä¸‹ï¼Œä¸€äº›æ³¨é‡Šè€…å¯èƒ½ä¼šè¡¨çŽ°ä¸ºâ€œåžƒåœ¾é‚®ä»¶å‘é€è€…â€ï¼Œä»¥ä¼˜åŒ–å®Œæˆæ›´å¤šä»»åŠ¡ä»¥èŽ·å–æ›´é«˜æŠ¥é…¬ã€‚MACEçš„ç›®æ ‡æ˜¯è¯†åˆ«åžƒåœ¾é‚®ä»¶å‘é€è€…ã€‚ç»™å®šä»»åŠ¡$i$å’Œæ³¨é‡Šè€…$j$ï¼Œ$T_i$æ˜¯çœŸå®žæ ‡ç­¾ï¼Œ$A_{ij}$æ˜¯åˆ†é…çš„æ ‡ç­¾ï¼Œ$S_{ij}$æ¨¡æ‹Ÿäº†æ³¨é‡Šè€…$j$å‘é€åžƒåœ¾é‚®ä»¶çš„æ¦‚çŽ‡ã€‚ç„¶åŽç”Ÿæˆè¿‡ç¨‹å¯ä»¥è¡¨ç¤ºå¦‚ä¸‹ã€‚å‚æ•°$\theta_j$å®šä¹‰äº†æ³¨é‡Šè€…$j$çš„å¯ä¿¡åº¦ï¼ˆä¸å‘é€åžƒåœ¾é‚®ä»¶çš„æ¦‚çŽ‡ï¼‰ï¼Œå‚æ•°$\xi_j$å®šä¹‰äº†æ³¨é‡Šè€…åœ¨å‘é€åžƒåœ¾é‚®ä»¶æ—¶çš„è¡Œä¸ºã€‚'
- en: '$$ \begin{align} & \text{for } i = 1 \dots N : \\ & \quad T_i \sim \text{Uniform}
    \\ & \quad \text{for } j = 1 \dots M : \\ & \quad \quad S_{ij} \sim \text{Bernoulli}(1
    - \theta_j) \\ & \quad \quad \text{if } S_{ij} = 0 : \\ & \quad \quad \quad A_{ij}
    = T_i \\ & \quad \quad \text{else } : \\ & \quad \quad \quad A_{ij} \sim \text{Multinomial}(\xi_j)
    \\ \end{align} $$'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '$$ \begin{align} & \text{for } i = 1 \dots N : \\ & \quad T_i \sim \text{Uniform}
    \\ & \quad \text{for } j = 1 \dots M : \\ & \quad \quad S_{ij} \sim \text{Bernoulli}(1
    - \theta_j) \\ & \quad \quad \text{if } S_{ij} = 0 : \\ & \quad \quad \quad A_{ij}
    = T_i \\ & \quad \quad \text{else } : \\ & \quad \quad \quad A_{ij} \sim \text{Multinomial}(\xi_j)
    \\ \end{align} $$'
- en: 'Then we can learn $\theta, \xi$ to maximize the observed data, in the form
    of the marginal data likelihood, where $A$ is the matrix of annotations, $S$ is
    the matrix of competence indicators and $T$ is the matrix of true labels:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åŽæˆ‘ä»¬å¯ä»¥å­¦ä¹ $\theta, \xi$ä»¥æœ€å¤§åŒ–è§‚å¯Ÿåˆ°çš„æ•°æ®ï¼Œä»¥è¾¹é™…æ•°æ®ä¼¼ç„¶çš„å½¢å¼ï¼Œå…¶ä¸­$A$æ˜¯æ³¨é‡ŠçŸ©é˜µï¼Œ$S$æ˜¯èƒ½åŠ›æŒ‡ç¤ºå™¨çŸ©é˜µï¼Œ$T$æ˜¯çœŸå®žæ ‡ç­¾çŸ©é˜µï¼š
- en: $$ P(A; \theta, \xi) = \sum_{T, S} \big[ \prod_{i=1}^N P(T_i) \cdot \prod_{j=1}^M
    P(S_{ij}; \theta_j) \cdot P(A_{ij} \vert S_{ij}, T_i; \xi_j) \big] $$
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: $$ P(A; \theta, \xi) = \sum_{T, S} \big[ \prod_{i=1}^N P(T_i) \cdot \prod_{j=1}^M
    P(S_{ij}; \theta_j) \cdot P(A_{ij} \vert S_{ij}, T_i; \xi_j) \big] $$
- en: Either EM (Expectationâ€“maximization) or VB (Variational Bayes) can be applied
    to maximize the above marginal likelihood. During EM optimization, at M-step,
    a fixed value $\delta$ is added to the fractional counts before normalizing. During
    VB training, they applied symmetric Beta priors on $\theta_j$ and symmetric Dirichlet
    priors on $\xi_j$. When recovering the correct answers, we can take majority vote
    weighted by the annotatorsâ€™ $\theta$ estimates.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥åº”ç”¨EMï¼ˆæœŸæœ›æœ€å¤§åŒ–ï¼‰æˆ–VBï¼ˆå˜åˆ†è´å¶æ–¯ï¼‰æ¥æœ€å¤§åŒ–ä¸Šè¿°è¾¹é™…ä¼¼ç„¶ã€‚åœ¨EMä¼˜åŒ–æœŸé—´ï¼Œåœ¨Mæ­¥éª¤ä¸­ï¼Œåœ¨å½’ä¸€åŒ–ä¹‹å‰ï¼Œä¼šå°†å›ºå®šå€¼$\delta$æ·»åŠ åˆ°åˆ†æ•°è®¡æ•°ä¸­ã€‚åœ¨VBè®­ç»ƒæœŸé—´ï¼Œä»–ä»¬åœ¨$\theta_j$ä¸Šåº”ç”¨å¯¹ç§°Betaå…ˆéªŒï¼Œåœ¨$\xi_j$ä¸Šåº”ç”¨å¯¹ç§°Dirichletå…ˆéªŒã€‚åœ¨æ¢å¤æ­£ç¡®ç­”æ¡ˆæ—¶ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡ç”¨æŒ‰æ³¨é‡Šè€…$\theta$ä¼°è®¡åŠ æƒçš„å¤šæ•°æŠ•ç¥¨ã€‚
- en: Rater Disagreement & Two Paradigms
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯„åˆ†è€…ä¸ä¸€è‡´ & ä¸¤ç§èŒƒå¼
- en: The aggregation process described above depends on an assumption that there
    exists *one* underlying gold answer and thus we can evaluate annotatorsâ€™ performance
    accordingly. However, in many topics, especially in safety, social, or cultural
    areas, people can disagree and often this disagreement is valid and then it comes
    down to how much we want to apply a strict rule versus embracing diversity.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°èšåˆè¿‡ç¨‹ä¾èµ–äºŽè¿™æ ·ä¸€ä¸ªå‡è®¾ï¼Œå³å­˜åœ¨*ä¸€ä¸ª*æ½œåœ¨çš„é»„é‡‘ç­”æ¡ˆï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ç›¸åº”åœ°è¯„ä¼°æ³¨é‡Šè€…çš„è¡¨çŽ°ã€‚ç„¶è€Œï¼Œåœ¨è®¸å¤šä¸»é¢˜ä¸­ï¼Œç‰¹åˆ«æ˜¯åœ¨å®‰å…¨ã€ç¤¾ä¼šæˆ–æ–‡åŒ–é¢†åŸŸï¼Œäººä»¬å¯èƒ½å­˜åœ¨åˆ†æ­§ï¼Œè€Œä¸”è¿™ç§åˆ†æ­§é€šå¸¸æ˜¯æœ‰æ•ˆçš„ï¼Œç„¶åŽé—®é¢˜å°±åœ¨äºŽæˆ‘ä»¬æœ‰å¤šå¤§ç¨‹åº¦ä¸Šæƒ³è¦åº”ç”¨ä¸¥æ ¼çš„è§„åˆ™è€Œä¸æ˜¯æ‹¥æŠ±å¤šæ ·æ€§ã€‚
- en: '[Aroyo & Welty (2015)](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2564)
    discussed a set of â€œmythsâ€ in the practice of human annotation collection and
    found all of them somewhat inaccurate, key findings including:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[Aroyo & Weltyï¼ˆ2015ï¼‰](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2564)
    è®¨è®ºäº†äººç±»æ³¨é‡Šæ”¶é›†å®žè·µä¸­çš„ä¸€ç»„â€œç¥žè¯â€ï¼Œå‘çŽ°æ‰€æœ‰è¿™äº›ç¥žè¯éƒ½æœ‰äº›ä¸å‡†ç¡®ï¼Œä¸»è¦å‘çŽ°åŒ…æ‹¬ï¼š'
- en: Often there is more than one correct interpretation for some samples. We need
    diverse perspectives via e.g. having multiple people to review annotation quality.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºŽæŸäº›æ ·æœ¬ï¼Œå¾€å¾€å­˜åœ¨å¤šç§æ­£ç¡®çš„è§£é‡Šã€‚æˆ‘ä»¬éœ€è¦é€šè¿‡ä¾‹å¦‚è®©å¤šäººå®¡æŸ¥æ³¨é‡Šè´¨é‡æ¥èŽ·å¾—å¤šå…ƒåŒ–çš„è§‚ç‚¹ã€‚
- en: Disagreement is not always bad. We should reduce disagreements caused by errors
    or poorly designed process but other disagreements can give us rich information.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ä¸€è‡´å¹¶éžæ€»æ˜¯ä¸å¥½çš„ã€‚æˆ‘ä»¬åº”è¯¥å‡å°‘ç”±é”™è¯¯æˆ–è®¾è®¡ä¸ä½³çš„è¿‡ç¨‹å¼•èµ·çš„åˆ†æ­§ï¼Œä½†å…¶ä»–åˆ†æ­§å¯ä»¥ä¸ºæˆ‘ä»¬æä¾›ä¸°å¯Œçš„ä¿¡æ¯ã€‚
- en: If it is caused by a task not well defined, we should enhance the instruction.
    However, a more detailed guideline does not resolve innate diversity among opinions.
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æžœè¿™æ˜¯ç”±ä»»åŠ¡å®šä¹‰ä¸æ¸…æ™°å¼•èµ·çš„ï¼Œæˆ‘ä»¬åº”è¯¥åŠ å¼ºè¯´æ˜Žã€‚ç„¶è€Œï¼Œæ›´è¯¦ç»†çš„æŒ‡å—å¹¶ä¸èƒ½è§£å†³æ„è§ä¹‹é—´å›ºæœ‰çš„å¤šæ ·æ€§ã€‚
- en: Experts may not always be better than lay people, but they would have a big
    gap in terms of considering whatâ€™s important.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸“å®¶æœªå¿…æ€»æ˜¯æ¯”æ™®é€šäººæ›´å¥½ï¼Œä½†åœ¨è€ƒè™‘ä»€ä¹ˆæ˜¯é‡è¦çš„æ–¹é¢ï¼Œä»–ä»¬ä¹‹é—´å­˜åœ¨å¾ˆå¤§å·®è·ã€‚
- en: Ground truth annotations can change in time, especially those related to timely
    events or news.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ°é¢çœŸå®žæ³¨é‡Šå¯èƒ½ä¼šéšæ—¶é—´å˜åŒ–ï¼Œç‰¹åˆ«æ˜¯ä¸ŽåŠæ—¶äº‹ä»¶æˆ–æ–°é—»ç›¸å…³çš„æ³¨é‡Šã€‚
- en: Later, [Rottger et al. (2021)](https://arxiv.org/abs/2112.07475) formulated
    the difference into two contrasting paradigms for data annotation for subjective
    NLP tasks.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: éšåŽï¼Œ[Rottgerç­‰äººï¼ˆ2021ï¼‰](https://arxiv.org/abs/2112.07475) å°†è¿™ç§å·®å¼‚å½¢å¼åŒ–ä¸ºä¸»è§‚NLPä»»åŠ¡æ•°æ®æ³¨é‡Šçš„ä¸¤ç§å¯¹ç«‹èŒƒå¼ã€‚
- en: '|  | Descriptive | Prescriptive |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | æè¿°æ€§ | è§„èŒƒæ€§ |'
- en: '| --- | --- | --- |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Definition | Encourage annotator subjectivity, trying to model many beliefs.
    | Discourage annotator subjectivity, trying to consistently apply one belief.
    |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| å®šä¹‰ | é¼“åŠ±æ³¨é‡Šè€…ä¸»è§‚æ€§ï¼Œè¯•å›¾æ¨¡æ‹Ÿå¤šç§ä¿¡å¿µã€‚ | é˜»æ­¢æ³¨é‡Šè€…ä¸»è§‚æ€§ï¼Œè¯•å›¾ä¸€è‡´åœ°åº”ç”¨ä¸€ä¸ªä¿¡å¿µã€‚ |'
- en: '| Pros | - Can help to identify which entries are more subjective; - Embrace
    diversity'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '| ä¼˜ç‚¹ | - å¯å¸®åŠ©è¯†åˆ«å“ªäº›æ¡ç›®æ›´ä¸»è§‚ï¼› - æ‹¥æŠ±å¤šæ ·æ€§'
- en: '| - More aligned with standard NLP setup. - Easier to do QC by measuring disagreement
    or doing label aggregation.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '| - æ›´ç¬¦åˆæ ‡å‡†NLPè®¾ç½®ã€‚ - é€šè¿‡æµ‹é‡ä¸ä¸€è‡´æ€§æˆ–è¿›è¡Œæ ‡ç­¾èšåˆæ¥æ›´å®¹æ˜“è¿›è¡Œè´¨é‡æŽ§åˆ¶ã€‚'
- en: '|'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Cons | - Metrics like rater disagreement cannot be used to measure data quality
    or annotator performance; - Cannot be used for training models that are optimized
    for outputting one preset behavior.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '| ç¼ºç‚¹ | - æ— æ³•ä½¿ç”¨è¯„åˆ†è€…ä¸ä¸€è‡´ç­‰æŒ‡æ ‡æ¥è¡¡é‡æ•°æ®è´¨é‡æˆ–æ³¨é‡Šè€…è¡¨çŽ°ï¼› - ä¸èƒ½ç”¨äºŽè®­ç»ƒé’ˆå¯¹è¾“å‡ºä¸€ä¸ªé¢„è®¾è¡Œä¸ºè¿›è¡Œä¼˜åŒ–çš„æ¨¡åž‹ã€‚'
- en: '| - Expensive and challenging to create high-quality annotation guidelines,
    which can never be perfect, in practice; - Training annotators to get familiar
    with guideline in order to apply it properly is also challenging;'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '| - åˆ›å»ºé«˜è´¨é‡çš„æ³¨é‡ŠæŒ‡å—æ—¢æ˜‚è´µåˆå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œåœ¨å®žè·µä¸­æ°¸è¿œæ— æ³•å®Œç¾Žï¼› - åŸ¹è®­æ³¨é‡Šè€…ç†Ÿæ‚‰æŒ‡å—ä»¥ä¾¿æ­£ç¡®åº”ç”¨ä¹Ÿå…·æœ‰æŒ‘æˆ˜æ€§ï¼›'
- en: '- Cannot capture an interpretable diversity of beliefs or consistently encode
    one specific belief.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '- æ— æ³•æ•æ‰å¯è§£é‡Šçš„ä¿¡ä»°å¤šæ ·æ€§æˆ–å§‹ç»ˆç¼–ç ä¸€ä¸ªç‰¹å®šä¿¡å¿µã€‚'
- en: '|'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'The descriptive paradigm allows us to understand a number of important effects
    as well as to account for different perspectives. For example, annotator identity
    (e.g. African American, LGBTQ) is found to be a statistically significant factor
    in how they would label identify-related content as toxic ([Goyal et al. 2022](https://arxiv.org/abs/2205.00501)).
    Topics can be another main driver for diverse opinions. [Wang et al. (2023)](https://research.google/pubs/all-that-agrees-is-not-gold-evaluating-ground-truth-labels-and-dialogue-content-for-safety/)
    studied the human evaluation process of safety of an AI conversation system and
    compared results between labels by Trust & Safety (T&S) professionals and crowdsourcing
    annotators. They intentionally collected rich metadata associated with crowd annotators
    like demographic or behavior information. Comparing T&S expert labels and crowd
    annotations, they found that agreement rates vary across semantic topics and the
    level of severity:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æè¿°æ€§èŒƒå¼ä½¿æˆ‘ä»¬èƒ½å¤Ÿç†è§£è®¸å¤šé‡è¦æ•ˆåº”ï¼Œå¹¶è€ƒè™‘ä¸åŒçš„è§‚ç‚¹ã€‚ä¾‹å¦‚ï¼Œæ³¨é‡Šè€…èº«ä»½ï¼ˆä¾‹å¦‚éžè£”ç¾Žå›½äººï¼ŒLGBTQï¼‰è¢«å‘çŽ°æ˜¯ä¸€ä¸ªåœ¨ä»–ä»¬å¦‚ä½•æ ‡è®°ä¸Žèº«ä»½ç›¸å…³å†…å®¹ä¸ºæœ‰æ¯’æ—¶çš„ç»Ÿè®¡æ˜¾è‘—å› ç´ ï¼ˆ[Goyalç­‰äººï¼Œ2022](https://arxiv.org/abs/2205.00501)ï¼‰ã€‚ä¸»é¢˜å¯ä»¥æ˜¯ä¸åŒæ„è§çš„å¦ä¸€ä¸ªä¸»è¦é©±åŠ¨å› ç´ ã€‚[Wangç­‰äººï¼ˆ2023ï¼‰](https://research.google/pubs/all-that-agrees-is-not-gold-evaluating-ground-truth-labels-and-dialogue-content-for-safety/)ç ”ç©¶äº†äººç±»å¯¹è¯ç³»ç»Ÿå®‰å…¨æ€§çš„è¯„ä¼°è¿‡ç¨‹ï¼Œå¹¶æ¯”è¾ƒäº†ä¿¡ä»»ä¸Žå®‰å…¨ï¼ˆT&Sï¼‰ä¸“ä¸šäººå‘˜å’Œä¼—åŒ…æ³¨é‡Šè€…æ ‡ç­¾ä¹‹é—´çš„ç»“æžœã€‚ä»–ä»¬æœ‰æ„æ”¶é›†äº†ä¸Žä¼—åŒ…æ³¨é‡Šè€…ç›¸å…³çš„ä¸°å¯Œå…ƒæ•°æ®ï¼Œå¦‚äººå£ç»Ÿè®¡ä¿¡æ¯æˆ–è¡Œä¸ºä¿¡æ¯ã€‚æ¯”è¾ƒT&Sä¸“å®¶æ ‡ç­¾å’Œä¼—åŒ…æ³¨é‡Šï¼Œä»–ä»¬å‘çŽ°åè®®çŽ‡åœ¨è¯­ä¹‰ä¸»é¢˜å’Œä¸¥é‡ç¨‹åº¦ä¸Šæœ‰æ‰€ä¸åŒï¼š
- en: Agreement rate differs a lot across different topics; ranging from 0.96 on violence/gory
    to 0.25 on personal topics.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åè®®çŽ‡åœ¨ä¸åŒä¸»é¢˜ä¹‹é—´å·®å¼‚å¾ˆå¤§ï¼›ä»Žæš´åŠ›/è¡€è…¥çš„0.96åˆ°ä¸ªäººä¸»é¢˜çš„0.25ä¸ç­‰ã€‚
- en: Agreement rates are higher on â€œextremeâ€ and â€œbenignâ€ conversations, given four
    label options marking â€œbenignâ€, â€œdebatableâ€, â€œmoderateâ€ to â€œextremeâ€.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨â€œæžç«¯â€å’Œâ€œè‰¯æ€§â€å¯¹è¯ä¸­ï¼Œåè®®çŽ‡è¾ƒé«˜ï¼Œç»™å‡ºå››ä¸ªæ ‡ç­¾é€‰é¡¹æ ‡è®°ä¸ºâ€œè‰¯æ€§â€ï¼Œâ€œæœ‰äº‰è®®â€ï¼Œâ€œä¸­ç­‰â€åˆ°â€œæžç«¯â€ã€‚
- en: '![](../Images/5cc3e2f47f65949e121a8d69adec8928.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/5cc3e2f47f65949e121a8d69adec8928.png)'
- en: 'Fig. 4\. Correlations between non-expert and expert annotations vary a lot
    across topics. (Image source: [Wang et al. 2023](https://research.google/pubs/all-that-agrees-is-not-gold-evaluating-ground-truth-labels-and-dialogue-content-for-safety/))'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4. éžä¸“å®¶å’Œä¸“å®¶æ³¨é‡Šä¹‹é—´çš„ç›¸å…³æ€§åœ¨ä¸åŒä¸»é¢˜ä¹‹é—´å·®å¼‚å¾ˆå¤§ã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼š[Wangç­‰äººï¼Œ2023](https://research.google/pubs/all-that-agrees-is-not-gold-evaluating-ground-truth-labels-and-dialogue-content-for-safety/)ï¼‰
- en: '[Zhang et al. (2023)](https://arxiv.org/abs/2311.04345) proposed a taxonomy
    of rater disagreement to analyze the root causes. Among the listed causes, disagreement
    due to stochastic errors or inconsistency on the individual level should be avoided.
    In cases when a rater gives different labels to the same task when asked multiple
    times, some of those are most likely caused by human errors. Based on this intuition,
    the disagreement deconvolution method ([Gordon et al. 2021](https://dl.acm.org/doi/abs/10.1145/3411764.3445423))
    disentangles stable opinions from errors by anchoring each individualâ€™s opinion
    to their own primary label and thus encouraging *intra*-rater consistency.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[Zhangç­‰äººï¼ˆ2023ï¼‰](https://arxiv.org/abs/2311.04345)æå‡ºäº†ä¸€ä¸ªè¯„åˆ†è€…ä¸ä¸€è‡´æ€§çš„åˆ†ç±»æ³•ï¼Œä»¥åˆ†æžæ ¹æœ¬åŽŸå› ã€‚åœ¨åˆ—å‡ºçš„åŽŸå› ä¸­ï¼Œç”±äºŽéšæœºé”™è¯¯æˆ–ä¸ªä½“æ°´å¹³ä¸Šçš„ä¸ä¸€è‡´æ€§è€Œå¯¼è‡´çš„ä¸ä¸€è‡´æ€§åº”è¯¥è¢«é¿å…ã€‚åœ¨è¯„åˆ†è€…åœ¨å¤šæ¬¡è¯¢é—®æ—¶ç»™å‡ºç›¸åŒä»»åŠ¡ä¸åŒæ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œå…¶ä¸­ä¸€äº›å¾ˆå¯èƒ½æ˜¯ç”±äººä¸ºé”™è¯¯å¼•èµ·çš„ã€‚åŸºäºŽè¿™ç§ç›´è§‰ï¼Œä¸ä¸€è‡´æ€§è§£æž„æ–¹æ³•ï¼ˆ[Gordonç­‰äººï¼Œ2021](https://dl.acm.org/doi/abs/10.1145/3411764.3445423)ï¼‰é€šè¿‡å°†æ¯ä¸ªä¸ªä½“çš„æ„è§é”šå®šåˆ°ä»–ä»¬è‡ªå·±çš„ä¸»è¦æ ‡ç­¾ï¼Œä»Žè€Œé¼“åŠ±*å†…éƒ¨*è¯„åˆ†è€…ä¸€è‡´æ€§ã€‚'
- en: '![](../Images/926cd3e977f2849c1cfd71c5db545df7.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/926cd3e977f2849c1cfd71c5db545df7.png)'
- en: 'Fig. 5\. A taxonomy of causes for rater disagreement. (Image source: [Zhang
    et al. 2023](https://arxiv.org/abs/2311.04345))'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5. è¯„åˆ†è€…ä¸ä¸€è‡´æ€§çš„åŽŸå› åˆ†ç±»æ³•ã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼š[Zhangç­‰äººï¼Œ2023](https://arxiv.org/abs/2311.04345)ï¼‰
- en: 'Disagreement deconvolution relies on probabilistic graph modeling:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä¸€è‡´æ€§è§£æž„ä¾èµ–äºŽæ¦‚çŽ‡å›¾å»ºæ¨¡ï¼š
- en: Estimate how often an annotator returns non-primary labels, $p_\text{flip}$
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¼°è®¡æ³¨é‡Šè€…è¿”å›žéžä¸»è¦æ ‡ç­¾çš„é¢‘çŽ‡ï¼Œ$p_\text{flip}$
- en: Per sample, get an adjusted label distribution $p^*$ of primary labels based
    on $p_\text{flip}$
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¯ä¸ªæ ·æœ¬ï¼Œæ ¹æ®$p_\text{flip}$å¾—åˆ°ä¸€ä¸ªè°ƒæ•´åŽçš„ä¸»è¦æ ‡ç­¾åˆ†å¸ƒ$p^*$
- en: Sample from $p^*$ as a new test set.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»Ž$p^*$ä¸­æŠ½å–ä¸€ä¸ªæ–°çš„æµ‹è¯•é›†ã€‚
- en: Measure performance metrics against the new test set.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ ¹æ®æ–°çš„æµ‹è¯•é›†æ¥è¡¡é‡æ€§èƒ½æŒ‡æ ‡ã€‚
- en: 'Given $C$-category classification, the sampling process of the generative model
    is stated as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®š$C$ç±»åˆ«åˆ†ç±»ï¼Œç”Ÿæˆæ¨¡åž‹çš„æŠ½æ ·è¿‡ç¨‹å¦‚ä¸‹æ‰€è¿°ï¼š
- en: $$ \begin{aligned} y^*\mid x &\sim \text{Categorial}([C], p^*(y\mid x)) \\ y_\text{other}\mid
    y^* &\sim \text{Categorial}([C]\setminus\{y^*\}, \frac{1}{C-1}) \\ z_\text{flip}
    \mid x &\sim \text{Bernoulli}(p_\text{flip}(x)) \\ y\mid y^*, y_\text{other},
    z_\text{flip} &= y^* (1 - z_\text{flip}) + y_\text{other} z_\text{flip} \end{aligned}
    $$
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} y^*\mid x &\sim \text{åˆ†ç±»}([C], p^*(y\mid x)) \\ y_\text{other}\mid
    y^* &\sim \text{åˆ†ç±»}([C]\setminus\{y^*\}, \frac{1}{C-1}) \\ z_\text{flip} \mid
    x &\sim \text{ä¼¯åŠªåˆ©}(p_\text{flip}(x)) \\ y\mid y^*, y_\text{other}, z_\text{flip}
    &= y^* (1 - z_\text{flip}) + y_\text{other} z_\text{flip} \end{aligned} $$
- en: 'Given the true $p(y\mid x)$ and $p_\text{flip}$ that can be estimated from
    the data, we would update the label distribution of primary labels:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šå¯ä»¥ä»Žæ•°æ®ä¸­ä¼°è®¡çš„çœŸå®ž$p(y\mid x)$å’Œ$p_\text{flip}$ï¼Œæˆ‘ä»¬å°†æ›´æ–°ä¸»è¦æ ‡ç­¾çš„æ ‡ç­¾åˆ†å¸ƒï¼š
- en: $$ p^*(y\mid x) = \frac{p(y\mid x) - \frac{p_\text{flip}(x)}{C-1}}{1 - \frac{C
    \cdot p_\text{flip}(x)}{C - 1}} $$
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: $$ p^*(y\mid x) = \frac{p(y\mid x) - \frac{p_\text{flip}(x)}{C-1}}{1 - \frac{C
    \cdot p_\text{flip}(x)}{C - 1}} $$
- en: A new test set sampled from $p^*(y \mid x)$ represents the primary labels with
    individual inconsistency noise removed. It can be used for evaluation, as a noise-free
    test set.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ä»Ž$p^*(y \mid x)$ä¸­æŠ½å–çš„æ–°æµ‹è¯•é›†è¡¨ç¤ºåŽ»é™¤ä¸ªä½“ä¸ä¸€è‡´å™ªå£°çš„ä¸»è¦æ ‡ç­¾ã€‚å®ƒå¯ä»¥ç”¨äºŽè¯„ä¼°ï¼Œä½œä¸ºä¸€ä¸ªæ— å™ªå£°çš„æµ‹è¯•é›†ã€‚
- en: 'To capture systematic disagreement among annotators when learning to predict
    labels, [Davani et al. (2021)](https://arxiv.org/abs/2110.05719) experimented
    with a multi-annotator model where predicting each annotatorâ€™s labels is treated
    as one sub-task. Say, the classification task is defined on an annotated dataset
    $D=(X, A, Y)$, where $X$ is the text instances, $A$ is the set of annotators and
    $Y$ is the annotation matrix, $y_{ij} \in Y$ represents a binary label assigned
    by $a_j \in A$ to the sample $x_i \in X$. The majority vote for $x_i$ is denoted
    as $\bar{y}_{i,}$. The experiment is to train a classification head on top of
    a pre-trained BERT model and compares 4 setups:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ•æ‰å­¦ä¹ é¢„æµ‹æ ‡ç­¾æ—¶æ³¨é‡Šè€…ä¹‹é—´çš„ç³»ç»Ÿæ€§åˆ†æ­§ï¼Œ[Davaniç­‰äººï¼ˆ2021ï¼‰](https://arxiv.org/abs/2110.05719)å°è¯•äº†ä¸€ä¸ªå¤šæ³¨é‡Šè€…æ¨¡åž‹çš„å®žéªŒï¼Œå…¶ä¸­é¢„æµ‹æ¯ä¸ªæ³¨é‡Šè€…çš„æ ‡ç­¾è¢«è§†ä¸ºä¸€ä¸ªå­ä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œåˆ†ç±»ä»»åŠ¡åœ¨ä¸€ä¸ªå¸¦æ³¨é‡Šçš„æ•°æ®é›†$D=(X,
    A, Y)$ä¸Šå®šä¹‰ï¼Œå…¶ä¸­$X$æ˜¯æ–‡æœ¬å®žä¾‹ï¼Œ$A$æ˜¯æ³¨é‡Šè€…é›†åˆï¼Œ$Y$æ˜¯æ³¨é‡ŠçŸ©é˜µï¼Œ$y_{ij} \in Y$è¡¨ç¤º$A$ä¸­çš„$a_j$ä¸ºæ ·æœ¬$x_i \in
    X$åˆ†é…çš„äºŒè¿›åˆ¶æ ‡ç­¾ã€‚$x_i$çš„å¤šæ•°æŠ•ç¥¨è¡¨ç¤ºä¸º$\bar{y}_{i,}$ã€‚å®žéªŒæ˜¯åœ¨ä¸€ä¸ªé¢„è®­ç»ƒçš„BERTæ¨¡åž‹ä¹‹ä¸Šè®­ç»ƒä¸€ä¸ªåˆ†ç±»å¤´ï¼Œå¹¶æ¯”è¾ƒ4ç§è®¾ç½®ï¼š
- en: 'Baseline: Directly predict the majority vote $\bar{y}_i$, not using the full
    annotation matrix $Y$.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŸºçº¿ï¼šç›´æŽ¥é¢„æµ‹å¤šæ•°æŠ•ç¥¨$\bar{y}_i$ï¼Œä¸ä½¿ç”¨å®Œæ•´çš„æ³¨é‡ŠçŸ©é˜µ$Y$ã€‚
- en: 'Ensemble: Train one model per annotator separately to predict $y_{ij}$ and
    then the results are aggregated by majority vote.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é›†æˆï¼šåˆ†åˆ«è®­ç»ƒæ¯ä¸ªæ³¨é‡Šè€…çš„ä¸€ä¸ªæ¨¡åž‹æ¥é¢„æµ‹$y_{ij}$ï¼Œç„¶åŽé€šè¿‡å¤šæ•°æŠ•ç¥¨æ¥èšåˆç»“æžœã€‚
- en: 'Multi-label: Learn to predict $\vert A \vert$ labels to represent all annotatorsâ€™
    labels per sample $\langle y_{i1}, \dots, y_{i\vert A \vert} \rangle$, with a
    shared MLP layer and then outputs are aggregated.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¤šæ ‡ç­¾ï¼šå­¦ä¹ é¢„æµ‹$\vert A \vert$ä¸ªæ ‡ç­¾æ¥è¡¨ç¤ºæ¯ä¸ªæ ·æœ¬$\langle y_{i1}, \dots, y_{i\vert A \vert}
    \rangle$çš„æ‰€æœ‰æ³¨é‡Šè€…çš„æ ‡ç­¾ï¼Œä½¿ç”¨å…±äº«çš„MLPå±‚ï¼Œç„¶åŽèšåˆè¾“å‡ºã€‚
- en: 'Multi-task: Similar to multi-label, but each annotatorâ€™s prediction head is
    learned from a separated MLP layer, such that we allocate extra compute to learn
    the difference among annotators.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¤šä»»åŠ¡ï¼šç±»ä¼¼äºŽå¤šæ ‡ç­¾ï¼Œä½†æ¯ä¸ªæ³¨é‡Šè€…çš„é¢„æµ‹å¤´æ˜¯ä»Žä¸€ä¸ªå•ç‹¬çš„MLPå±‚å­¦ä¹ çš„ï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥åˆ†é…é¢å¤–çš„è®¡ç®—èµ„æºæ¥å­¦ä¹ æ³¨é‡Šè€…ä¹‹é—´çš„å·®å¼‚ã€‚
- en: Experiment results on the [GHC (Gab Hate Corpus)](https://osf.io/edua3/) dataset
    showed that the multi-task model achieves the best F1 score and also can naturally
    provide prediction uncertainty estimation, correlated with annotation disagreement.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[GHCï¼ˆGab Hate Corpusï¼‰](https://osf.io/edua3/)æ•°æ®é›†ä¸Šçš„å®žéªŒç»“æžœæ˜¾ç¤ºï¼Œå¤šä»»åŠ¡æ¨¡åž‹å®žçŽ°äº†æœ€ä½³çš„F1åˆ†æ•°ï¼Œå¹¶ä¸”è¿˜å¯ä»¥è‡ªç„¶åœ°æä¾›ä¸Žæ³¨é‡Šä¸ä¸€è‡´ç›¸å…³çš„é¢„æµ‹ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚
- en: '![](../Images/fda5099c17a2b907dc72b62f3a1bd8cc.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fda5099c17a2b907dc72b62f3a1bd8cc.png)'
- en: 'Fig. 6\. Illustration of different architectures for modeling multiple annotators''
    labels. (Image source: [Davani et al. 2021](https://arxiv.org/abs/2110.05719))'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾6. ä¸åŒæž¶æž„ç”¨äºŽå»ºæ¨¡å¤šä¸ªæ³¨é‡Šè€…çš„æ ‡ç­¾ã€‚ (å›¾ç‰‡æ¥æºï¼š[Davaniç­‰äºº 2021](https://arxiv.org/abs/2110.05719))
- en: Jury Learning ([Gordon et al. 2022](https://arxiv.org/abs/2202.02950)) mimics
    the [jury process](https://www.uscourts.gov/services-forms/jury-service/juror-selection-process)
    by modeling the different annotatorsâ€™ labeling behavior conditioned on their characteristics.
    Starting with a dataset with labels and demographic characteristics of each labeler,
    we train a model to learn to predict labels made by every individual annotator,
    each as a potential juror. At decision time, practitioners can specify the composition
    of a group of jurors to determine a sampling strategy. The final decision is made
    by aggregating labels from jurors from multiple trials.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: é™ªå®¡å›¢å­¦ä¹ ï¼ˆ[Gordon et al. 2022](https://arxiv.org/abs/2202.02950)ï¼‰æ¨¡æ‹Ÿäº†[é™ªå®¡å›¢æµç¨‹](https://www.uscourts.gov/services-forms/jury-service/juror-selection-process)ï¼Œé€šè¿‡å»ºæ¨¡ä¸åŒæ³¨é‡Šè€…çš„æ ‡æ³¨è¡Œä¸ºï¼Œæ¡ä»¶æ˜¯ä»–ä»¬çš„ç‰¹å¾ã€‚
    ä»Žå…·æœ‰æ¯ä¸ªæ ‡æ³¨è€…æ ‡ç­¾å’Œäººå£ç‰¹å¾çš„æ•°æ®é›†å¼€å§‹ï¼Œæˆ‘ä»¬è®­ç»ƒä¸€ä¸ªæ¨¡åž‹æ¥å­¦ä¹ é¢„æµ‹æ¯ä¸ªä¸ªä½“æ³¨é‡Šè€…æ‰€åšçš„æ ‡ç­¾ï¼Œæ¯ä¸ªéƒ½æ˜¯æ½œåœ¨çš„é™ªå®¡å‘˜ã€‚ åœ¨å†³ç­–æ—¶ï¼Œä»Žä¸šè€…å¯ä»¥æŒ‡å®šé™ªå®¡å›¢çš„ç»„æˆæ¥ç¡®å®šæŠ½æ ·ç­–ç•¥ã€‚
    æœ€ç»ˆå†³å®šæ˜¯é€šè¿‡æ±‡æ€»æ¥è‡ªå¤šæ¬¡å®¡åˆ¤çš„é™ªå®¡å‘˜çš„æ ‡ç­¾åšå‡ºçš„ã€‚
- en: '![](../Images/83c221c6e414739b04a4e34638db0cf3.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/83c221c6e414739b04a4e34638db0cf3.png)'
- en: 'Fig. 7\. Illustration of how jury learning works. (Image source: [Gordon et
    al. 2022](https://arxiv.org/abs/2202.02950))'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 7\. é™ªå®¡å›¢å­¦ä¹ çš„å·¥ä½œåŽŸç†ç¤ºæ„å›¾ã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼š[Gordon et al. 2022](https://arxiv.org/abs/2202.02950)ï¼‰
- en: The jury learning model is a [DCN (Deep & Cross network)](https://arxiv.org/abs/2008.13535)
    , commonly for recommendation use case, that is jointly trained to learn comment
    embedding, annotator embedding and group (annotatorâ€™s characteristics) embedding.
    The text content is processed by a pre-trained BERT, which is also jointly fine-tuned
    but for a shorter period to avoid overfitting.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: é™ªå®¡å›¢å­¦ä¹ æ¨¡åž‹æ˜¯ä¸€ä¸ª[DCNï¼ˆæ·±åº¦ä¸Žäº¤å‰ç½‘ç»œï¼‰](https://arxiv.org/abs/2008.13535)ï¼Œé€šå¸¸ç”¨äºŽæŽ¨èç”¨ä¾‹ï¼Œè”åˆè®­ç»ƒä»¥å­¦ä¹ è¯„è®ºåµŒå…¥ã€æ³¨é‡Šè€…åµŒå…¥å’Œç»„ï¼ˆæ³¨é‡Šè€…ç‰¹å¾ï¼‰åµŒå…¥ã€‚
    æ–‡æœ¬å†…å®¹é€šè¿‡é¢„è®­ç»ƒçš„ BERT å¤„ç†ï¼Œä¹Ÿè”åˆå¾®è°ƒï¼Œä½†æ—¶é—´è¾ƒçŸ­ä»¥é¿å…è¿‡æ‹Ÿåˆã€‚
- en: '![](../Images/762f64268fc04e7d64a685d7e95bc70b.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/762f64268fc04e7d64a685d7e95bc70b.png)'
- en: 'Fig. 8\. DCN model architecture for jury learning. (Image source: [Gordon et
    al. 2022](https://arxiv.org/abs/2202.02950))'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8\. ç”¨äºŽé™ªå®¡å›¢å­¦ä¹ çš„ DCN æ¨¡åž‹æž¶æž„ã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼š[Gordon et al. 2022](https://arxiv.org/abs/2202.02950)ï¼‰
- en: Their experiment runs on the [toxicity diversity dataset](https://data.esrg.stanford.edu/study/toxicity-perspectives)
    and compares jury learning with a baseline model which is a fine-tuned BERT to
    predict individual annotatorâ€™s label without using metadata. Performance is measured
    in MAE (mean absolute error). Jury learning consistently outperforms the annotator-agnostic
    baseline on the full test set as well as each group segment.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–ä»¬çš„å®žéªŒè¿è¡Œåœ¨[æœ‰æ¯’å¤šæ ·æ€§æ•°æ®é›†](https://data.esrg.stanford.edu/study/toxicity-perspectives)ä¸Šï¼Œå¹¶å°†é™ªå®¡å›¢å­¦ä¹ ä¸Žä¸€ä¸ªåŸºçº¿æ¨¡åž‹è¿›è¡Œæ¯”è¾ƒï¼Œè¯¥åŸºçº¿æ¨¡åž‹æ˜¯ä¸€ä¸ªç»è¿‡å¾®è°ƒçš„
    BERTï¼Œç”¨äºŽé¢„æµ‹ä¸ªä½“æ³¨é‡Šè€…çš„æ ‡ç­¾è€Œä¸ä½¿ç”¨å…ƒæ•°æ®ã€‚ æ€§èƒ½ä»¥ MAEï¼ˆå¹³å‡ç»å¯¹è¯¯å·®ï¼‰è¿›è¡Œè¡¡é‡ã€‚ é™ªå®¡å›¢å­¦ä¹ åœ¨æ•´ä¸ªæµ‹è¯•é›†ä»¥åŠæ¯ä¸ªç»„æ®µä¸Šå§‹ç»ˆä¼˜äºŽæ³¨é‡Šè€…æ— å…³çš„åŸºçº¿ã€‚
- en: '![](../Images/255290b72a600fa5151a65ba72051a01.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/255290b72a600fa5151a65ba72051a01.png)'
- en: 'Fig. 9\. Experiment results comparing an annotator-agnostic baseline with jury
    learning. (Image source: [Gordon et al. 2022](https://arxiv.org/abs/2202.02950))'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 9\. å®žéªŒç»“æžœæ¯”è¾ƒäº†ä¸€ä¸ªä¸Žé™ªå®¡å›¢å­¦ä¹ æ— å…³çš„åŸºçº¿æ¨¡åž‹ã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼š[Gordon et al. 2022](https://arxiv.org/abs/2202.02950)ï¼‰
- en: Data Quality â†” Model Training
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ•°æ®è´¨é‡ â†” æ¨¡åž‹è®­ç»ƒ
- en: Once a dataset is constructed, many methods can help identify mislabels according
    to the training dynamics. Note that we only focus on methods to find and exclude
    data points with potentially incorrect labels, not about [how to train a model
    with noisy data](https://lilianweng.github.io/posts/2022-04-15-data-gen/#training-with-noisy-data).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æž„å»ºäº†æ•°æ®é›†ï¼Œè®¸å¤šæ–¹æ³•å¯ä»¥å¸®åŠ©æ ¹æ®è®­ç»ƒåŠ¨æ€è¯†åˆ«é”™è¯¯æ ‡ç­¾ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬åªå…³æ³¨äºŽæ‰¾åˆ°å¹¶æŽ’é™¤æ½œåœ¨é”™è¯¯æ ‡ç­¾çš„æ•°æ®ç‚¹çš„æ–¹æ³•ï¼Œè€Œä¸æ˜¯å…³äºŽ[å¦‚ä½•ä½¿ç”¨æœ‰å™ªå£°æ•°æ®è®­ç»ƒæ¨¡åž‹](https://lilianweng.github.io/posts/2022-04-15-data-gen/#training-with-noisy-data)ã€‚
- en: Influence Functions
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å½±å“å‡½æ•°
- en: '**Influence functions** is a classic technique from robust statistics ([Hampel,
    1974](https://www.jstor.org/stable/2285666)) to measure the effect of training
    data points by describing how the model parameters change as we upweight a training
    point by an infinitesimal amount. [Koh & Liang (2017)](https://arxiv.org/abs/1703.04730)
    introduced the concept to be applied to deep neural networks.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**å½±å“å‡½æ•°**æ˜¯æ¥è‡ªå¥å£®ç»Ÿè®¡å­¦çš„ç»å…¸æŠ€æœ¯ï¼ˆ[Hampel, 1974](https://www.jstor.org/stable/2285666)ï¼‰ï¼Œç”¨äºŽè¡¡é‡è®­ç»ƒæ•°æ®ç‚¹çš„å½±å“ï¼Œæè¿°æ¨¡åž‹å‚æ•°åœ¨æˆ‘ä»¬å¾®è°ƒè®­ç»ƒç‚¹æ—¶å¦‚ä½•å˜åŒ–ã€‚[Koh
    & Liang (2017)](https://arxiv.org/abs/1703.04730) å°†è¿™ä¸ªæ¦‚å¿µå¼•å…¥åˆ°æ·±åº¦ç¥žç»ç½‘ç»œä¸­ã€‚'
- en: 'Given $n$ data samples in the train set, $z_i = (x_i, y_i)$ for $i =1, \dots,
    n$, The model parameter $\theta$ is optimized to minimize a loss: $\hat{\theta}
    = \arg\min_{\theta \in \Theta} \frac{1}{n}\sum_{i=1}^n \mathcal{L}(z_i, \theta)$.
    The change of model parameters after we remove a single data point $z$ is denoted
    as $\hat{\theta}_{-z} - \hat{\theta}$ where $\hat{\theta}_{-z} = \arg\min_{\theta
    \in \Theta} \frac{1}{n} \sum_{z_i \neq z} \mathcal{L}(z_i, \theta)$. However,
    computing this literally for every sample is too expensive. One way to approximate
    this is to compute the parameter change given a small upweight $\epsilon$ on $z$.
    By definition, the influence of upweighting $z$ by $\epsilon$ is given by:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šè®­ç»ƒé›†ä¸­çš„$n$ä¸ªæ•°æ®æ ·æœ¬ï¼Œ$z_i = (x_i, y_i)$ï¼Œ$i =1, \dots, n$ï¼Œæ¨¡åž‹å‚æ•°$\theta$è¢«ä¼˜åŒ–ä»¥æœ€å°åŒ–æŸå¤±ï¼š$\hat{\theta}
    = \arg\min_{\theta \in \Theta} \frac{1}{n}\sum_{i=1}^n \mathcal{L}(z_i, \theta)$ã€‚ç§»é™¤å•ä¸ªæ•°æ®ç‚¹$z$åŽçš„æ¨¡åž‹å‚æ•°å˜åŒ–è¡¨ç¤ºä¸º$\hat{\theta}_{-z}
    - \hat{\theta}$ï¼Œå…¶ä¸­$\hat{\theta}_{-z} = \arg\min_{\theta \in \Theta} \frac{1}{n}
    \sum_{z_i \neq z} \mathcal{L}(z_i, \theta)$ã€‚ç„¶è€Œï¼Œä¸ºæ¯ä¸ªæ ·æœ¬è®¡ç®—è¿™ä¸ªå˜åŒ–æ˜¯å¤ªæ˜‚è´µçš„ã€‚ä¸€ç§è¿‘ä¼¼çš„æ–¹æ³•æ˜¯è®¡ç®—åœ¨$z$ä¸ŠåŠ å°æƒé‡$\epsilon$æ—¶çš„å‚æ•°å˜åŒ–ã€‚æ ¹æ®å®šä¹‰ï¼Œå¯¹$z$è¿›è¡ŒåŠ æƒçš„å½±å“ç”±ä»¥ä¸‹å…¬å¼ç»™å‡ºï¼š
- en: $$ \mathcal{I}_{\text{up,params}}(z) = \frac{d\hat{\theta}_{\epsilon,z}}{d\epsilon}\bigg\vert_{\epsilon=0}=-\mathbf{H}^{-1}_{\hat{\theta}}
    \nabla_\theta \mathcal{L}(z, \hat{\theta}) $$
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathcal{I}_{\text{up,params}}(z) = \frac{d\hat{\theta}_{\epsilon,z}}{d\epsilon}\bigg\vert_{\epsilon=0}=-\mathbf{H}^{-1}_{\hat{\theta}}
    \nabla_\theta \mathcal{L}(z, \hat{\theta}) $$
- en: where $\hat{\theta}_{\epsilon,z} = \arg\min_{\theta \in \Theta} \frac{1}{n}\sum_{i=1}^n
    \mathcal{L}(z_i, \theta) + \epsilon L(z, \theta)$ and $\mathbf{H}^{-1}_{\hat{\theta}}
    = \frac{1}{n}\sum_{i=1}^n \nabla^2_\theta \mathcal{L}(z_i, \hat{\theta})$. Removing
    a data point $x$ is equivalent to upweighting it by $\epsilon = -\frac{1}{n}$
    and therefore $\hat{\theta}_{-z} - \hat{\theta} \approx -\frac{1}{n} \mathcal{I}_{\text{up,params}}(z)$.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­$\hat{\theta}_{\epsilon,z} = \arg\min_{\theta \in \Theta} \frac{1}{n}\sum_{i=1}^n
    \mathcal{L}(z_i, \theta) + \epsilon L(z, \theta)$ï¼Œ$\mathbf{H}^{-1}_{\hat{\theta}}
    = \frac{1}{n}\sum_{i=1}^n \nabla^2_\theta \mathcal{L}(z_i, \hat{\theta})$ã€‚ç§»é™¤æ•°æ®ç‚¹$x$ç­‰åŒäºŽåœ¨å…¶ä¸ŠåŠ æƒ$\epsilon
    = -\frac{1}{n}$ï¼Œå› æ­¤$\hat{\theta}_{-z} - \hat{\theta} \approx -\frac{1}{n} \mathcal{I}_{\text{up,params}}(z)$ã€‚
- en: 'The influence of upweighting $z$ on the loss at a test point $z_\text{test}$
    is given by applying the chain rule:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æµ‹è¯•ç‚¹$z_\text{test}$å¤„ï¼Œå¯¹$z$è¿›è¡ŒåŠ æƒå¯¹æŸå¤±çš„å½±å“å¯ä»¥é€šè¿‡åº”ç”¨é“¾å¼æ³•åˆ™å¾—åˆ°ï¼š
- en: $$ \begin{aligned} \mathcal{I}_{\text{up,loss}}(z, z_\text{test}) &= \frac{d
    \mathcal{L}(z_\text{test}, \hat{\theta}_{\epsilon,z})}{d\epsilon}\bigg\vert_{\epsilon=0}
    \\ &= \nabla_\theta \mathcal{L}(z_\text{test}, \hat{\theta})^\top \frac{d \hat{\theta}_{\epsilon,z}}{d\epsilon}\bigg\vert_{\epsilon=0}
    \\ &= - \nabla_\theta \mathcal{L}(z_\text{test}, \hat{\theta})^\top \mathbf{H}^{-1}_{\hat{\theta}}
    \nabla_\theta \mathcal{L}(z, \hat{\theta}) \end{aligned} $$
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \mathcal{I}_{\text{up,loss}}(z, z_\text{test}) &= \frac{d
    \mathcal{L}(z_\text{test}, \hat{\theta}_{\epsilon,z})}{d\epsilon}\bigg\vert_{\epsilon=0}
    \\ &= \nabla_\theta \mathcal{L}(z_\text{test}, \hat{\theta})^\top \frac{d \hat{\theta}_{\epsilon,z}}{d\epsilon}\bigg\vert_{\epsilon=0}
    \\ &= - \nabla_\theta \mathcal{L}(z_\text{test}, \hat{\theta})^\top \mathbf{H}^{-1}_{\hat{\theta}}
    \nabla_\theta \mathcal{L}(z, \hat{\theta}) \end{aligned} $$
- en: Using the influence function we can measure the effect of a single data point
    on model parameters and loss function in closed forms. It can help approximate
    leave-one-out retraining without actually running all the retraining. To identify
    mislabeled data, we can measure $\mathcal{I}_\text{up,loss}(z_i, z_i)$, approximating
    the prediction error on $z_i$ if $z_i$ is removed from the training set.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å½±å“å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥ä»¥é—­åˆå½¢å¼æµ‹é‡å•ä¸ªæ•°æ®ç‚¹å¯¹æ¨¡åž‹å‚æ•°å’ŒæŸå¤±å‡½æ•°çš„å½±å“ã€‚å®ƒå¯ä»¥å¸®åŠ©è¿‘ä¼¼ leave-one-out é‡æ–°è®­ç»ƒï¼Œè€Œæ— éœ€å®žé™…è¿è¡Œæ‰€æœ‰é‡æ–°è®­ç»ƒã€‚ä¸ºäº†è¯†åˆ«é”™è¯¯æ ‡è®°çš„æ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥æµ‹é‡$\mathcal{I}_\text{up,loss}(z_i,
    z_i)$ï¼Œè¿‘ä¼¼äºŽå¦‚æžœå°†$z_i$ä»Žè®­ç»ƒé›†ä¸­ç§»é™¤ï¼Œåˆ™$z_i$çš„é¢„æµ‹è¯¯å·®ã€‚
- en: '![](../Images/d20674396e2af7f5b7710d14a1e297e1.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d20674396e2af7f5b7710d14a1e297e1.png)'
- en: 'Fig. 10\. Influence functions values match leave-one-out training results on
    10-class MNIST. (Image source: [Kohn & Liang, 2017](https://arxiv.org/abs/1703.04730))'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾10. å½±å“å‡½æ•°å€¼ä¸Ž10ç±» MNIST ä¸Šçš„ leave-one-out è®­ç»ƒç»“æžœç›¸åŒ¹é…ã€‚ (å›¾ç‰‡æ¥æº: [Kohn & Liang, 2017](https://arxiv.org/abs/1703.04730))'
- en: Given the closed form, influence functions is still hard to be scaled up because
    the inverse Hessian vector product is hard to compute. [Grosse et al. (2023)](https://arxiv.org/abs/2308.03296)
    experimented with the EK-FAC (Eigenvalue-corrected Kronecker-Factored Approximate
    Curvature; [George et al. 2018](https://arxiv.org/abs/1806.03884)) approximation
    instead.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: é‰´äºŽé—­åˆå½¢å¼ï¼Œå½±å“å‡½æ•°ä»ç„¶å¾ˆéš¾æ‰©å±•ï¼Œå› ä¸ºæ±‚é€† Hessian çŸ¢é‡ä¹˜ç§¯å¾ˆéš¾è®¡ç®—ã€‚[Grosse ç­‰äºº (2023)](https://arxiv.org/abs/2308.03296)
    å°è¯•ä½¿ç”¨ EK-FACï¼ˆEigenvalue-corrected Kronecker-Factored Approximate Curvature; [George
    ç­‰äºº 2018](https://arxiv.org/abs/1806.03884)ï¼‰è¿‘ä¼¼ã€‚
- en: Prediction Changes during Training
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒè¿‡ç¨‹ä¸­çš„é¢„æµ‹å˜åŒ–
- en: 'Another branch of methods are to track the changes of model prediction during
    training to identify cases which seem hard to be learned. **Data Maps** ([Swayamdipta
    et al. 2020](https://arxiv.org/abs/2009.10795)) tracks two attributes of model
    behavior dynamics during training to analyze the quality of dataset:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç±»æ–¹æ³•æ˜¯è·Ÿè¸ªæ¨¡åž‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„é¢„æµ‹å˜åŒ–ï¼Œä»¥è¯†åˆ«ä¼¼ä¹Žéš¾ä»¥å­¦ä¹ çš„æƒ…å†µã€‚**æ•°æ®æ˜ å°„**ï¼ˆ[Swayamdiptaç­‰äººï¼Œ2020](https://arxiv.org/abs/2009.10795)ï¼‰è·Ÿè¸ªæ¨¡åž‹è¡Œä¸ºåŠ¨æ€çš„ä¸¤ä¸ªå±žæ€§ï¼Œä»¥åˆ†æžæ•°æ®é›†çš„è´¨é‡ï¼š
- en: '**Confidence**: The modelâ€™s confidence in the true label, defined as the mean
    model probability of the true label across epochs. They also used a coarse-grained
    metric, â€œcorrectnessâ€, defined as the fraction of times when the model predicts
    the correct label across epochs.'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ç½®ä¿¡åº¦**ï¼šæ¨¡åž‹å¯¹çœŸå®žæ ‡ç­¾çš„ç½®ä¿¡åº¦ï¼Œå®šä¹‰ä¸ºè·¨åŽ†å…ƒçš„çœŸå®žæ ‡ç­¾çš„å¹³å‡æ¨¡åž‹æ¦‚çŽ‡ã€‚ä»–ä»¬è¿˜ä½¿ç”¨äº†ä¸€ä¸ªç²—ç²’åº¦çš„åº¦é‡ï¼Œâ€œæ­£ç¡®æ€§â€ï¼Œå®šä¹‰ä¸ºæ¨¡åž‹åœ¨è·¨åŽ†å…ƒé¢„æµ‹æ­£ç¡®æ ‡ç­¾çš„æ¬¡æ•°çš„æ¯”ä¾‹ã€‚'
- en: '**Variability**: The variation of the confidence, defined as the standard deviation
    of model probability of the true label across epochs.'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å˜å¼‚æ€§**ï¼šç½®ä¿¡åº¦çš„å˜å¼‚æ€§ï¼Œå®šä¹‰ä¸ºè·¨åŽ†å…ƒçš„çœŸå®žæ ‡ç­¾çš„æ¨¡åž‹æ¦‚çŽ‡çš„æ ‡å‡†å·®ã€‚'
- en: '![](../Images/cb90212e4cc5430457ce335a9a786974.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb90212e4cc5430457ce335a9a786974.png)'
- en: 'Fig. 11\. Data map for SNLI training set, based on a RoBERTa classifier. (Image
    source: [Swayamdipta et al. 2020](https://arxiv.org/abs/2009.10795))'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾11\. åŸºäºŽRoBERTaåˆ†ç±»å™¨çš„SNLIè®­ç»ƒé›†æ•°æ®æ˜ å°„ã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼š[Swayamdiptaç­‰äººï¼Œ2020](https://arxiv.org/abs/2009.10795)ï¼‰
- en: Hard-to-learn (low confidence, low variability) samples are more likely to be
    mislabeled. They ran an experiment on WinoGrande dataset with 1% flipped label
    data. After retraining, flipped instances move to the lower confidence and slightly
    higher variability regions, indicating that the hard-to-learn regions contains
    mislabeled samples. Given this, we can train a classifier on equal numbers of
    label flipped and clean samples using only the confidence score (unsure why the
    paper didnâ€™t use both confidence and variability as features). This simple noise
    classifier then can be used on the original dataset to identify potentially mislabeled
    instances.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: éš¾ä»¥å­¦ä¹ ï¼ˆä½Žç½®ä¿¡åº¦ï¼Œä½Žå˜å¼‚æ€§ï¼‰çš„æ ·æœ¬æ›´æœ‰å¯èƒ½è¢«é”™è¯¯æ ‡è®°ã€‚ä»–ä»¬åœ¨WinoGrandeæ•°æ®é›†ä¸Šè¿›è¡Œäº†ä¸€ä¸ªå®žéªŒï¼Œå…¶ä¸­æœ‰1%çš„æ ‡ç­¾ç¿»è½¬æ•°æ®ã€‚é‡æ–°è®­ç»ƒåŽï¼Œç¿»è½¬çš„å®žä¾‹ç§»åŠ¨åˆ°ä½Žç½®ä¿¡åº¦å’Œç¨é«˜å˜å¼‚æ€§åŒºåŸŸï¼Œè¡¨æ˜Žéš¾ä»¥å­¦ä¹ çš„åŒºåŸŸåŒ…å«äº†é”™è¯¯æ ‡è®°çš„æ ·æœ¬ã€‚åŸºäºŽæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä»…ä½¿ç”¨ç½®ä¿¡åº¦åˆ†æ•°åœ¨ç›¸ç­‰æ•°é‡çš„æ ‡ç­¾ç¿»è½¬å’Œå¹²å‡€æ ·æœ¬ä¸Šè®­ç»ƒåˆ†ç±»å™¨ï¼ˆä¸ç¡®å®šä¸ºä»€ä¹ˆè®ºæ–‡æ²¡æœ‰åŒæ—¶ä½¿ç”¨ç½®ä¿¡åº¦å’Œå˜å¼‚æ€§ä½œä¸ºç‰¹å¾ï¼‰ã€‚ç„¶åŽï¼Œè¿™ä¸ªç®€å•çš„å™ªå£°åˆ†ç±»å™¨å¯ä»¥ç”¨äºŽåŽŸå§‹æ•°æ®é›†ï¼Œä»¥è¯†åˆ«æ½œåœ¨çš„é”™è¯¯æ ‡è®°å®žä¾‹ã€‚
- en: '![](../Images/1cf6ecfda87658c664b8bc87eb1db451.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1cf6ecfda87658c664b8bc87eb1db451.png)'
- en: 'Fig. 12\. Data points originally with high confidence and low variability scores
    moved to low confidence, slightly higher variability regions after labels get
    flipped. (Image source: [Swayamdipta et al. 2020](https://arxiv.org/abs/2009.10795))'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾12\. åŽŸæœ¬å…·æœ‰é«˜ç½®ä¿¡åº¦å’Œä½Žå˜å¼‚æ€§åˆ†æ•°çš„æ•°æ®ç‚¹åœ¨æ ‡ç­¾ç¿»è½¬åŽç§»åŠ¨åˆ°ä½Žç½®ä¿¡åº¦ã€ç¨é«˜å˜å¼‚æ€§åŒºåŸŸã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼š[Swayamdiptaç­‰äººï¼Œ2020](https://arxiv.org/abs/2009.10795)ï¼‰
- en: However, we should not consider all hard-to-learn samples to be incorrect. In
    fact, the paper hypothesizes that ambiguous (high variability) and hard-to-learn
    (low confidence, low variability) samples are more informative for learning. Experiments
    showed that they are good for OOD generalization, giving better results on OOD
    eval, even in comparison to 100% training set.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæˆ‘ä»¬ä¸åº”è¯¥è®¤ä¸ºæ‰€æœ‰éš¾ä»¥å­¦ä¹ çš„æ ·æœ¬éƒ½æ˜¯é”™è¯¯çš„ã€‚äº‹å®žä¸Šï¼Œè®ºæ–‡å‡è®¾æ¨¡æ£±ä¸¤å¯ï¼ˆé«˜å˜å¼‚æ€§ï¼‰å’Œéš¾ä»¥å­¦ä¹ ï¼ˆä½Žç½®ä¿¡åº¦ï¼Œä½Žå˜å¼‚æ€§ï¼‰çš„æ ·æœ¬å¯¹å­¦ä¹ æ›´å…·ä¿¡æ¯é‡ã€‚å®žéªŒè¯æ˜Žï¼Œå®ƒä»¬å¯¹OODæ³›åŒ–æ•ˆæžœå¾ˆå¥½ï¼Œåœ¨OODè¯„ä¼°ä¸­å–å¾—äº†æ›´å¥½çš„ç»“æžœï¼Œç”šè‡³æ¯”100%è®­ç»ƒé›†è¿˜è¦å¥½ã€‚
- en: 'To investigate whether neural networks have a tendency to **forget** previously
    learned information, [Mariya Toneva et al. (2019)](https://arxiv.org/abs/1812.05159)
    designed an experiment: They track the model prediction for each sample during
    the training process and count the transitions for each sample from being classified
    correctly to incorrectly or vice-versa. Then samples can be categorized accordingly,'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è°ƒæŸ¥ç¥žç»ç½‘ç»œæ˜¯å¦æœ‰é—å¿˜å…ˆå‰å­¦åˆ°ä¿¡æ¯çš„å€¾å‘ï¼Œ[Mariya Tonevaç­‰äººï¼ˆ2019ï¼‰](https://arxiv.org/abs/1812.05159)è®¾è®¡äº†ä¸€ä¸ªå®žéªŒï¼šä»–ä»¬è·Ÿè¸ªæ¨¡åž‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹ï¼Œå¹¶è®¡ç®—æ¯ä¸ªæ ·æœ¬ä»Žè¢«æ­£ç¡®åˆ†ç±»åˆ°é”™è¯¯åˆ†ç±»æˆ–åä¹‹çš„è½¬æ¢æ¬¡æ•°ã€‚ç„¶åŽå¯ä»¥ç›¸åº”åœ°å¯¹æ ·æœ¬è¿›è¡Œåˆ†ç±»ï¼Œ
- en: '*Forgettable* (redundant) samples: If the class label changes across training
    epochs.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*æ˜“å¿˜*ï¼ˆå†—ä½™ï¼‰æ ·æœ¬ï¼šå¦‚æžœç±»åˆ«æ ‡ç­¾åœ¨è®­ç»ƒåŽ†å…ƒä¸­å‘ç”Ÿå˜åŒ–ã€‚'
- en: '*Unforgettable* samples: If the class label assignment is consistent across
    training epochs. Those samples are never forgotten once learned.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*éš¾å¿˜*æ ·æœ¬ï¼šå¦‚æžœç±»åˆ«æ ‡ç­¾åˆ†é…åœ¨è®­ç»ƒåŽ†å…ƒä¸­ä¿æŒä¸€è‡´ã€‚è¿™äº›æ ·æœ¬ä¸€æ—¦å­¦ä¹ å°±æ°¸è¿œä¸ä¼šè¢«é—å¿˜ã€‚'
- en: They found that there are a large number of unforgettable examples that are
    never forgotten once learnt. Examples with noisy labels or images with â€œuncommonâ€
    features (visually complicated to classify) are among the most forgotten examples.
    The experiments empirically validated that unforgettable examples can be safely
    removed without compromising model performance.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–ä»¬å‘çŽ°æœ‰å¤§é‡ä¸€æ—¦å­¦ä¹ å°±ä¸ä¼šè¢«é—å¿˜çš„ä¸å¯å¿˜è®°çš„ç¤ºä¾‹ã€‚å…·æœ‰å˜ˆæ‚æ ‡ç­¾æˆ–å…·æœ‰â€œä¸å¯»å¸¸â€ç‰¹å¾çš„å›¾åƒï¼ˆåœ¨è§†è§‰ä¸Šéš¾ä»¥åˆ†ç±»ï¼‰æ˜¯æœ€å®¹æ˜“è¢«é—å¿˜çš„ç¤ºä¾‹ä¹‹ä¸€ã€‚å®žéªŒè¯æ˜Žï¼Œä¸å¯å¿˜è®°çš„ç¤ºä¾‹å¯ä»¥å®‰å…¨åœ°åˆ é™¤è€Œä¸å½±å“æ¨¡åž‹æ€§èƒ½ã€‚
- en: In the implementation, the forgetting event is only counted when a sample is
    included in the current training batch; that is, they compute forgetting across
    presentations of the same example in subsequent mini-batches. The number of forgetting
    events per sample is quite stable across different seeds and forgettable examples
    have a small tendency to be first-time learned later in the training. The forgetting
    events are also found to be transferable throughout the training period and between
    architectures.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®žçŽ°ä¸­ï¼Œé—å¿˜äº‹ä»¶ä»…åœ¨æ ·æœ¬åŒ…å«åœ¨å½“å‰è®­ç»ƒæ‰¹æ¬¡ä¸­æ—¶è®¡æ•°ï¼›ä¹Ÿå°±æ˜¯è¯´ï¼Œä»–ä»¬åœ¨åŽç»­å°æ‰¹æ¬¡çš„å‘ˆçŽ°ä¸­è®¡ç®—å¯¹åŒä¸€ç¤ºä¾‹çš„é—å¿˜ã€‚æ¯ä¸ªæ ·æœ¬çš„é—å¿˜äº‹ä»¶æ•°é‡åœ¨ä¸åŒç§å­ä¹‹é—´éžå¸¸ç¨³å®šï¼Œæ˜“å¿˜è®°çš„ç¤ºä¾‹æœ‰ä¸€å®šå€¾å‘åœ¨è®­ç»ƒåŽæœŸé¦–æ¬¡å­¦ä¹ ã€‚é—å¿˜äº‹ä»¶è¿˜è¢«å‘çŽ°åœ¨æ•´ä¸ªè®­ç»ƒæœŸé—´å’Œä¸åŒæž¶æž„ä¹‹é—´æ˜¯å¯ä¼ é€’çš„ã€‚
- en: '[Pleiss, et al. (2020)](https://arxiv.org/abs/2001.10528) developed a method
    named **AUM (Area under the Margin)** to spot wrong labels based on such an assumption:
    Say, a BIRD image is mistakenly marked as DOG. The gradient update would encourage
    generalization from other BIRD images to this BIRD image, while the DOG label
    provides an incorrect supervised signal to encourage the update to go another
    way. Hence, there exists tension between generalization and (wrong) prediction
    in gradient update signals.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[Pleissç­‰äººï¼ˆ2020ï¼‰](https://arxiv.org/abs/2001.10528)å¼€å‘äº†ä¸€ç§åä¸º**AUMï¼ˆè¾¹ç¼˜ä¸‹é¢ç§¯ï¼‰**çš„æ–¹æ³•ï¼ŒåŸºäºŽè¿™æ ·ä¸€ä¸ªå‡è®¾æ¥å‘çŽ°é”™è¯¯æ ‡ç­¾ï¼šæ¯”å¦‚ï¼Œä¸€å¼ é¸Ÿç±»å›¾åƒè¢«é”™è¯¯åœ°æ ‡è®°ä¸ºç‹—ç±»ã€‚æ¢¯åº¦æ›´æ–°ä¼šé¼“åŠ±ä»Žå…¶ä»–é¸Ÿç±»å›¾åƒåˆ°è¿™å¼ é¸Ÿç±»å›¾åƒçš„æ³›åŒ–ï¼Œè€Œç‹—ç±»æ ‡ç­¾æä¾›äº†ä¸€ä¸ªä¸æ­£ç¡®çš„ç›‘ç£ä¿¡å·ï¼Œé¼“åŠ±æ›´æ–°æœç€å¦ä¸€ç§æ–¹å¼è¿›è¡Œã€‚å› æ­¤ï¼Œåœ¨æ¢¯åº¦æ›´æ–°ä¿¡å·ä¸­å­˜åœ¨æ³›åŒ–å’Œï¼ˆé”™è¯¯ï¼‰é¢„æµ‹ä¹‹é—´çš„ç´§å¼ å…³ç³»ã€‚'
- en: 'Given a classification dataset $(\mathbf{x}, y) \in \mathcal{D}_\text{train}$,
    let $z^{(t)}_i(\mathbf{x}) \in \mathbb{R}$ be the logit corresponding to class
    $i$ at epoch $t$. The margin at epoch $t$ is the difference between the assigned
    logit and the next largest logit:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šä¸€ä¸ªåˆ†ç±»æ•°æ®é›†$(\mathbf{x}, y) \in \mathcal{D}_\text{train}$ï¼Œè®©$z^{(t)}_i(\mathbf{x})
    \in \mathbb{R}$è¡¨ç¤ºåœ¨ç¬¬$t$ä¸ªæ—¶æœŸå¯¹åº”äºŽç±»åˆ«$i$çš„logitã€‚åœ¨ç¬¬$t$ä¸ªæ—¶æœŸçš„è¾¹é™…æ˜¯åˆ†é…çš„logitä¸Žä¸‹ä¸€ä¸ªæœ€å¤§logitä¹‹é—´çš„å·®å¼‚ï¼š
- en: $$ M^{(t)}(\mathbf{x}, y) = z_y^{(t)}(\mathbf{x}) - \max_{i \neq y} z^{(t)}_i(\mathbf{x}),\quad
    \text{AUM}(\mathbf{x}, y) = \frac{1}{T} \sum^T_{t=1} M^{(t)}(\mathbf{x}, y) $$
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: $$ M^{(t)}(\mathbf{x}, y) = z_y^{(t)}(\mathbf{x}) - \max_{i \neq y} z^{(t)}_i(\mathbf{x}),\quad
    \text{AUM}(\mathbf{x}, y) = \frac{1}{T} \sum^T_{t=1} M^{(t)}(\mathbf{x}, y) $$
- en: A negative margin indicates a wrong prediction and a large positive margin suggests
    high confidence in a correct prediction. The hypothesis is that mislabeled samples
    would have a smaller margin than correct samples due to the tension of generalization
    via SGD triggered by other samples.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: è´Ÿè¾¹ç¼˜è¡¨ç¤ºé”™è¯¯é¢„æµ‹ï¼Œå¤§æ­£è¾¹ç¼˜è¡¨æ˜Žå¯¹æ­£ç¡®é¢„æµ‹çš„é«˜ç½®ä¿¡åº¦ã€‚å‡è®¾æ˜¯ï¼Œç”±äºŽSGDé€šè¿‡å…¶ä»–æ ·æœ¬è§¦å‘çš„æ³›åŒ–ç´§å¼ ï¼Œé”™è¯¯æ ‡è®°çš„æ ·æœ¬çš„è¾¹ç¼˜ä¼šæ¯”æ­£ç¡®æ ·æœ¬å°ã€‚
- en: 'In order to determine the threshold, they insert fake data, named â€œthreshold
    samplesâ€, to determine the threshold:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç¡®å®šé˜ˆå€¼ï¼Œä»–ä»¬æ’å…¥äº†åä¸ºâ€œé˜ˆå€¼æ ·æœ¬â€çš„è™šå‡æ•°æ®æ¥ç¡®å®šé˜ˆå€¼ï¼š
- en: Create a subset of threshold samples $\mathcal{D}_\text{thr}$. If there are
    $N$ training samples for $C$ classes, we randomly sample $N/(C+1)$ samples and
    switch all their labels to a fake new class $C+1$.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªé˜ˆå€¼æ ·æœ¬å­é›†$\mathcal{D}_\text{thr}$ã€‚å¦‚æžœæœ‰$N$ä¸ªè®­ç»ƒæ ·æœ¬ç”¨äºŽ$C$ä¸ªç±»åˆ«ï¼Œæˆ‘ä»¬éšæœºæŠ½å–$N/(C+1)$ä¸ªæ ·æœ¬ï¼Œå¹¶å°†å®ƒä»¬æ‰€æœ‰çš„æ ‡ç­¾æ›´æ”¹ä¸ºä¸€ä¸ªè™šå‡çš„æ–°ç±»åˆ«$C+1$ã€‚
- en: 'Merge threshold samples into the original dataset: $\mathcal{D}â€™ = { (\mathbf{x},
    C+1): \mathbf{x} \in \mathcal{D}_\text{thr}} \cup (\mathcal{D} \setminus\mathcal{D}_\text{thr})$;'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'å°†é˜ˆå€¼æ ·æœ¬åˆå¹¶åˆ°åŽŸå§‹æ•°æ®é›†ä¸­ï¼š$\mathcal{D}â€™ = { (\mathbf{x}, C+1): \mathbf{x} \in \mathcal{D}_\text{thr}}
    \cup (\mathcal{D} \setminus\mathcal{D}_\text{thr})$;'
- en: Train the model on $\mathcal{D}â€™$ and measure AUM of all the data;
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨$\mathcal{D}â€™$ä¸Šè®­ç»ƒæ¨¡åž‹å¹¶æµ‹é‡æ‰€æœ‰æ•°æ®çš„AUMï¼›
- en: Compute the threshold $\alpha$ as the 99th percentile of AUM of threshold samples;
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¡ç®—é˜ˆå€¼$\alpha$ä¸ºé˜ˆå€¼æ ·æœ¬AUMçš„ç¬¬99ç™¾åˆ†ä½æ•°ï¼›
- en: 'Identify mislabeled data using $\alpha$ a threshold: ${(\mathbf{x}, y) \in
    \mathcal{D} \setminus \mathcal{D}_\text{thr}: \text{AUM}_{\mathbf{x}, y} \leq
    \alpha}$'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'ä½¿ç”¨é˜ˆå€¼$\alpha$è¯†åˆ«é”™è¯¯æ ‡è®°çš„æ•°æ®ï¼š${(\mathbf{x}, y) \in \mathcal{D} \setminus \mathcal{D}_\text{thr}:
    \text{AUM}_{\mathbf{x}, y} \leq \alpha}$'
- en: '![](../Images/c0055053102a89c5634c93ce9b8e18ec.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c0055053102a89c5634c93ce9b8e18ec.png)'
- en: 'Fig. 13\. How the AUM of threshold samples help separate out mislabeled samples.
    (Image source: [Pleiss et al. 2020](https://arxiv.org/abs/2001.10528))'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 13\. é˜ˆå€¼æ ·æœ¬çš„AUMå¦‚ä½•å¸®åŠ©åˆ†ç¦»é”™è¯¯æ ‡è®°çš„æ ·æœ¬ã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼š[Pleiss et al. 2020](https://arxiv.org/abs/2001.10528)ï¼‰
- en: '![](../Images/deed79a2ecda1f81b539b1b363c25ed4.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/deed79a2ecda1f81b539b1b363c25ed4.png)'
- en: 'Fig. 14\. Test error on CIFAR 10/100 with randomly mislabeled samples, comparing
    different methods for data filter or noisy data training. (Image source: [Pleiss
    et al. 2020](https://arxiv.org/abs/2001.10528))'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 14\. åœ¨ CIFAR 10/100 ä¸Šæµ‹è¯•è¯¯æ ‡è®°æ ·æœ¬æ—¶çš„æµ‹è¯•é”™è¯¯ï¼Œæ¯”è¾ƒä¸åŒæ–¹æ³•çš„æ•°æ®è¿‡æ»¤æˆ–å˜ˆæ‚æ•°æ®è®­ç»ƒã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼š[Pleiss et al.
    2020](https://arxiv.org/abs/2001.10528)ï¼‰
- en: Noisy Cross-Validation
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å˜ˆæ‚äº¤å‰éªŒè¯
- en: The **NCV (Noisy Cross-Validation)** method ([Chen et al. 2019](https://arxiv.org/abs/1905.05040))
    divides the dataset into half at random, and then identifies data samples as â€œcleanâ€
    if its label matches the predicted label provided by the model that is only trained
    on the other half of the dataset. Clean samples are expected to be more trustworthy.
    INCV (Iterative Noisy Cross-Validation) runs NCV iteratively where more clean
    samples are added into the trusted candidate set $\mathcal{C}$ and more noisy
    samples are removed.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**NCVï¼ˆå˜ˆæ‚äº¤å‰éªŒè¯ï¼‰**æ–¹æ³•ï¼ˆ[Chen et al. 2019](https://arxiv.org/abs/1905.05040)ï¼‰å°†æ•°æ®é›†éšæœºåˆ†æˆä¸¤åŠï¼Œç„¶åŽå°†æ•°æ®æ ·æœ¬æ ‡è®°ä¸ºâ€œå¹²å‡€â€ï¼Œå¦‚æžœå…¶æ ‡ç­¾ä¸Žä»…åœ¨æ•°æ®é›†çš„å¦ä¸€åŠä¸Šè®­ç»ƒçš„æ¨¡åž‹æä¾›çš„é¢„æµ‹æ ‡ç­¾åŒ¹é…ã€‚å¹²å‡€æ ·æœ¬é¢„è®¡æ›´å¯ä¿¡ã€‚INCï¼ˆè¿­ä»£å˜ˆæ‚äº¤å‰éªŒè¯ï¼‰è¿­ä»£è¿è¡ŒNCVï¼Œå…¶ä¸­æ›´å¤šå¹²å‡€æ ·æœ¬è¢«æ·»åŠ åˆ°å—ä¿¡ä»»çš„å€™é€‰é›†$\mathcal{C}$ä¸­ï¼Œæ›´å¤šå˜ˆæ‚æ ·æœ¬è¢«ç§»é™¤ã€‚'
- en: '![](../Images/79311807a83a2aeec5efa64d4adf43fd.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/79311807a83a2aeec5efa64d4adf43fd.png)'
- en: 'Fig. 15\. Algorithm of INCV (iterative noisy cross-validation). (Image source:
    [Chen et al. 2019](https://arxiv.org/abs/1905.05040))'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 15\. INCVï¼ˆè¿­ä»£å˜ˆæ‚äº¤å‰éªŒè¯ï¼‰ç®—æ³•ã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼š[Chen et al. 2019](https://arxiv.org/abs/1905.05040)ï¼‰
- en: References
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] Francis Galton [â€œVox populiâ€](https://www.nature.com/articles/075450a0)
    Nature 75, 450-451 (1907).'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] å¼—æœ—è¥¿æ–¯Â·é«˜å°”é¡¿ [â€œæ°‘æ„â€](https://www.nature.com/articles/075450a0) Nature 75, 450-451ï¼ˆ1907å¹´ï¼‰ã€‚'
- en: '[2] Sambasivan et al. [â€œEveryone wants to do the model work, not the data workâ€:
    Data Cascades in High-Stakes AI"](https://dl.acm.org/doi/10.1145/3411764.3445518)
    CHI 2021'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Sambasivanç­‰äºº [â€œæ¯ä¸ªäººéƒ½æƒ³åšæ¨¡åž‹å·¥ä½œï¼Œè€Œä¸æ˜¯æ•°æ®å·¥ä½œâ€ï¼šé«˜é£Žé™©AIä¸­çš„æ•°æ®çº§è”](https://dl.acm.org/doi/10.1145/3411764.3445518)
    CHI 2021'
- en: '[3] Chris Callison-Burch. [â€œFast, Cheap, and Creative: Evaluating Translation
    Quality Using Amazonâ€™s Mechanical Turkâ€](https://aclanthology.org/D09-1030/) EMNLP
    2009'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Chris Callison-Burch. [â€œå¿«é€Ÿã€å»‰ä»·å’Œåˆ›é€ æ€§ï¼šä½¿ç”¨äºšé©¬é€Šçš„ Mechanical Turk è¯„ä¼°ç¿»è¯‘è´¨é‡â€](https://aclanthology.org/D09-1030/)
    EMNLP 2009'
- en: '[4] Rottger et al. [â€œTwo Contrasting Data Annotation Paradigms for Subjective
    NLP Tasksâ€](https://arxiv.org/abs/2112.07475) NAACL 2022.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Rottgerç­‰äºº [â€œä¸»è§‚NLPä»»åŠ¡çš„ä¸¤ç§å¯¹æ¯”æ•°æ®æ ‡æ³¨èŒƒå¼â€](https://arxiv.org/abs/2112.07475) NAACL
    2022ã€‚'
- en: '[5] Aroyo & Welty [â€œTruth Is a Lie: Crowd Truth and the Seven Myths of Human
    Annotationâ€](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2564)
    AI MagazineÂ 36.1: 15-24 (2015).'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Aroyo & Welty [â€œçœŸç›¸æ˜¯è°Žè¨€ï¼šä¼—åŒ…çœŸç›¸å’Œäººç±»æ ‡æ³¨çš„ä¸ƒå¤§ç¥žè¯â€](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2564)
    AI Magazine 36.1ï¼š15-24ï¼ˆ2015å¹´ï¼‰ã€‚'
- en: '[6] Hovy et al. [â€œLearning Whom to Trust with MACEâ€](https://aclanthology.org/N13-1132.pdf)
    NAACL-HLT 2013.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Hovyç­‰äºº [â€œä½¿ç”¨MACEå­¦ä¹ ä¿¡ä»»å¯¹è±¡â€](https://aclanthology.org/N13-1132.pdf) NAACL-HLT
    2013ã€‚'
- en: '[7] Wang et al. [â€œAll that Agrees Is Not Gold: Evaluating Ground Truth Labels
    and Dialogue Content for Safetyâ€](https://research.google/pubs/all-that-agrees-is-not-gold-evaluating-ground-truth-labels-and-dialogue-content-for-safety/)
    2023.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] çŽ‹ç­‰äºº [â€œä¸€åˆ‡ä¸€è‡´å¹¶éžé»„é‡‘ï¼šè¯„ä¼°å®‰å…¨çš„åœ°é¢çœŸç›¸æ ‡ç­¾å’Œå¯¹è¯å†…å®¹â€](https://research.google/pubs/all-that-agrees-is-not-gold-evaluating-ground-truth-labels-and-dialogue-content-for-safety/)
    2023å¹´ã€‚'
- en: '[8] Zhang et al. [â€œA Taxonomy of Rater Disagreements: Surveying Challenges
    & Opportunities from the Perspective of Annotating Online Toxicityâ€](https://arxiv.org/abs/2311.04345)
    arXiv preprint arXiv:2311.04345Â (2023).'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] å¼ ç­‰äºº [â€œè¯„åˆ†è€…åˆ†æ­§çš„åˆ†ç±»æ³•ï¼šä»Žåœ¨çº¿æ¯’æ€§æ³¨é‡Šçš„æ³¨é‡Šè§’åº¦è°ƒæŸ¥æŒ‘æˆ˜å’Œæœºä¼šâ€](https://arxiv.org/abs/2311.04345)
    arXiv é¢„å°æœ¬ arXiv:2311.04345ï¼ˆ2023å¹´ï¼‰ã€‚'
- en: '[9] Davani et al. [â€œDealing with disagreements: Looking beyond the majority
    vote in subjective annotationsâ€](https://arxiv.org/abs/2110.05719) ACL 2022.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Davaniç­‰äºº [â€œå¤„ç†åˆ†æ­§ï¼šè¶…è¶Šå¤šæ•°æŠ•ç¥¨åœ¨ä¸»è§‚æ³¨é‡Šä¸­â€](https://arxiv.org/abs/2110.05719) ACL 2022ã€‚'
- en: '[10] Gordon et al. [â€œJury Learning: Integrating Dissenting Voices into Machine
    Learning Modelsâ€](https://arxiv.org/abs/2202.02950) CHI 2022.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Gordonç­‰äºº [â€œé™ªå®¡å›¢å­¦ä¹ ï¼šå°†å¼‚è®®å£°éŸ³æ•´åˆåˆ°æœºå™¨å­¦ä¹ æ¨¡åž‹ä¸­â€](https://arxiv.org/abs/2202.02950) CHI
    2022ã€‚'
- en: '[11] Gordon et al. [â€œThe Disagreement Deconvolution: Bringing Machine Learning
    Performance Metrics In Line With Realityâ€](https://dl.acm.org/doi/abs/10.1145/3411764.3445423)
    CHI 2021'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Gordonç­‰äºº [â€œåˆ†æ­§è§£å·ç§¯ï¼šä½¿æœºå™¨å­¦ä¹ æ€§èƒ½æŒ‡æ ‡ç¬¦åˆçŽ°å®žâ€](https://dl.acm.org/doi/abs/10.1145/3411764.3445423)
    CHI 2021'
- en: '[12] Daniel et al. 2018 [â€œQuality Control in Crowdsourcing: A Survey of Quality
    Attributes, Assessment Techniques, and Assurance Actionsâ€](https://arxiv.org/abs/1801.02546)
    ACM Computing Surveys (CSUR), 51(1), 1-40 (2018).'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Danielç­‰äººã€‚2018å¹´[â€œä¼—åŒ…ä¸­çš„è´¨é‡æŽ§åˆ¶ï¼šè´¨é‡å±žæ€§ã€è¯„ä¼°æŠ€æœ¯å’Œä¿è¯æŽªæ–½è°ƒæŸ¥â€](https://arxiv.org/abs/1801.02546)
    ACMè®¡ç®—è°ƒæŸ¥ï¼ˆCSURï¼‰ï¼Œ51(1)ï¼Œ1-40ï¼ˆ2018å¹´ï¼‰ã€‚'
- en: '[13] Koh & Liang. [â€œUnderstanding Black-box Predictions via Influence Functionsâ€](https://arxiv.org/abs/1703.04730)
    ICML 2017.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Koh & Liangã€‚[â€œé€šè¿‡å½±å“å‡½æ•°ç†è§£é»‘ç›’é¢„æµ‹â€](https://arxiv.org/abs/1703.04730) ICML 2017ã€‚'
- en: '[14] Grosse et al. [â€œStudying Large Language Model Generalization with Influence
    Functionsâ€](https://arxiv.org/abs/2308.03296) arXiv preprint arXiv:2308.03296
    (2023).'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Grosseç­‰äººã€‚[â€œåˆ©ç”¨å½±å“å‡½æ•°ç ”ç©¶å¤§åž‹è¯­è¨€æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›â€](https://arxiv.org/abs/2308.03296) arXivé¢„å°æœ¬arXiv:2308.03296ï¼ˆ2023å¹´ï¼‰ã€‚'
- en: '[15] Swayamdipta et al. [â€œDataset Cartography: Mapping and Diagnosing Datasets
    with Training Dynamicsâ€](https://arxiv.org/abs/2009.10795) EMNLP 2020.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Swayamdiptaç­‰äººã€‚[â€œæ•°æ®é›†åˆ¶å›¾ï¼šé€šè¿‡è®­ç»ƒåŠ¨æ€ç»˜åˆ¶å’Œè¯Šæ–­æ•°æ®é›†â€](https://arxiv.org/abs/2009.10795)
    EMNLP 2020ã€‚'
- en: '[16] Toneva, et al. [â€œAn Empirical Study of Example Forgetting during Deep
    Neural Network Learningâ€](https://arxiv.org/abs/1812.05159) ICLR 2019.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Tonevaç­‰äººã€‚[â€œæ·±åº¦ç¥žç»ç½‘ç»œå­¦ä¹ è¿‡ç¨‹ä¸­ç¤ºä¾‹é—å¿˜çš„å®žè¯ç ”ç©¶â€](https://arxiv.org/abs/1812.05159) ICLR
    2019ã€‚'
- en: '[17] Pleiss, et al. [â€œIdentifying Mislabeled Data using the Area Under the
    Margin Rankingâ€](https://arxiv.org/abs/2001.10528) NeuriPS 2020.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Pleissç­‰äººã€‚[â€œåˆ©ç”¨è¾¹ç¼˜æŽ’åä¸‹é¢ç§¯è¯†åˆ«é”™è¯¯æ ‡è®°æ•°æ®â€](https://arxiv.org/abs/2001.10528) NeuriPS
    2020ã€‚'
- en: '[18] Chen et al. [â€œUnderstanding and utilizing deep neural networks trained
    with noisy labelsâ€](https://arxiv.org/abs/1905.05040) ICML 2019.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Chenç­‰äººã€‚[â€œç†è§£å’Œåˆ©ç”¨ä½¿ç”¨å«æœ‰å™ªå£°æ ‡ç­¾è®­ç»ƒçš„æ·±åº¦ç¥žç»ç½‘ç»œâ€](https://arxiv.org/abs/1905.05040) ICML
    2019ã€‚'
