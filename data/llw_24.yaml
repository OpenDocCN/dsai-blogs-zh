- en: Meta Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å…ƒå¼ºåŒ–å­¦ä¹ 
- en: åŸæ–‡ï¼š[https://lilianweng.github.io/posts/2019-06-23-meta-rl/](https://lilianweng.github.io/posts/2019-06-23-meta-rl/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://lilianweng.github.io/posts/2019-06-23-meta-rl/](https://lilianweng.github.io/posts/2019-06-23-meta-rl/)
- en: In my earlier post on [meta-learning](https://lilianweng.github.io/posts/2018-11-30-meta-learning/),
    the problem is mainly defined in the context of few-shot classification. Here
    I would like to explore more into cases when we try to â€œmeta-learnâ€ [Reinforcement
    Learning (RL)](https://lilianweng.github.io/posts/2018-02-19-rl-overview/) tasks
    by developing an agent that can solve unseen tasks fast and efficiently.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä¹‹å‰å…³äº[å…ƒå­¦ä¹ ](https://lilianweng.github.io/posts/2018-11-30-meta-learning/)çš„å¸–å­ä¸­ï¼Œé—®é¢˜ä¸»è¦æ˜¯åœ¨å°‘æ ·æœ¬åˆ†ç±»çš„èƒŒæ™¯ä¸‹å®šä¹‰çš„ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘æƒ³æ›´æ·±å…¥æ¢è®¨å½“æˆ‘ä»¬å°è¯•é€šè¿‡å¼€å‘ä¸€ä¸ªå¯ä»¥å¿«é€Ÿé«˜æ•ˆè§£å†³æœªè§ä»»åŠ¡çš„ä»£ç†æ¥â€œå…ƒå­¦ä¹ â€[å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰](https://lilianweng.github.io/posts/2018-02-19-rl-overview/)ä»»åŠ¡çš„æƒ…å†µã€‚
- en: To recap, a good meta-learning model is expected to generalize to new tasks
    or new environments that have never been encountered during training. The adaptation
    process, essentially a *mini learning session*, happens at test with limited exposure
    to the new configurations. Even without any explicit fine-tuning (no gradient
    backpropagation on trainable variables), the meta-learning model autonomously
    adjusts internal hidden states to learn.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“ä¸€ä¸‹ï¼Œä¸€ä¸ªå¥½çš„å…ƒå­¦ä¹ æ¨¡å‹åº”è¯¥èƒ½å¤Ÿæ¨å¹¿åˆ°åœ¨è®­ç»ƒæœŸé—´ä»æœªé‡åˆ°è¿‡çš„æ–°ä»»åŠ¡æˆ–æ–°ç¯å¢ƒã€‚é€‚åº”è¿‡ç¨‹ï¼Œæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ª*å°å‹å­¦ä¹ ä¼šè¯*ï¼Œåœ¨æµ‹è¯•ä¸­å¯¹æ–°é…ç½®è¿›è¡Œæœ‰é™æš´éœ²ã€‚å³ä½¿æ²¡æœ‰ä»»ä½•æ˜ç¡®çš„å¾®è°ƒï¼ˆä¸å¯¹å¯è®­ç»ƒå˜é‡è¿›è¡Œæ¢¯åº¦åå‘ä¼ æ’­ï¼‰ï¼Œå…ƒå­¦ä¹ æ¨¡å‹ä¹Ÿä¼šè‡ªä¸»è°ƒæ•´å†…éƒ¨éšè—çŠ¶æ€ä»¥å­¦ä¹ ã€‚
- en: Training RL algorithms can be notoriously difficult sometimes. If the meta-learning
    agent could become so smart that the distribution of solvable unseen tasks grows
    extremely broad, we are on track towards [general purpose methods](http://incompleteideas.net/IncIdeas/BitterLesson.html)
    â€” essentially building a â€œbrainâ€ which would solve all kinds of RL problems without
    much human interference or manual feature engineering. Sounds amazing, right?
    ğŸ’–
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå¼ºåŒ–å­¦ä¹ ç®—æ³•æœ‰æ—¶å¯èƒ½éå¸¸å›°éš¾ã€‚å¦‚æœå…ƒå­¦ä¹ ä»£ç†èƒ½å¤Ÿå˜å¾—å¦‚æ­¤èªæ˜ï¼Œä»¥è‡³äºå¯è§£å†³çš„æœªè§ä»»åŠ¡çš„åˆ†å¸ƒå˜å¾—æå…¶å¹¿æ³›ï¼Œæˆ‘ä»¬å°±æœç€[é€šç”¨æ–¹æ³•](http://incompleteideas.net/IncIdeas/BitterLesson.html)çš„æ–¹å‘è¿ˆè¿›äº†â€”â€”åŸºæœ¬ä¸Šæ„å»ºä¸€ä¸ªå¯ä»¥è§£å†³å„ç§å¼ºåŒ–å­¦ä¹ é—®é¢˜çš„â€œå¤§è„‘â€ï¼Œå‡ ä¹ä¸éœ€è¦äººç±»å¹²é¢„æˆ–æ‰‹åŠ¨ç‰¹å¾å·¥ç¨‹ã€‚å¬èµ·æ¥å¾ˆç¥å¥‡ï¼Œå¯¹å§ï¼ŸğŸ’–
- en: On the Origin of Meta-RL
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å…³äºå…ƒå¼ºåŒ–å­¦ä¹ çš„èµ·æº
- en: Back in 2001
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å›åˆ°2001å¹´
- en: I encountered a paper written in 2001 by [Hochreiter et al.](http://snowedin.net/tmp/Hochreiter2001.pdf)
    when reading [Wang et al., 2016](https://arxiv.org/pdf/1611.05763.pdf). Although
    the idea was proposed for supervised learning, there are so many resemblances
    to the current approach to meta-RL.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å½“é˜…è¯»[Wangç­‰äººï¼Œ2016å¹´](https://arxiv.org/pdf/1611.05763.pdf)æ—¶ï¼Œæˆ‘é‡åˆ°äº†[Hochreiterç­‰äººï¼Œ2001å¹´](http://snowedin.net/tmp/Hochreiter2001.pdf)æ’°å†™çš„ä¸€ç¯‡è®ºæ–‡ã€‚å°½ç®¡è¿™ä¸ªæƒ³æ³•æ˜¯ä¸ºç›‘ç£å­¦ä¹ æå‡ºçš„ï¼Œä½†ä¸å½“å‰å…ƒå¼ºåŒ–å­¦ä¹ æ–¹æ³•æœ‰å¾ˆå¤šç›¸ä¼¼ä¹‹å¤„ã€‚
- en: '![](../Images/2463abee23d720181366b8c89906e5e7.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2463abee23d720181366b8c89906e5e7.png)'
- en: 'Fig. 1\. The meta-learning system consists of the supervisory and the subordinate
    systems. The subordinate system is a recurrent neural network that takes as input
    both the observation at the current time step, $x\_t$ and the label at the last
    time step, $y\_{t-1}$. (Image source: [Hochreiter et al., 2001](http://snowedin.net/tmp/Hochreiter2001.pdf))'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1. å…ƒå­¦ä¹ ç³»ç»Ÿç”±ç›‘ç£ç³»ç»Ÿå’Œä»å±ç³»ç»Ÿç»„æˆã€‚ä»å±ç³»ç»Ÿæ˜¯ä¸€ä¸ªå¾ªç¯ç¥ç»ç½‘ç»œï¼Œå®ƒä»¥å½“å‰æ—¶é—´æ­¥çš„è§‚å¯Ÿ$x\_t$å’Œä¸Šä¸€ä¸ªæ—¶é—´æ­¥çš„æ ‡ç­¾$y\_{t-1}$ä½œä¸ºè¾“å…¥ã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼š[Hochreiterç­‰äººï¼Œ2001å¹´](http://snowedin.net/tmp/Hochreiter2001.pdf)ï¼‰
- en: Hochreiterâ€™s meta-learning model is a recurrent network with LSTM cell. LSTM
    is a good choice because it can internalize a history of inputs and tune its own
    weights effectively through [BPTT](https://en.wikipedia.org/wiki/Backpropagation_through_time).
    The training data contains $K$ sequences and each sequence is consist of $N$ samples
    generated by a target function $f_k(.), k=1, \dots, K$,
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Hochreiterçš„å…ƒå­¦ä¹ æ¨¡å‹æ˜¯ä¸€ä¸ªå¸¦æœ‰LSTMå•å…ƒçš„å¾ªç¯ç½‘ç»œã€‚ LSTM æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ï¼Œå› ä¸ºå®ƒå¯ä»¥å†…éƒ¨åŒ–è¾“å…¥çš„å†å²å¹¶é€šè¿‡[BPTT](https://en.wikipedia.org/wiki/Backpropagation_through_time)æœ‰æ•ˆåœ°è°ƒæ•´è‡ªå·±çš„æƒé‡ã€‚è®­ç»ƒæ•°æ®åŒ…å«$K$ä¸ªåºåˆ—ï¼Œæ¯ä¸ªåºåˆ—ç”±ç›®æ ‡å‡½æ•°$f_k(.)$ç”Ÿæˆçš„$N$ä¸ªæ ·æœ¬ç»„æˆï¼Œå…¶ä¸­$k=1,
    \dots, K$ï¼Œ
- en: '$$ \{\text{input: }(\mathbf{x}^k_i, \mathbf{y}^k_{i-1}) \to \text{label: }\mathbf{y}^k_i\}_{i=1}^N
    \text{ where }\mathbf{y}^k_i = f_k(\mathbf{x}^k_i) $$'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '$$ \{\text{è¾“å…¥: }(\mathbf{x}^k_i, \mathbf{y}^k_{i-1}) \to \text{æ ‡ç­¾: }\mathbf{y}^k_i\}_{i=1}^N
    \text{ å…¶ä¸­ }\mathbf{y}^k_i = f_k(\mathbf{x}^k_i) $$'
- en: Noted that *the last label* $\mathbf{y}^k_{i-1}$ is also provided as an auxiliary
    input so that the function can learn the presented mapping.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„*ä¸Šä¸€ä¸ªæ ‡ç­¾* $\mathbf{y}^k_{i-1}$ ä¹Ÿä½œä¸ºè¾…åŠ©è¾“å…¥æä¾›ï¼Œä»¥ä¾¿å‡½æ•°å¯ä»¥å­¦ä¹ æ‰€å‘ˆç°çš„æ˜ å°„ã€‚
- en: In the experiment of decoding two-dimensional quadratic functions, $a x_1^2
    + b x_2^2 + c x_1 x_2 + d x_1 + e x_2 + f$, with coefficients $a$-$f$ are randomly
    sampled from [-1, 1], this meta-learning system was able to approximate the function
    after seeing only ~35 examples.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è§£ç äºŒç»´äºŒæ¬¡å‡½æ•°$a x_1^2 + b x_2^2 + c x_1 x_2 + d x_1 + e x_2 + f$çš„å®éªŒä¸­ï¼Œç³»æ•°$a$-$f$ä»[-1,
    1]ä¸­éšæœºæŠ½æ ·ï¼Œè¿™ä¸ªå…ƒå­¦ä¹ ç³»ç»Ÿåœ¨ä»…çœ‹åˆ°çº¦35ä¸ªç¤ºä¾‹åå°±èƒ½è¿‘ä¼¼è¡¨ç¤ºå‡½æ•°ã€‚
- en: Proposal in 2016
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2016å¹´çš„æè®®
- en: In the modern days of DL, [Wang et al.](https://arxiv.org/abs/1611.05763) (2016)
    and [Duan et al.](https://arxiv.org/abs/1611.02779) (2017) simultaneously proposed
    the very similar idea of **Meta-RL** (it is called **RL^2** in the second paper).
    A meta-RL model is trained over a distribution of MDPs, and at test time, it is
    able to learn to solve a new task quickly. The goal of meta-RL is ambitious, taking
    one step further towards general algorithms.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ·±åº¦å­¦ä¹ çš„ç°ä»£æ—¶ä»£ï¼Œ[Wangç­‰äºº](https://arxiv.org/abs/1611.05763)ï¼ˆ2016å¹´ï¼‰å’Œ[Duanç­‰äºº](https://arxiv.org/abs/1611.02779)ï¼ˆ2017å¹´ï¼‰åŒæ—¶æå‡ºäº†éå¸¸ç›¸ä¼¼çš„**å…ƒå¼ºåŒ–å­¦ä¹ **çš„æƒ³æ³•ï¼ˆåœ¨ç¬¬äºŒç¯‡è®ºæ–‡ä¸­ç§°ä¸º**RL^2**ï¼‰ã€‚å…ƒå¼ºåŒ–å­¦ä¹ æ¨¡å‹åœ¨ä¸€ç³»åˆ—MDPä¸Šè¿›è¡Œè®­ç»ƒï¼Œåœ¨æµ‹è¯•æ—¶ï¼Œèƒ½å¤Ÿå¿«é€Ÿå­¦ä¹ è§£å†³æ–°ä»»åŠ¡ã€‚å…ƒå¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡æ˜¯é›„å¿ƒå‹ƒå‹ƒçš„ï¼Œè¿ˆå‡ºäº†é€šå‘é€šç”¨ç®—æ³•çš„ä¸€æ­¥ã€‚
- en: Define Meta-RL
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®šä¹‰å…ƒå¼ºåŒ–å­¦ä¹ 
- en: '*Meta Reinforcement Learning*, in short, is to do [meta-learning](https://lilianweng.github.io/posts/2018-11-30-meta-learning/)
    in the field of [reinforcement learning](https://lilianweng.github.io/posts/2018-02-19-rl-overview/).
    Usually the train and test tasks are different but drawn from the same family
    of problems; i.e., experiments in the papers included multi-armed bandit with
    different reward probabilities, mazes with different layouts, same robots but
    with different physical parameters in simulator, and many others.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*å…ƒå¼ºåŒ–å­¦ä¹ *ï¼Œç®€è€Œè¨€ä¹‹ï¼Œæ˜¯åœ¨[å¼ºåŒ–å­¦ä¹ ](https://lilianweng.github.io/posts/2018-02-19-rl-overview/)é¢†åŸŸè¿›è¡Œ[å…ƒå­¦ä¹ ](https://lilianweng.github.io/posts/2018-11-30-meta-learning/)ã€‚é€šå¸¸è®­ç»ƒå’Œæµ‹è¯•ä»»åŠ¡ä¸åŒï¼Œä½†æ¥è‡ªåŒä¸€ç±»é—®é¢˜æ—ï¼›å³ï¼Œè®ºæ–‡ä¸­çš„å®éªŒåŒ…æ‹¬å…·æœ‰ä¸åŒå¥–åŠ±æ¦‚ç‡çš„å¤šè‡‚èµŒåšæœºï¼Œå…·æœ‰ä¸åŒå¸ƒå±€çš„è¿·å®«ï¼Œæ¨¡æ‹Ÿå™¨ä¸­å…·æœ‰ä¸åŒç‰©ç†å‚æ•°çš„ç›¸åŒæœºå™¨äººï¼Œä»¥åŠè®¸å¤šå…¶ä»–å®éªŒã€‚'
- en: Formulation
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…¬å¼åŒ–
- en: 'Letâ€™s say we have a distribution of tasks, each formularized as an [MDP](https://lilianweng.github.io/posts/2018-02-19-rl-overview/#markov-decision-processes)
    (Markov Decision Process), $M_i \in \mathcal{M}$. An MDP is determined by a 4-tuple,
    $M_i= \langle \mathcal{S}, \mathcal{A}, P_i, R_i \rangle$:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬æœ‰ä¸€ç³»åˆ—ä»»åŠ¡çš„åˆ†å¸ƒï¼Œæ¯ä¸ªä»»åŠ¡éƒ½è¢«å½¢å¼åŒ–ä¸ºä¸€ä¸ª[MDP](https://lilianweng.github.io/posts/2018-02-19-rl-overview/#markov-decision-processes)ï¼ˆé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼‰ï¼Œ$M_i
    \in \mathcal{M}$ã€‚ä¸€ä¸ªMDPç”±ä¸€ä¸ª4å…ƒç»„ç¡®å®šï¼Œ$M_i= \langle \mathcal{S}, \mathcal{A}, P_i, R_i
    \rangle$ï¼š
- en: '| Symbol | Meaning |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| ç¬¦å· | æ„ä¹‰ |'
- en: '| --- | --- |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| $\mathcal{S}$ | A set of states. |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{S}$ | ä¸€ç»„çŠ¶æ€ã€‚ |'
- en: '| $\mathcal{A}$ | A set of actions. |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{A}$ | ä¸€ç»„åŠ¨ä½œã€‚ |'
- en: '| $P_i: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \to \mathbb{R}_{+}$
    | Transition probability function. |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| $P_i: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \to \mathbb{R}_{+}$
    | è½¬ç§»æ¦‚ç‡å‡½æ•°ã€‚ |'
- en: '| $R_i: \mathcal{S} \times \mathcal{A} \to \mathbb{R}$ | Reward function. |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| $R_i: \mathcal{S} \times \mathcal{A} \to \mathbb{R}$ | å¥–åŠ±å‡½æ•°ã€‚ |'
- en: (RL^2 paper adds an extra parameter, horizon $T$, into the MDP tuple to emphasize
    that each MDP should have a finite horizon.)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ï¼ˆRL^2è®ºæ–‡åœ¨MDPå…ƒç»„ä¸­æ·»åŠ äº†ä¸€ä¸ªé¢å¤–çš„å‚æ•°ï¼Œhorizon $T$ï¼Œä»¥å¼ºè°ƒæ¯ä¸ªMDPåº”å…·æœ‰æœ‰é™çš„horizonã€‚ï¼‰
- en: 'Note that common state $\mathcal{S}$ and action space $\mathcal{A}$ are used
    above, so that a (stochastic) policy: $\pi_\theta: \mathcal{S} \times \mathcal{A}
    \to \mathbb{R}_{+}$ would get inputs compatible across different tasks. The test
    tasks are sampled from the same distribution $\mathcal{M}$ or slightly modified
    version.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¯·æ³¨æ„ä¸Šè¿°ä½¿ç”¨äº†å¸¸è§çŠ¶æ€$\mathcal{S}$å’ŒåŠ¨ä½œç©ºé—´$\mathcal{A}$ï¼Œä»¥ä¾¿ä¸€ä¸ªï¼ˆéšæœºçš„ï¼‰ç­–ç•¥ï¼š$\pi_\theta: \mathcal{S}
    \times \mathcal{A} \to \mathbb{R}_{+}$èƒ½å¤Ÿè·å¾—è·¨ä¸åŒä»»åŠ¡å…¼å®¹çš„è¾“å…¥ã€‚æµ‹è¯•ä»»åŠ¡ä»ç›¸åŒåˆ†å¸ƒ$\mathcal{M}$æˆ–ç¨ä½œä¿®æ”¹ä¸­æŠ½æ ·ã€‚'
- en: '![](../Images/871d0aec0b4fad03559e589b55192887.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/871d0aec0b4fad03559e589b55192887.png)'
- en: 'Fig. 2\. Illustration of meta-RL, containing two optimization loops. The outer
    loop samples a new environment in every iteration and adjusts parameters that
    determine the agent''s behavior. In the inner loop, the agent interacts with the
    environment and optimizes for the maximal reward. (Image source: [Botvinick, et
    al. 2019](https://www.cell.com/action/showPdf?pii=S1364-6613%2819%2930061-0))'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2. å…ƒå¼ºåŒ–å­¦ä¹ çš„ç¤ºæ„å›¾ï¼ŒåŒ…å«ä¸¤ä¸ªä¼˜åŒ–å¾ªç¯ã€‚å¤–éƒ¨å¾ªç¯åœ¨æ¯æ¬¡è¿­ä»£ä¸­å¯¹æ–°ç¯å¢ƒè¿›è¡Œé‡‡æ ·ï¼Œå¹¶è°ƒæ•´ç¡®å®šä»£ç†è¡Œä¸ºçš„å‚æ•°ã€‚åœ¨å†…éƒ¨å¾ªç¯ä¸­ï¼Œä»£ç†ä¸ç¯å¢ƒäº¤äº’å¹¶ä¼˜åŒ–ä»¥è·å¾—æœ€å¤§å¥–åŠ±ã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼š[Botvinickç­‰äººï¼Œ2019](https://www.cell.com/action/showPdf?pii=S1364-6613%2819%2930061-0)ï¼‰
- en: Main Differences from RL
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸å¼ºåŒ–å­¦ä¹ çš„ä¸»è¦åŒºåˆ«
- en: The overall configure of meta-RL is very similar to an ordinary RL algorithm,
    except that **the last reward** $r_{t-1}$ and **the last action** $a_{t-1}$ are
    also incorporated into the policy observation in addition to the current state
    $s_t$.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å…ƒå¼ºåŒ–å­¦ä¹ çš„æ•´ä½“é…ç½®ä¸æ™®é€šçš„RLç®—æ³•éå¸¸ç›¸ä¼¼ï¼Œåªæ˜¯**ä¸Šä¸€ä¸ªå¥–åŠ±** $r_{t-1}$ å’Œ**ä¸Šä¸€ä¸ªåŠ¨ä½œ** $a_{t-1}$ ä¹Ÿè¢«çº³å…¥åˆ°ç­–ç•¥è§‚å¯Ÿä¸­ï¼Œé™¤äº†å½“å‰çŠ¶æ€
    $s_t$ã€‚
- en: 'In RL: $\pi_\theta(s_t) \to$ a distribution over $\mathcal{A}$'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨RLä¸­ï¼š$\pi_\theta(s_t) \to$ ä¸€ä¸ªå…³äº $\mathcal{A}$ çš„åˆ†å¸ƒ
- en: 'In meta-RL: $\pi_\theta(a_{t-1}, r_{t-1}, s_t) \to$ a distribution over $\mathcal{A}$'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨å…ƒå¼ºåŒ–å­¦ä¹ ä¸­ï¼š$\pi_\theta(a_{t-1}, r_{t-1}, s_t) \to$ ä¸€ä¸ªå…³äº $\mathcal{A}$ çš„åˆ†å¸ƒ
- en: The intention of this design is to feed a history into the model so that the
    policy can internalize the dynamics between states, rewards, and actions in the
    current MDP and adjust its strategy accordingly. This is well aligned with the
    setup in [Hochreiterâ€™s system](#back-in-2001). Both meta-RL and RL^2 implemented
    an LSTM policy and the LSTMâ€™s hidden states serve as a *memory* for tracking characteristics
    of the trajectories. Because the policy is recurrent, there is no need to feed
    the last state as inputs explicitly.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§è®¾è®¡çš„æ„å›¾æ˜¯å°†å†å²è®°å½•è¾“å…¥æ¨¡å‹ï¼Œä»¥ä¾¿ç­–ç•¥å¯ä»¥å†…åŒ–å½“å‰MDPä¸­çŠ¶æ€ã€å¥–åŠ±å’ŒåŠ¨ä½œä¹‹é—´çš„åŠ¨æ€ï¼Œå¹¶ç›¸åº”åœ°è°ƒæ•´å…¶ç­–ç•¥ã€‚è¿™ä¸[Hochreiterçš„ç³»ç»Ÿ](#back-in-2001)ä¸­çš„è®¾ç½®éå¸¸ä¸€è‡´ã€‚å…ƒå¼ºåŒ–å­¦ä¹ å’ŒRL^2éƒ½å®ç°äº†ä¸€ä¸ªLSTMç­–ç•¥ï¼ŒLSTMçš„éšè—çŠ¶æ€ä½œä¸ºè·Ÿè¸ªè½¨è¿¹ç‰¹å¾çš„*è®°å¿†*ã€‚ç”±äºç­–ç•¥æ˜¯å¾ªç¯çš„ï¼Œå› æ­¤ä¸éœ€è¦æ˜ç¡®åœ°å°†ä¸Šä¸€ä¸ªçŠ¶æ€ä½œä¸ºè¾“å…¥é¦ˆé€ã€‚
- en: 'The training procedure works as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒè¿‡ç¨‹å¦‚ä¸‹ï¼š
- en: Sample a new MDP, $M_i \sim \mathcal{M}$;
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä» $\mathcal{M}$ ä¸­æŠ½å–ä¸€ä¸ªæ–°çš„MDPï¼Œ$M_i \sim \mathcal{M}$ï¼›
- en: '**Reset the hidden state** of the model;'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**é‡ç½®æ¨¡å‹çš„éšè—çŠ¶æ€**ï¼›'
- en: Collect multiple trajectories and update the model weights;
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ”¶é›†å¤šæ¡è½¨è¿¹å¹¶æ›´æ–°æ¨¡å‹æƒé‡ï¼›
- en: Repeat from step 1.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é‡å¤ä»æ­¥éª¤1å¼€å§‹ã€‚
- en: '![](../Images/5f38a4ba80cb585699e5ad68e00ecdf1.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f38a4ba80cb585699e5ad68e00ecdf1.png)'
- en: 'Fig. 3\. In the meta-RL paper, different actor-critic architectures all use
    a recurrent model. Last reward and last action are additional inputs. The observation
    is fed into the LSTM either as a one-hot vector or as an embedding vector after
    passed through an encoder model. (Image source: [Wang et al., 2016](https://arxiv.org/abs/1611.05763))'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3ã€‚åœ¨å…ƒå¼ºåŒ–å­¦ä¹ è®ºæ–‡ä¸­ï¼Œä¸åŒçš„æ¼”å‘˜-è¯„è®ºå®¶æ¶æ„éƒ½ä½¿ç”¨äº†ä¸€ä¸ªå¾ªç¯æ¨¡å‹ã€‚æœ€åçš„å¥–åŠ±å’Œæœ€åçš„åŠ¨ä½œæ˜¯é¢å¤–çš„è¾“å…¥ã€‚è§‚å¯Ÿç»“æœè¢«é¦ˆé€åˆ°LSTMä¸­ï¼Œå¯ä»¥æ˜¯ä¸€ä¸ªç‹¬çƒ­å‘é‡ï¼Œä¹Ÿå¯ä»¥æ˜¯é€šè¿‡ç¼–ç å™¨æ¨¡å‹ä¼ é€’åçš„åµŒå…¥å‘é‡ã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼š[Wangç­‰äººï¼Œ2016](https://arxiv.org/abs/1611.05763)ï¼‰
- en: '![](../Images/d1cd1a4af792c0d958a3b02001f881e7.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1cd1a4af792c0d958a3b02001f881e7.png)'
- en: 'Fig. 4\. As described in the RL^2 paper, illustration of the procedure of the
    model interacting with a series of MDPs in training time . (Image source: [Duan
    et al., 2017](https://arxiv.org/abs/1611.02779))'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4ã€‚å¦‚RL^2è®ºæ–‡ä¸­æ‰€è¿°ï¼Œå±•ç¤ºäº†æ¨¡å‹åœ¨è®­ç»ƒæ—¶ä¸ä¸€ç³»åˆ—MDPäº¤äº’çš„è¿‡ç¨‹ã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼š[Duanç­‰äººï¼Œ2017](https://arxiv.org/abs/1611.02779)ï¼‰
- en: Key Components
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…³é”®ç»„ä»¶
- en: 'There are three key components in Meta-RL:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å…ƒå¼ºåŒ–å­¦ä¹ ä¸­æœ‰ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼š
- en: â­ **A Model with Memory**
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â­ **å¸¦æœ‰è®°å¿†çš„æ¨¡å‹**
- en: A recurrent neural network maintains a hidden state. Thus, it could acquire
    and memorize the knowledge about the current task by updating the hidden state
    during rollouts. Without memory, meta-RL would not work.
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå¾ªç¯ç¥ç»ç½‘ç»œç»´æŠ¤ä¸€ä¸ªéšè—çŠ¶æ€ã€‚å› æ­¤ï¼Œå®ƒå¯ä»¥é€šè¿‡åœ¨rolloutsæœŸé—´æ›´æ–°éšè—çŠ¶æ€æ¥è·å–å’Œè®°å¿†å…³äºå½“å‰ä»»åŠ¡çš„çŸ¥è¯†ã€‚æ²¡æœ‰è®°å¿†ï¼Œå…ƒå¼ºåŒ–å­¦ä¹ å°†æ— æ³•è¿è¡Œã€‚
- en: â­ **Meta-learning Algorithm**
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â­ **å…ƒå­¦ä¹ ç®—æ³•**
- en: A meta-learning algorithm refers to how we can update the model weights to optimize
    for the purpose of solving an unseen task fast at test time. In both Meta-RL and
    RL^2 papers, the meta-learning algorithm is the ordinary gradient descent update
    of LSTM with hidden state reset between a switch of MDPs.
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å…ƒå­¦ä¹ ç®—æ³•æŒ‡çš„æ˜¯æˆ‘ä»¬å¦‚ä½•æ›´æ–°æ¨¡å‹æƒé‡ï¼Œä»¥ä¾¿åœ¨æµ‹è¯•æ—¶å¿«é€Ÿè§£å†³æœªè§è¿‡çš„ä»»åŠ¡ã€‚åœ¨å…ƒå¼ºåŒ–å­¦ä¹ å’ŒRL^2è®ºæ–‡ä¸­ï¼Œå…ƒå­¦ä¹ ç®—æ³•æ˜¯åœ¨MDPåˆ‡æ¢æ—¶é‡ç½®éšè—çŠ¶æ€çš„æ™®é€šæ¢¯åº¦ä¸‹é™æ›´æ–°LSTMã€‚
- en: â­ **A Distribution of MDPs**
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â­ **MDPçš„åˆ†å¸ƒ**
- en: While the agent is exposed to a variety of environments and tasks during training,
    it has to learn how to adapt to different MDPs.
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å½“ä»£ç†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æš´éœ²äºå„ç§ç¯å¢ƒå’Œä»»åŠ¡æ—¶ï¼Œå®ƒå¿…é¡»å­¦ä¼šå¦‚ä½•é€‚åº”ä¸åŒçš„MDPã€‚
- en: According to [Botvinick et al.](https://www.cell.com/action/showPdf?pii=S1364-6613%2819%2930061-0)
    (2019), one source of slowness in RL training is *weak [inductive bias](https://en.wikipedia.org/wiki/Inductive_bias)*
    ( = â€œa set of assumptions that the learner uses to predict outputs given inputs
    that it has not encounteredâ€). As a general ML rule, a learning algorithm with
    weak inductive bias will be able to master a wider range of variance, but usually,
    will be less sample-efficient. Therefore, to narrow down the hypotheses with stronger
    inductive biases help improve the learning speed.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®[Botvinickç­‰äºº](https://www.cell.com/action/showPdf?pii=S1364-6613%2819%2930061-0)ï¼ˆ2019ï¼‰çš„è¯´æ³•ï¼ŒRLè®­ç»ƒä¸­çš„ä¸€ç§ç¼“æ…¢æºäº*å¼±[å½’çº³åå·®](https://en.wikipedia.org/wiki/Inductive_bias)*ï¼ˆ=â€œå­¦ä¹ è€…ç”¨æ¥é¢„æµ‹ç»™å®šæœªé‡åˆ°çš„è¾“å…¥æ—¶çš„è¾“å‡ºçš„ä¸€ç»„å‡è®¾â€ï¼‰ã€‚ä½œä¸ºä¸€èˆ¬çš„MLè§„åˆ™ï¼Œå…·æœ‰å¼±å½’çº³åå·®çš„å­¦ä¹ ç®—æ³•å°†èƒ½å¤ŸæŒæ¡æ›´å¹¿æ³›çš„å˜åŒ–èŒƒå›´ï¼Œä½†é€šå¸¸ä¼šæ›´å°‘åœ°åˆ©ç”¨æ ·æœ¬ã€‚å› æ­¤ï¼Œé€šè¿‡ç¼©å°å…·æœ‰æ›´å¼ºå½’çº³åå·®çš„å‡è®¾æœ‰åŠ©äºæé«˜å­¦ä¹ é€Ÿåº¦ã€‚
- en: 'In meta-RL, we impose certain types of inductive biases from the *task distribution*
    and store them in *memory*. Which inductive bias to adopt at test time depends
    on the *algorithm*. Together, these three key components depict a compelling view
    of meta-RL: Adjusting the weights of a recurrent network is slow but it allows
    the model to work out a new task fast with its own RL algorithm implemented in
    its internal activity dynamics.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å…ƒå¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬ä»*ä»»åŠ¡åˆ†å¸ƒ*ä¸­æ–½åŠ æŸäº›ç±»å‹çš„å½’çº³åå·®ï¼Œå¹¶å°†å®ƒä»¬å­˜å‚¨åœ¨*å†…å­˜*ä¸­ã€‚åœ¨æµ‹è¯•æ—¶é‡‡ç”¨å“ªç§å½’çº³åå·®å–å†³äº*ç®—æ³•*ã€‚è¿™ä¸‰ä¸ªå…³é”®ç»„ä»¶å…±åŒæç»˜äº†å…ƒå¼ºåŒ–å­¦ä¹ çš„ä¸€ä¸ªå¼•äººæ³¨ç›®çš„è§†è§’ï¼šè°ƒæ•´å¾ªç¯ç½‘ç»œçš„æƒé‡è™½ç„¶æ…¢ï¼Œä½†å®ƒå…è®¸æ¨¡å‹é€šè¿‡å…¶å†…éƒ¨æ´»åŠ¨åŠ¨æ€ä¸­å®ç°çš„è‡ªå·±çš„RLç®—æ³•å¿«é€Ÿè§£å†³æ–°ä»»åŠ¡ã€‚
- en: 'Meta-RL interestingly and not very surprisingly matches the ideas in the [AI-GAs](https://arxiv.org/abs/1905.10985)
    (â€œAI-Generating Algorithmsâ€) paper by Jeff Clune (2019). He proposed that one
    efficient way towards building general AI is to make learning as automatic as
    possible. The AI-GAs approach involves three pillars: (1) meta-learning architectures,
    (2) meta-learning algorithms, and (3) automatically generated environments for
    effective learning.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰è¶£è€Œåˆä¸å¤ªä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå…ƒå¼ºåŒ–å­¦ä¹ ä¸Jeff Cluneï¼ˆ2019ï¼‰çš„[AI-GAs](https://arxiv.org/abs/1905.10985)ï¼ˆâ€œAI-Generating
    Algorithmsâ€ï¼‰è®ºæ–‡ä¸­çš„æƒ³æ³•ç›¸åŒ¹é…ã€‚ä»–æå‡ºï¼Œæ„å»ºé€šç”¨äººå·¥æ™ºèƒ½çš„ä¸€ç§æœ‰æ•ˆæ–¹å¼æ˜¯å°½å¯èƒ½ä½¿å­¦ä¹ è‡ªåŠ¨åŒ–ã€‚AI-GAsæ–¹æ³•æ¶‰åŠä¸‰å¤§æ”¯æŸ±ï¼šï¼ˆ1ï¼‰å…ƒå­¦ä¹ æ¶æ„ï¼Œï¼ˆ2ï¼‰å…ƒå­¦ä¹ ç®—æ³•ï¼Œä»¥åŠï¼ˆ3ï¼‰ä¸ºæœ‰æ•ˆå­¦ä¹ è€Œè‡ªåŠ¨ç”Ÿæˆçš„ç¯å¢ƒã€‚
- en: '* * *'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'The topic of designing good recurrent network architectures is a bit too broad
    to be discussed here, so I will skip it. Next, letâ€™s look further into another
    two components: meta-learning algorithms in the context of meta-RL and how to
    acquire a variety of training MDPs.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾è®¡è‰¯å¥½çš„å¾ªç¯ç½‘ç»œæ¶æ„çš„ä¸»é¢˜æœ‰ç‚¹å¤ªå¹¿æ³›ï¼Œè¿™é‡Œä¸è®¨è®ºã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬è¿›ä¸€æ­¥æ¢è®¨å¦å¤–ä¸¤ä¸ªç»„æˆéƒ¨åˆ†ï¼šåœ¨å…ƒå¼ºåŒ–å­¦ä¹ èƒŒæ™¯ä¸‹çš„å…ƒå­¦ä¹ ç®—æ³•ä»¥åŠå¦‚ä½•è·å–å„ç§è®­ç»ƒMDPã€‚
- en: Meta-Learning Algorithms for Meta-RL
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç”¨äºå…ƒå¼ºåŒ–å­¦ä¹ çš„å…ƒå­¦ä¹ ç®—æ³•
- en: My previous [post](https://lilianweng.github.io/posts/2018-11-30-meta-learning/)
    on meta-learning has covered several classic meta-learning algorithms. Here Iâ€™m
    gonna include more related to RL.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¹‹å‰çš„[å¸–å­](https://lilianweng.github.io/posts/2018-11-30-meta-learning/)å…³äºå…ƒå­¦ä¹ å·²ç»æ¶µç›–äº†å‡ ç§ç»å…¸çš„å…ƒå­¦ä¹ ç®—æ³•ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘å°†åŒ…æ‹¬æ›´å¤šä¸RLç›¸å…³çš„å†…å®¹ã€‚
- en: Optimizing Model Weights for Meta-learning
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸ºå…ƒå­¦ä¹ ä¼˜åŒ–æ¨¡å‹æƒé‡
- en: Both MAML ([Finn, et al. 2017](https://arxiv.org/abs/1703.03400)) and Reptile
    ([Nichol et al., 2018](https://arxiv.org/abs/1803.02999)) are methods on updating
    model parameters in order to achieve good generalization performance on new tasks.
    See an earlier post [section](https://lilianweng.github.io/posts/2018-11-30-meta-learning/#optimization-based)
    on MAML and Reptile.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: MAMLï¼ˆ[Finnç­‰äººï¼Œ2017](https://arxiv.org/abs/1703.03400)ï¼‰å’ŒReptileï¼ˆ[Nicholç­‰äººï¼Œ2018](https://arxiv.org/abs/1803.02999)ï¼‰éƒ½æ˜¯æ›´æ–°æ¨¡å‹å‚æ•°ä»¥åœ¨æ–°ä»»åŠ¡ä¸Šå®ç°è‰¯å¥½æ³›åŒ–æ€§èƒ½çš„æ–¹æ³•ã€‚è¯·å‚é˜…æœ‰å…³MAMLå’ŒReptileçš„æ—©æœŸå¸–å­[éƒ¨åˆ†](https://lilianweng.github.io/posts/2018-11-30-meta-learning/#optimization-based)ã€‚
- en: Meta-learning Hyperparameters
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…ƒå­¦ä¹ è¶…å‚æ•°
- en: The [return](https://lilianweng.github.io/posts/2018-02-19-rl-overview/#value-function)
    function in an RL problem, $G_t^{(n)}$ or $G_t^\lambda$, involves a few hyperparameters
    that are often set heuristically, like the discount factor [$\gamma$](https://lilianweng.github.io/posts/2018-02-19-rl-overview/#value-function)
    and the bootstrapping parameter [$\lambda$](https://lilianweng.github.io/posts/2018-02-19-rl-overview/#combining-td-and-mc-learning).
    Meta-gradient RL ([Xu et al., 2018](http://papers.nips.cc/paper/7507-meta-gradient-reinforcement-learning.pdf))
    considers them as *meta-parameters*, $\eta=\{\gamma, \lambda \}$, that can be
    tuned and learned *online* while an agent is interacting with the environment.
    Therefore, the return becomes a function of $\eta$ and dynamically adapts itself
    to a specific task over time.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼ºåŒ–å­¦ä¹ é—®é¢˜ä¸­ï¼Œ$G_t^{(n)}$æˆ–$G_t^\lambda$çš„[å›æŠ¥](https://lilianweng.github.io/posts/2018-02-19-rl-overview/#value-function)å‡½æ•°æ¶‰åŠä¸€äº›ç»å¸¸å¯å‘å¼è®¾ç½®çš„è¶…å‚æ•°ï¼Œå¦‚æŠ˜æ‰£å› å­[$\gamma$](https://lilianweng.github.io/posts/2018-02-19-rl-overview/#value-function)å’Œè‡ªä¸¾å‚æ•°[$\lambda$](https://lilianweng.github.io/posts/2018-02-19-rl-overview/#combining-td-and-mc-learning)ã€‚å…ƒæ¢¯åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆ[Xuç­‰ï¼Œ2018](http://papers.nips.cc/paper/7507-meta-gradient-reinforcement-learning.pdf)ï¼‰å°†å®ƒä»¬è§†ä¸º*å…ƒå‚æ•°*ï¼Œ$\eta=\{\gamma,
    \lambda \}$ï¼Œå¯ä»¥åœ¨ä»£ç†ä¸ç¯å¢ƒäº¤äº’æ—¶*åœ¨çº¿*è°ƒæ•´å’Œå­¦ä¹ ã€‚å› æ­¤ï¼Œå›æŠ¥æˆä¸º$\eta$çš„å‡½æ•°ï¼Œå¹¶éšç€æ—¶é—´åŠ¨æ€åœ°é€‚åº”ç‰¹å®šä»»åŠ¡ã€‚
- en: $$ \begin{aligned} G_\eta^{(n)}(\tau_t) &= R_{t+1} + \gamma R_{t+2} + \dots
    + \gamma^{n-1}R_{t+n} + \gamma^n v_\theta(s_{t+n}) & \scriptstyle{\text{; n-step
    return}} \\ G_\eta^{\lambda}(\tau_t) &= (1-\lambda) \sum_{n=1}^\infty \lambda^{n-1}
    G_\eta^{(n)} & \scriptstyle{\text{; Î»-return, mixture of n-step returns}} \end{aligned}
    $$
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} G_\eta^{(n)}(\tau_t) &= R_{t+1} + \gamma R_{t+2} + \dots
    + \gamma^{n-1}R_{t+n} + \gamma^n v_\theta(s_{t+n}) & \scriptstyle{\text{; næ­¥å›æŠ¥}}
    \\ G_\eta^{\lambda}(\tau_t) &= (1-\lambda) \sum_{n=1}^\infty \lambda^{n-1} G_\eta^{(n)}
    & \scriptstyle{\text{; Î»å›æŠ¥ï¼Œnæ­¥å›æŠ¥çš„æ··åˆ}} \end{aligned} $$
- en: During training, we would like to update the policy parameters with gradients
    as a function of all the information in hand, $\thetaâ€™ = \theta + f(\tau, \theta,
    \eta)$, where $\theta$ are the current model weights, $\tau$ is a sequence of
    trajectories, and $\eta$ are the meta-parameters.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›æ ¹æ®æ‰‹å¤´æ‰€æœ‰ä¿¡æ¯çš„æ¢¯åº¦æ›´æ–°ç­–ç•¥å‚æ•°ï¼Œ$\theta' = \theta + f(\tau, \theta, \eta)$ï¼Œå…¶ä¸­$\theta$æ˜¯å½“å‰æ¨¡å‹æƒé‡ï¼Œ$\tau$æ˜¯ä¸€ç³»åˆ—è½¨è¿¹ï¼Œ$\eta$æ˜¯å…ƒå‚æ•°ã€‚
- en: 'Meanwhile, letâ€™s say we have a meta-objective function $J(\tau, \theta, \eta)$
    as a performance measure. The training process follows the principle of online
    cross-validation, using a sequence of consecutive experiences:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ—¶ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå…ƒç›®æ ‡å‡½æ•°$J(\tau, \theta, \eta)$ä½œä¸ºæ€§èƒ½åº¦é‡ã€‚è®­ç»ƒè¿‡ç¨‹éµå¾ªåœ¨çº¿äº¤å‰éªŒè¯åŸåˆ™ï¼Œä½¿ç”¨ä¸€ç³»åˆ—è¿ç»­çš„ç»éªŒï¼š
- en: Starting with parameter $\theta$, the policy $\pi_\theta$ is updated on the
    first batch of samples $\tau$, resulting in $\thetaâ€™$.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»å‚æ•°$\theta$å¼€å§‹ï¼Œç­–ç•¥$\pi_\theta$åœ¨ç¬¬ä¸€æ‰¹æ ·æœ¬$\tau$ä¸Šè¿›è¡Œæ›´æ–°ï¼Œå¾—åˆ°$\theta'$ã€‚
- en: Then we continue running the policy $\pi_{\thetaâ€™}$ to collect a new set of
    experiences $\tauâ€™$, just following $\tau$ consecutively in time. The performance
    is measured as $J(\tauâ€™, \thetaâ€™, \bar{\eta})$ with a fixed meta-parameter $\bar{\eta}$.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬ç»§ç»­è¿è¡Œç­–ç•¥$\pi_{\theta'}$ä»¥æ”¶é›†ä¸€ç»„æ–°çš„ç»éªŒ$\tau'$ï¼Œåªæ˜¯æŒ‰æ—¶é—´é¡ºåºè¿ç»­åœ°è·Ÿéš$\tau$ã€‚æ€§èƒ½ä»¥å›ºå®šçš„å…ƒå‚æ•°$\bar{\eta}$æ¥è¡¡é‡ä¸º$J(\tau',
    \theta', \bar{\eta})$ã€‚
- en: 'The gradient of meta-objective $J(\tauâ€™, \thetaâ€™, \bar{\eta})$ w.r.t. $\eta$
    is used to update $\eta$:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å…ƒç›®æ ‡å‡½æ•°$J(\tau', \theta', \bar{\eta})$ç›¸å¯¹äº$\eta$çš„æ¢¯åº¦ç”¨äºæ›´æ–°$\eta$ï¼š
- en: $$ \begin{aligned} \Delta \eta &= -\beta \frac{\partial J(\tau', \theta', \bar{\eta})}{\partial
    \eta} \\ &= -\beta \frac{\partial J(\tau', \theta', \bar{\eta})}{\partial \theta'}
    \frac{d\theta'}{d\eta} & \scriptstyle{\text{ ; single variable chain rule.}} \\
    &= -\beta \frac{\partial J(\tau', \theta', \bar{\eta})}{\partial \theta'} \frac{\partial
    (\theta + f(\tau, \theta, \eta))}{\partial\eta} \\ &= -\beta \frac{\partial J(\tau',
    \theta', \bar{\eta})}{\partial \theta'} \Big(\frac{d\theta}{d\eta} + \frac{\partial
    f(\tau, \theta, \eta)}{\partial\theta}\frac{d\theta}{d\eta} + \frac{\partial f(\tau,
    \theta, \eta)}{\partial\eta}\frac{d\eta}{d\eta} \Big) & \scriptstyle{\text{; multivariable
    chain rule.}}\\ &= -\beta \frac{\partial J(\tau', \theta', \bar{\eta})}{\partial
    \theta'} \Big( \color{red}{\big(\mathbf{I} + \frac{\partial f(\tau, \theta, \eta)}{\partial\theta}\big)}\frac{d\theta}{d\eta}
    + \frac{\partial f(\tau, \theta, \eta)}{\partial\eta}\Big) & \scriptstyle{\text{;
    secondary gradient term in red.}} \end{aligned} $$
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \Delta \eta &= -\beta \frac{\partial J(\tau', \theta', \bar{\eta})}{\partial
    \eta} \\ &= -\beta \frac{\partial J(\tau', \theta', \bar{\eta})}{\partial \theta'}
    \frac{d\theta'}{d\eta} & \scriptstyle{\text{ ; å•å˜é‡é“¾å¼æ³•åˆ™ã€‚}} \\ &= -\beta \frac{\partial
    J(\tau', \theta', \bar{\eta})}{\partial \theta'} \frac{\partial (\theta + f(\tau,
    \theta, \eta))}{\partial\eta} \\ &= -\beta \frac{\partial J(\tau', \theta', \bar{\eta})}{\partial
    \theta'} \Big(\frac{d\theta}{d\eta} + \frac{\partial f(\tau, \theta, \eta)}{\partial\theta}\frac{d\theta}{d\eta}
    + \frac{\partial f(\tau, \theta, \eta)}{\partial\eta}\frac{d\eta}{d\eta} \Big)
    & \scriptstyle{\text{; å¤šå˜é‡é“¾å¼æ³•åˆ™ã€‚}}\\ &= -\beta \frac{\partial J(\tau', \theta',
    \bar{\eta})}{\partial \theta'} \Big( \color{red}{\big(\mathbf{I} + \frac{\partial
    f(\tau, \theta, \eta)}{\partial\theta}\big)}\frac{d\theta}{d\eta} + \frac{\partial
    f(\tau, \theta, \eta)}{\partial\eta}\Big) & \scriptstyle{\text{; çº¢è‰²çš„äºŒæ¬¡æ¢¯åº¦é¡¹ã€‚}} \end{aligned}
    $$
- en: where $\beta$ is the learning rate for $\eta$.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\beta$ æ˜¯ $\eta$ çš„å­¦ä¹ ç‡ã€‚
- en: 'The meta-gradient RL algorithm simplifies the computation by setting the secondary
    gradient term to zero, $\mathbf{I} + \partial g(\tau, \theta, \eta)/\partial\theta
    = 0$ â€” this choice prefers the immediate effect of the meta-parameters $\eta$
    on the parameters $\theta$. Eventually we get:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å…ƒæ¢¯åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•é€šè¿‡å°†äºŒæ¬¡æ¢¯åº¦é¡¹è®¾ç½®ä¸ºé›¶æ¥ç®€åŒ–è®¡ç®—ï¼Œ$\mathbf{I} + \partial g(\tau, \theta, \eta)/\partial\theta
    = 0$ â€” è¿™ä¸ªé€‰æ‹©æ›´åå‘äºå…ƒå‚æ•° $\eta$ å¯¹å‚æ•° $\theta$ çš„ç›´æ¥å½±å“ã€‚æœ€ç»ˆæˆ‘ä»¬å¾—åˆ°ï¼š
- en: $$ \Delta \eta = -\beta \frac{\partial J(\tau', \theta', \bar{\eta})}{\partial
    \theta'} \frac{\partial f(\tau, \theta, \eta)}{\partial\eta} $$
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \Delta \eta = -\beta \frac{\partial J(\tau', \theta', \bar{\eta})}{\partial
    \theta'} \frac{\partial f(\tau, \theta, \eta)}{\partial\eta} $$
- en: 'Experiments in the paper adopted the meta-objective function same as $TD(\lambda)$
    algorithm, minimizing the error between the approximated value function $v_\theta(s)$
    and the $\lambda$-return:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡ä¸­çš„å®éªŒé‡‡ç”¨äº†ä¸ $TD(\lambda)$ ç®—æ³•ç›¸åŒçš„å…ƒç›®æ ‡å‡½æ•°ï¼Œæœ€å°åŒ–è¿‘ä¼¼å€¼å‡½æ•° $v_\theta(s)$ ä¸ $\lambda$-å›æŠ¥ä¹‹é—´çš„è¯¯å·®ï¼š
- en: $$ \begin{aligned} J(\tau, \theta, \eta) &= (G^\lambda_\eta(\tau) - v_\theta(s))^2
    \\ J(\tau', \theta', \bar{\eta}) &= (G^\lambda_{\bar{\eta}}(\tau') - v_{\theta'}(s'))^2
    \end{aligned} $$
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} J(\tau, \theta, \eta) &= (G^\lambda_\eta(\tau) - v_\theta(s))^2
    \\ J(\tau', \theta', \bar{\eta}) &= (G^\lambda_{\bar{\eta}}(\tau') - v_{\theta'}(s'))^2
    \end{aligned} $$
- en: Meta-learning the Loss Function
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…ƒå­¦ä¹ æŸå¤±å‡½æ•°
- en: In policy gradient algorithms, the expected total reward is maximized by updating
    the policy parameters $\theta$ in the direction of estimated gradient ([Schulman
    et al., 2016](https://arxiv.org/abs/1506.02438)),
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç­–ç•¥æ¢¯åº¦ç®—æ³•ä¸­ï¼Œé€šè¿‡æ›´æ–°ç­–ç•¥å‚æ•° $\theta$ æœç€ä¼°è®¡æ¢¯åº¦çš„æ–¹å‘è¿›è¡Œï¼Œæœ€å¤§åŒ–æœŸæœ›æ€»å¥–åŠ±ï¼ˆ[Schulman et al., 2016](https://arxiv.org/abs/1506.02438)ï¼‰ï¼Œ
- en: $$ g = \mathbb{E}[\sum_{t=0}^\infty \Psi_t \nabla_\theta \log \pi_\theta (a_t
    \mid s_t)] $$
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: $$ g = \mathbb{E}[\sum_{t=0}^\infty \Psi_t \nabla_\theta \log \pi_\theta (a_t
    \mid s_t)] $$
- en: 'where the candidates for $\Psi_t$ include the trajectory return $G_t$, the
    Q value $Q(s_t, a_t)$, or the advantage value $A(s_t, a_t)$. The corresponding
    surrogate loss function for the policy gradient can be reverse-engineered:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\Psi_t$ çš„å€™é€‰åŒ…æ‹¬è½¨è¿¹å›æŠ¥ $G_t$ï¼ŒQ å€¼ $Q(s_t, a_t)$ï¼Œæˆ–ä¼˜åŠ¿å€¼ $A(s_t, a_t)$ã€‚ç­–ç•¥æ¢¯åº¦çš„ç›¸åº”æ›¿ä»£æŸå¤±å‡½æ•°å¯ä»¥è¢«åå‘å·¥ç¨‹å‡ºæ¥ï¼š
- en: $$ L_\text{pg} = \mathbb{E}[\sum_{t=0}^\infty \Psi_t \log \pi_\theta (a_t \mid
    s_t)] $$
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: $$ L_\text{pg} = \mathbb{E}[\sum_{t=0}^\infty \Psi_t \log \pi_\theta (a_t \mid
    s_t)] $$
- en: This loss function is a measure over a history of trajectories, $(s_0, a_0,
    r_0, \dots, s_t, a_t, r_t, \dots)$. **Evolved Policy Gradient** (**EPG**; [Houthooft,
    et al, 2018](https://papers.nips.cc/paper/7785-evolved-policy-gradients.pdf))
    takes a step further by defining the policy gradient loss function as a temporal
    convolution (1-D convolution) over the agentâ€™s past experience, $L_\phi$. The
    parameters $\phi$ of the loss function network are evolved in a way that an agent
    can achieve higher returns.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæŸå¤±å‡½æ•°æ˜¯å¯¹è½¨è¿¹å†å²$(s_0, a_0, r_0, \dots, s_t, a_t, r_t, \dots)$çš„åº¦é‡ã€‚**è¿›åŒ–ç­–ç•¥æ¢¯åº¦**ï¼ˆ**EPG**ï¼›[Houthooftç­‰äººï¼Œ2018](https://papers.nips.cc/paper/7785-evolved-policy-gradients.pdf)ï¼‰è¿›ä¸€æ­¥å®šä¹‰äº†ç­–ç•¥æ¢¯åº¦æŸå¤±å‡½æ•°ä¸ºä»£ç†è¿‡å»ç»éªŒçš„æ—¶é—´å·ç§¯ï¼ˆ1-Då·ç§¯ï¼‰$L_\phi$ã€‚æŸå¤±å‡½æ•°ç½‘ç»œçš„å‚æ•°$\phi$è¢«æ¼”åŒ–ï¼Œä»¥ä½¿ä»£ç†èƒ½å¤Ÿè·å¾—æ›´é«˜çš„å›æŠ¥ã€‚
- en: 'Similar to many meta-learning algorithms, EPG has two optimization loops:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è®¸å¤šå…ƒå­¦ä¹ ç®—æ³•ç±»ä¼¼ï¼ŒEPGæœ‰ä¸¤ä¸ªä¼˜åŒ–å¾ªç¯ï¼š
- en: In the internal loop, an agent learns to improve its policy $\pi_\theta$.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨å†…éƒ¨å¾ªç¯ä¸­ï¼Œä»£ç†å­¦ä¹ æ”¹è¿›å…¶ç­–ç•¥$\pi_\theta$ã€‚
- en: In the outer loop, the model updates the parameters $\phi$ of the loss function
    $L_\phi$. Because there is no explicit way to write down a differentiable equation
    between the return and the loss, EPG turned to [*Evolutionary Strategies*](https://en.wikipedia.org/wiki/Evolution_strategy)
    (ES).
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨å¤–å¾ªç¯ä¸­ï¼Œæ¨¡å‹æ›´æ–°æŸå¤±å‡½æ•°$L_\phi$çš„å‚æ•°$\phi$ã€‚ç”±äºå›æŠ¥å’ŒæŸå¤±ä¹‹é—´æ²¡æœ‰æ˜ç¡®çš„å¯å¾®æ–¹ç¨‹ï¼ŒEPGè½¬å‘[*è¿›åŒ–ç­–ç•¥*](https://en.wikipedia.org/wiki/Evolution_strategy)ï¼ˆESï¼‰ã€‚
- en: 'A general idea is to train a population of $N$ agents, each of them is trained
    with the loss function $L_{\phi + \sigma \epsilon_i}$ parameterized with $\phi$
    added with a small Gaussian noise $\epsilon_i \sim \mathcal{N}(0, \mathbf{I})$
    of standard deviation $\sigma$. During the inner loopâ€™s training, EPG tracks a
    history of experience and updates the policy parameters according to the loss
    function $L_{\phi + \sigma\epsilon_i}$ for each agent:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªä¸€èˆ¬çš„æƒ³æ³•æ˜¯è®­ç»ƒä¸€ä¸ªç”±$N$ä¸ªä»£ç†ç»„æˆçš„ç¾¤ä½“ï¼Œæ¯ä¸ªä»£ç†éƒ½ä½¿ç”¨å¸¦æœ‰$\phi$çš„æŸå¤±å‡½æ•°$L_{\phi + \sigma \epsilon_i}$è¿›è¡Œè®­ç»ƒï¼ŒåŠ ä¸Šæ ‡å‡†å·®ä¸º$\sigma$çš„å°é«˜æ–¯å™ªå£°$\epsilon_i
    \sim \mathcal{N}(0, \mathbf{I})$ã€‚åœ¨å†…å¾ªç¯çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒEPGè·Ÿè¸ªç»éªŒå†å²ï¼Œå¹¶æ ¹æ®æ¯ä¸ªä»£ç†çš„æŸå¤±å‡½æ•°$L_{\phi + \sigma\epsilon_i}$æ›´æ–°ç­–ç•¥å‚æ•°ï¼š
- en: $$ \theta_i \leftarrow \theta - \alpha_\text{in} \nabla_\theta L_{\phi + \sigma
    \epsilon_i} (\pi_\theta, \tau_{t-K, \dots, t}) $$
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \theta_i \leftarrow \theta - \alpha_\text{in} \nabla_\theta L_{\phi + \sigma
    \epsilon_i} (\pi_\theta, \tau_{t-K, \dots, t}) $$
- en: where $\alpha_\text{in}$ is the learning rate of the inner loop and $\tau_{t-K,
    \dots, t}$ is a sequence of $M$ transitions up to the current time step $t$.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­$\alpha_\text{in}$æ˜¯å†…å¾ªç¯çš„å­¦ä¹ ç‡ï¼Œ$\tau_{t-K, \dots, t}$æ˜¯ç›´åˆ°å½“å‰æ—¶é—´æ­¥$t$çš„$M$ä¸ªè½¬æ¢çš„åºåˆ—ã€‚
- en: Once the inner loop policy is mature enough, the policy is evaluated by the
    mean return $\bar{G}_{\phi+\sigma\epsilon_i}$ over multiple randomly sampled trajectories.
    Eventually, we are able to estimate the gradient of $\phi$ according to [NES](http://blog.otoro.net/2017/10/29/visual-evolution-strategies/)
    numerically ([Salimans et al, 2017](https://arxiv.org/abs/1703.03864)). While
    repeating this process, both the policy parameters $\theta$ and the loss function
    weights $\phi$ are being updated simultaneously to achieve higher returns.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦å†…å¾ªç¯ç­–ç•¥è¶³å¤Ÿæˆç†Ÿï¼Œè¯¥ç­–ç•¥å°†é€šè¿‡å¤šä¸ªéšæœºé‡‡æ ·çš„è½¨è¿¹çš„å¹³å‡å›æŠ¥$\bar{G}_{\phi+\sigma\epsilon_i}$è¿›è¡Œè¯„ä¼°ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ ¹æ®[NES](http://blog.otoro.net/2017/10/29/visual-evolution-strategies/)åœ¨æ•°å€¼ä¸Šè¯„ä¼°$\phi$çš„æ¢¯åº¦ï¼ˆ[Salimansç­‰äººï¼Œ2017](https://arxiv.org/abs/1703.03864)ï¼‰ã€‚åœ¨é‡å¤è¿™ä¸ªè¿‡ç¨‹çš„åŒæ—¶ï¼Œç­–ç•¥å‚æ•°$\theta$å’ŒæŸå¤±å‡½æ•°æƒé‡$\phi$åŒæ—¶æ›´æ–°ï¼Œä»¥å®ç°æ›´é«˜çš„å›æŠ¥ã€‚
- en: $$ \phi \leftarrow \phi + \alpha_\text{out} \frac{1}{\sigma N} \sum_{i=1}^N
    \epsilon_i G_{\phi+\sigma\epsilon_i} $$
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \phi \leftarrow \phi + \alpha_\text{out} \frac{1}{\sigma N} \sum_{i=1}^N
    \epsilon_i G_{\phi+\sigma\epsilon_i} $$
- en: where $\alpha_\text{out}$ is the learning rate of the outer loop.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­$\alpha_\text{out}$æ˜¯å¤–å¾ªç¯çš„å­¦ä¹ ç‡ã€‚
- en: In practice, the loss $L_\phi$ is bootstrapped with an ordinary policy gradient
    (such as REINFORCE or PPO) surrogate loss $L_\text{pg}$, $\hat{L} = (1-\alpha)
    L_\phi + \alpha L_\text{pg}$. The weight $\alpha$ is annealing from 1 to 0 gradually
    during training. At test time, the loss function parameter $\phi$ stays fixed
    and the loss value is computed over a history of experience to update the policy
    parameters $\theta$.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®è·µä¸­ï¼ŒæŸå¤±$L_\phi$ä½¿ç”¨æ™®é€šç­–ç•¥æ¢¯åº¦ï¼ˆå¦‚REINFORCEæˆ–PPOï¼‰æ›¿ä»£æŸå¤±$L_\text{pg}$è¿›è¡Œå¼•å¯¼ï¼Œ$\hat{L} = (1-\alpha)
    L_\phi + \alpha L_\text{pg}$ã€‚æƒé‡$\alpha$åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ¸ä»1é™è‡³0ã€‚åœ¨æµ‹è¯•æ—¶ï¼ŒæŸå¤±å‡½æ•°å‚æ•°$\phi$ä¿æŒå›ºå®šï¼ŒæŸå¤±å€¼æ˜¯æ ¹æ®ç»éªŒå†å²è®¡ç®—çš„ï¼Œä»¥æ›´æ–°ç­–ç•¥å‚æ•°$\theta$ã€‚
- en: Meta-learning the Exploration Strategies
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…ƒå­¦ä¹ æ¢ç´¢ç­–ç•¥
- en: The [exploitation vs exploration](https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/#exploitation-vs-exploration)
    dilemma is a critical problem in RL. Common ways to do exploration include $\epsilon$-greedy,
    random noise on actions, or stochastic policy with built-in randomness on the
    action space.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œ[å¼€å‘ vs æ¢ç´¢](https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/#exploitation-vs-exploration)å›°å¢ƒæ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚å¸¸è§çš„æ¢ç´¢æ–¹å¼åŒ…æ‹¬$\epsilon$-è´ªå¿ƒã€åœ¨åŠ¨ä½œä¸Šæ·»åŠ éšæœºå™ªå£°ï¼Œæˆ–è€…åœ¨åŠ¨ä½œç©ºé—´ä¸Šå…·æœ‰å†…ç½®éšæœºæ€§çš„éšæœºç­–ç•¥ã€‚
- en: '**MAESN** ([Gupta et al, 2018](http://papers.nips.cc/paper/7776-meta-reinforcement-learning-of-structured-exploration-strategies.pdf))
    is an algorithm to learn structured action noise from prior experience for better
    and more effective exploration. Simply adding random noise on actions cannot capture
    task-dependent or time-correlated exploration strategies. MAESN changes the policy
    to condition on a per-task random variable $z_i \sim \mathcal{N}(\mu_i, \sigma_i)$,
    for $i$-th task $M_i$, so we would have a policy $a \sim \pi_\theta(a\mid s, z_i)$.
    The latent variable $z_i$ is sampled once and fixed during one episode. Intuitively,
    the latent variable determines one type of behavior (or skills) that should be
    explored more at the beginning of a rollout and the agent would adjust its actions
    accordingly. Both the policy parameters and latent space are optimized to maximize
    the total task rewards. In the meantime, the policy learns to make use of the
    latent variables for exploration.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**MAESN**ï¼ˆ[Gupta et al, 2018](http://papers.nips.cc/paper/7776-meta-reinforcement-learning-of-structured-exploration-strategies.pdf)ï¼‰æ˜¯ä¸€ç§ç®—æ³•ï¼Œç”¨äºä»å…ˆå‰ç»éªŒä¸­å­¦ä¹ ç»“æ„åŒ–çš„åŠ¨ä½œå™ªå£°ï¼Œä»¥å®ç°æ›´å¥½æ›´æœ‰æ•ˆçš„æ¢ç´¢ã€‚ç®€å•åœ°åœ¨åŠ¨ä½œä¸Šæ·»åŠ éšæœºå™ªå£°æ— æ³•æ•æ‰ä»»åŠ¡ç›¸å…³æˆ–æ—¶é—´ç›¸å…³çš„æ¢ç´¢ç­–ç•¥ã€‚MAESNæ”¹å˜ç­–ç•¥ä»¥ä¾èµ–äºæ¯ä¸ªä»»åŠ¡çš„éšæœºå˜é‡$z_i
    \sim \mathcal{N}(\mu_i, \sigma_i)$ï¼Œå¯¹äºç¬¬$i$ä¸ªä»»åŠ¡$M_i$ï¼Œå› æ­¤æˆ‘ä»¬ä¼šæœ‰ä¸€ä¸ªç­–ç•¥$a \sim \pi_\theta(a\mid
    s, z_i)$ã€‚æ½œå˜é‡$z_i$åœ¨ä¸€ä¸ªepisodeä¸­è¢«é‡‡æ ·ä¸€æ¬¡å¹¶å›ºå®šã€‚ç›´è§‚åœ°ï¼Œæ½œå˜é‡ç¡®å®šäº†åœ¨ä¸€ä¸ªrolloutå¼€å§‹æ—¶åº”æ›´å¤šæ¢ç´¢çš„ä¸€ç§è¡Œä¸ºï¼ˆæˆ–æŠ€èƒ½ï¼‰ï¼Œä»£ç†ä¼šç›¸åº”åœ°è°ƒæ•´å…¶åŠ¨ä½œã€‚ç­–ç•¥å‚æ•°å’Œæ½œç©ºé—´éƒ½è¢«ä¼˜åŒ–ä»¥æœ€å¤§åŒ–æ€»ä»»åŠ¡å¥–åŠ±ã€‚åŒæ—¶ï¼Œç­–ç•¥å­¦ä¼šåˆ©ç”¨æ½œå˜é‡è¿›è¡Œæ¢ç´¢ã€‚'
- en: In addition, the loss function includes a KL divergence between the learned
    latent variable and a unit Gaussian prior, $D_\text{KL}(\mathcal{N}(\mu_i, \sigma_i)|\mathcal{N}(0,
    \mathbf{I}))$. On one hand, it restricts the learned latent space not too far
    from a common prior. On the other hand, it creates the variational evidence lower
    bound ([ELBO](http://users.umiacs.umd.edu/~xyang35/files/understanding-variational-lower.pdf))
    for the reward function. Interestingly the paper found that $(\mu_i, \sigma_i)$
    for each task are usually close to the prior at convergence.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°åŒ…æ‹¬å­¦ä¹ çš„æ½œå˜é‡ä¸å•ä½é«˜æ–¯å…ˆéªŒä¹‹é—´çš„KLæ•£åº¦ï¼Œ$D_\text{KL}(\mathcal{N}(\mu_i, \sigma_i)|\mathcal{N}(0,
    \mathbf{I}))$ã€‚ä¸€æ–¹é¢ï¼Œå®ƒé™åˆ¶äº†å­¦ä¹ çš„æ½œç©ºé—´ä¸è¦ç¦»å¸¸è§å…ˆéªŒå¤ªè¿œã€‚å¦ä¸€æ–¹é¢ï¼Œå®ƒä¸ºå¥–åŠ±å‡½æ•°åˆ›å»ºäº†å˜åˆ†è¯æ®ä¸‹ç•Œï¼ˆ[ELBO](http://users.umiacs.umd.edu/~xyang35/files/understanding-variational-lower.pdf)ï¼‰ã€‚æœ‰è¶£çš„æ˜¯ï¼Œè®ºæ–‡å‘ç°æ¯ä¸ªä»»åŠ¡çš„$(\mu_i,
    \sigma_i)$åœ¨æ”¶æ•›æ—¶é€šå¸¸æ¥è¿‘å…ˆéªŒã€‚
- en: '![](../Images/af5b2ce9a9b0fc9948f44823a922e026.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af5b2ce9a9b0fc9948f44823a922e026.png)'
- en: 'Fig. 5\. The policy is conditioned on a latent variable variable $z\_i \sim
    \mathcal{N}(\mu, \sigma)$ that is sampled once every episode. Each task has different
    hyperparameters for the latent variable distribution, $(\mu\_i, \sigma\_i)$ and
    they are optimized in the outer loop. (Image source: [Gupta et al, 2018](http://papers.nips.cc/paper/7776-meta-reinforcement-learning-of-structured-exploration-strategies.pdf))'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5. ç­–ç•¥ä»¥ä¸€ä¸ªæ¯ä¸ªepisodeé‡‡æ ·ä¸€æ¬¡çš„æ½œå˜é‡$z\_i \sim \mathcal{N}(\mu, \sigma)$ä¸ºæ¡ä»¶ã€‚æ¯ä¸ªä»»åŠ¡å¯¹äºæ½œå˜é‡åˆ†å¸ƒæœ‰ä¸åŒçš„è¶…å‚æ•°$(\mu\_i,
    \sigma\_i)$ï¼Œå®ƒä»¬åœ¨å¤–éƒ¨å¾ªç¯ä¸­è¢«ä¼˜åŒ–ã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼š[Gupta et al, 2018](http://papers.nips.cc/paper/7776-meta-reinforcement-learning-of-structured-exploration-strategies.pdf)ï¼‰
- en: Episodic Control
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æƒ…èŠ‚æ§åˆ¶
- en: A major criticism of RL is on its sample inefficiency. A large number of samples
    and small learning steps are required for incremental parameter adjustment in
    RL in order to maximize generalization and avoid catastrophic forgetting of earlier
    learning ([Botvinick et al., 2019](https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(19)30061-0)).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ çš„ä¸€ä¸ªä¸»è¦æ‰¹è¯„æ˜¯å…¶æ ·æœ¬æ•ˆç‡ä½ã€‚åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œä¸ºäº†æœ€å¤§åŒ–æ³›åŒ–å¹¶é¿å…æ—©æœŸå­¦ä¹ çš„ç¾éš¾æ€§é—å¿˜ï¼Œéœ€è¦å¤§é‡æ ·æœ¬å’Œå°çš„å­¦ä¹ æ­¥éª¤è¿›è¡Œå¢é‡å‚æ•°è°ƒæ•´ï¼ˆ[Botvinick
    et al., 2019](https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(19)30061-0)ï¼‰ã€‚
- en: '**Episodic control** ([Lengyel & Dayan, 2008](http://papers.nips.cc/paper/3311-hippocampal-contributions-to-control-the-third-way.pdf))
    is proposed as a solution to avoid forgetting and improve generalization while
    training at a faster speed. It is partially inspired by hypotheses on instance-based
    [hippocampal](https://en.wikipedia.org/wiki/Hippocampus) learning.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**æƒ…èŠ‚æ§åˆ¶**ï¼ˆ[Lengyelï¼†Dayanï¼Œ2008](http://papers.nips.cc/paper/3311-hippocampal-contributions-to-control-the-third-way.pdf)ï¼‰è¢«æå‡ºä½œä¸ºä¸€ä¸ªè§£å†³æ–¹æ¡ˆï¼Œä»¥é¿å…é—å¿˜å¹¶åœ¨æ›´å¿«çš„é€Ÿåº¦ä¸‹æé«˜æ³›åŒ–èƒ½åŠ›ã€‚å®ƒåœ¨ä¸€å®šç¨‹åº¦ä¸Šå—åˆ°äº†å…³äºåŸºäºå®ä¾‹çš„[æµ·é©¬](https://en.wikipedia.org/wiki/Hippocampus)å­¦ä¹ çš„å‡è®¾çš„å¯å‘ã€‚'
- en: 'An *episodic memory* keeps explicit records of past events and uses these records
    directly as point of reference for making new decisions (i.e. just like [metric-based](https://lilianweng.github.io/posts/2018-11-30-meta-learning/#metric-based)
    meta-learning). In **MFEC** (Model-Free Episodic Control; [Blundell et al., 2016](https://arxiv.org/abs/1606.04460)),
    the memory is modeled as a big table, storing the state-action pair $(s, a)$ as
    key and the corresponding Q-value $Q_\text{EC}(s, a)$ as value. When receiving
    a new observation $s$, the Q value is estimated in an non-parametric way as the
    average Q-value of top $k$ most similar samples:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*æƒ…èŠ‚è®°å¿†*ä¼šæ˜ç¡®è®°å½•è¿‡å»äº‹ä»¶ï¼Œå¹¶ç›´æ¥å°†è¿™äº›è®°å½•ç”¨ä½œåˆ¶å®šæ–°å†³ç­–çš„å‚è€ƒç‚¹ï¼ˆå³å°±åƒ[åŸºäºåº¦é‡çš„](https://lilianweng.github.io/posts/2018-11-30-meta-learning/#metric-based)å…ƒå­¦ä¹ ï¼‰ã€‚åœ¨**MFEC**ï¼ˆæ— æ¨¡å‹æƒ…èŠ‚æ§åˆ¶ï¼›[Blundellç­‰äººï¼Œ2016](https://arxiv.org/abs/1606.04460)ï¼‰ä¸­ï¼Œè®°å¿†è¢«å»ºæ¨¡ä¸ºä¸€ä¸ªå¤§è¡¨æ ¼ï¼Œå°†çŠ¶æ€-åŠ¨ä½œå¯¹$(s,
    a)$ä½œä¸ºé”®ï¼Œç›¸åº”çš„Qå€¼$Q_\text{EC}(s, a)$ä½œä¸ºå€¼è¿›è¡Œå­˜å‚¨ã€‚å½“æ¥æ”¶åˆ°æ–°çš„è§‚å¯Ÿ$s$æ—¶ï¼ŒQå€¼ä»¥éå‚æ•°åŒ–æ–¹å¼ä¼°è®¡ä¸ºæœ€ç›¸ä¼¼æ ·æœ¬çš„å‰$k$ä¸ªçš„å¹³å‡Qå€¼ï¼š'
- en: $$ \hat{Q}_\text{EC}(s, a) = \begin{cases} Q_\text{EC}(s, a) & \text{if } (s,a)
    \in Q_\text{EC}, \\ \frac{1}{k} \sum_{i=1}^k Q(s^{(i)}, a) & \text{otherwise}
    \end{cases} $$
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \hat{Q}_\text{EC}(s, a) = \begin{cases} Q_\text{EC}(s, a) & \text{if } (s,a)
    \in Q_\text{EC}, \\ \frac{1}{k} \sum_{i=1}^k Q(s^{(i)}, a) & \text{otherwise}
    \end{cases} $$
- en: 'where $s^{(i)}, i=1, \dots, k$ are top $k$ states with smallest distances to
    the state $s$. Then the action that yields the highest estimated Q value is selected.
    Then the memory table is updated according to the return received at $s_t$:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­$s^{(i)}, i=1, \dots, k$æ˜¯ä¸çŠ¶æ€$s$è·ç¦»æœ€å°çš„å‰$k$ä¸ªçŠ¶æ€ã€‚ç„¶åé€‰æ‹©äº§ç”Ÿæœ€é«˜ä¼°è®¡Qå€¼çš„åŠ¨ä½œã€‚ç„¶åæ ¹æ®åœ¨$s_t$å¤„æ”¶åˆ°çš„å›æŠ¥æ›´æ–°è®°å¿†è¡¨ï¼š
- en: $$ Q_\text{EC}(s, a) \leftarrow \begin{cases} \max\{Q_\text{EC}(s_t, a_t), G_t\}
    & \text{if } (s,a) \in Q_\text{EC}, \\ G_t & \text{otherwise} \end{cases} $$
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: $$ Q_\text{EC}(s, a) \leftarrow \begin{cases} \max\{Q_\text{EC}(s_t, a_t), G_t\}
    & \text{if } (s,a) \in Q_\text{EC}, \\ G_t & \text{otherwise} \end{cases} $$
- en: As a tabular RL method, MFEC suffers from large memory consumption and a lack
    of ways to generalize among similar states. The first one can be fixed with an
    LRU cache. Inspired by [metric-based](https://lilianweng.github.io/posts/2018-11-30-meta-learning/#metric-based)
    meta-learning, especially Matching Networks ([Vinyals et al., 2016](http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.pdf)),
    the generalization problem is improved in a follow-up algorithm, **NEC** (Neural
    Episodic Control; [Pritzel et al., 2016](https://arxiv.org/abs/1703.01988)).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºä¸€ç§è¡¨æ ¼åŒ–å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼ŒMFECå­˜åœ¨ç€å¤§é‡çš„å†…å­˜æ¶ˆè€—å’Œç¼ºä¹åœ¨ç›¸ä¼¼çŠ¶æ€ä¹‹é—´æ³›åŒ–çš„æ–¹æ³•ã€‚ç¬¬ä¸€ä¸ªé—®é¢˜å¯ä»¥é€šè¿‡LRUç¼“å­˜æ¥è§£å†³ã€‚å—åˆ°[åŸºäºåº¦é‡çš„](https://lilianweng.github.io/posts/2018-11-30-meta-learning/#metric-based)å…ƒå­¦ä¹ çš„å¯å‘ï¼Œç‰¹åˆ«æ˜¯åŒ¹é…ç½‘ç»œï¼ˆ[Vinyalsç­‰äººï¼Œ2016](http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.pdf)ï¼‰ï¼Œæ³›åŒ–é—®é¢˜åœ¨åç»­ç®—æ³•**NEC**ï¼ˆç¥ç»è®°å¿†æ§åˆ¶ï¼›[Pritzelç­‰äººï¼Œ2016](https://arxiv.org/abs/1703.01988)ï¼‰ä¸­å¾—åˆ°æ”¹å–„ã€‚
- en: The episodic memory in NEC is a Differentiable Neural Dictionary (**DND**),
    where the key is a convolutional embedding vector of input image pixels and the
    value stores estimated Q value. Given an inquiry key, the output is a weighted
    sum of values of top similar keys, where the weight is a normalized kernel measure
    between the query key and the selected key in the dictionary. This sounds like
    a hard [attention](https://lilianweng.github.io/posts/2018-06-24-attention/) machanism.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: NECä¸­çš„æƒ…èŠ‚è®°å¿†æ˜¯ä¸€ä¸ªå¯å¾®åˆ†ç¥ç»å­—å…¸ï¼ˆ**DND**ï¼‰ï¼Œå…¶ä¸­é”®æ˜¯è¾“å…¥å›¾åƒåƒç´ çš„å·ç§¯åµŒå…¥å‘é‡ï¼Œå€¼å­˜å‚¨ä¼°è®¡çš„Qå€¼ã€‚ç»™å®šä¸€ä¸ªæŸ¥è¯¢é”®ï¼Œè¾“å‡ºæ˜¯æœ€ç›¸ä¼¼é”®çš„å€¼çš„åŠ æƒå’Œï¼Œå…¶ä¸­æƒé‡æ˜¯æŸ¥è¯¢é”®å’Œå­—å…¸ä¸­é€‰å®šé”®ä¹‹é—´çš„å½’ä¸€åŒ–æ ¸åº¦é‡ã€‚è¿™å¬èµ·æ¥åƒæ˜¯ä¸€ç§å¤æ‚çš„[æ³¨æ„åŠ›](https://lilianweng.github.io/posts/2018-06-24-attention/)æœºåˆ¶ã€‚
- en: '![](../Images/8cbec10cd1740e403e3fbf9e5a4dd8ba.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8cbec10cd1740e403e3fbf9e5a4dd8ba.png)'
- en: 'Fig. 6 Illustrations of episodic memory module in NEC and two operations on
    a differentiable neural dictionary. (Image source: [Pritzel et al., 2016](https://arxiv.org/abs/1703.01988))'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾6 NECä¸­æƒ…èŠ‚è®°å¿†æ¨¡å—çš„ç¤ºæ„å›¾ä»¥åŠå¯å¾®åˆ†ç¥ç»å­—å…¸ä¸Šçš„ä¸¤ç§æ“ä½œã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼š[Pritzelç­‰äººï¼Œ2016](https://arxiv.org/abs/1703.01988)ï¼‰
- en: 'Further, **Episodic LSTM** ([Ritter et al., 2018](https://arxiv.org/abs/1805.09692))
    enhances the basic LSTM architecture with a DND episodic memory, which stores
    task context embeddings as keys and the LSTM cell states as values. The stored
    hidden states are retrieved and added directly to the current cell state through
    the same gating mechanism within LSTM:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œ**è®°å¿† LSTM**ï¼ˆ[Ritter ç­‰äººï¼Œ2018](https://arxiv.org/abs/1805.09692)ï¼‰é€šè¿‡ DND è®°å¿†å¢å¼ºäº†åŸºæœ¬
    LSTM ç»“æ„ï¼Œå…¶ä¸­å°†ä»»åŠ¡ä¸Šä¸‹æ–‡åµŒå…¥å­˜å‚¨ä¸ºé”®ï¼Œå°† LSTM å•å…ƒçŠ¶æ€å­˜å‚¨ä¸ºå€¼ã€‚å­˜å‚¨çš„éšè—çŠ¶æ€é€šè¿‡ LSTM å†…ç›¸åŒçš„é—¨æ§æœºåˆ¶ç›´æ¥æ£€ç´¢å¹¶æ·»åŠ åˆ°å½“å‰å•å…ƒçŠ¶æ€ä¸­ï¼š
- en: '![](../Images/313634cf6c32545d7a62dc7e7b4e5575.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/313634cf6c32545d7a62dc7e7b4e5575.png)'
- en: 'Fig. 7\. Illustration of the episodic LSTM architecture. The additional structure
    of episodic memory is in bold. (Image source: [Ritter et al., 2018](https://arxiv.org/abs/1805.09692))'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 7\. æ˜¾ç¤ºäº†è®°å¿† LSTM ç»“æ„çš„ç¤ºæ„å›¾ã€‚è®°å¿†ç»“æ„çš„é¢å¤–éƒ¨åˆ†ç”¨**ç²—ä½“**æ ‡å‡ºã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼š[Ritter ç­‰äººï¼Œ2018](https://arxiv.org/abs/1805.09692)ï¼‰
- en: $$ \begin{aligned} \mathbf{c}_t &= \mathbf{i}_t \circ \mathbf{c}_\text{in} +
    \mathbf{f}_t \circ \mathbf{c}_{t-1} + \color{green}{\mathbf{r}_t \circ \mathbf{c}_\text{ep}}
    &\\ \mathbf{i}_t &= \sigma(\mathbf{W}_{i} \cdot [\mathbf{h}_{t-1}, \mathbf{x}_t]
    + \mathbf{b}_i) & \scriptstyle{\text{; input gate}} \\ \mathbf{f}_t &= \sigma(\mathbf{W}_{f}
    \cdot [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_f) & \scriptstyle{\text{;
    forget gate}} \\ \color{green}{\mathbf{r}_t} & \color{green}{=} \color{green}{\sigma(\mathbf{W}_{r}
    \cdot [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_r)} & \scriptstyle{\text{;
    reinstatement gate}} \end{aligned} $$
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \mathbf{c}_t &= \mathbf{i}_t \circ \mathbf{c}_\text{in} +
    \mathbf{f}_t \circ \mathbf{c}_{t-1} + \color{green}{\mathbf{r}_t \circ \mathbf{c}_\text{ep}}
    &\\ \mathbf{i}_t &= \sigma(\mathbf{W}_{i} \cdot [\mathbf{h}_{t-1}, \mathbf{x}_t]
    + \mathbf{b}_i) & \scriptstyle{\text{; è¾“å…¥é—¨}} \\ \mathbf{f}_t &= \sigma(\mathbf{W}_{f}
    \cdot [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_f) & \scriptstyle{\text{;
    é—å¿˜é—¨}} \\ \color{green}{\mathbf{r}_t} & \color{green}{=} \color{green}{\sigma(\mathbf{W}_{r}
    \cdot [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_r)} & \scriptstyle{\text{;
    é‡å»ºé—¨}} \end{aligned} $$
- en: where $\mathbf{c}_t$ and $\mathbf{h}_t$ are hidden and cell state at time $t$;
    $\mathbf{i}_t$, $\mathbf{f}_t$ and $\mathbf{r}_t$ are input, forget and reinstatement
    gates, respectively; $\mathbf{c}_\text{ep}$ is the retrieved cell state from episodic
    memory. The newly added episodic memory components are marked in green.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­$\mathbf{c}_t$å’Œ$\mathbf{h}_t$åˆ†åˆ«æ˜¯æ—¶é—´$t$çš„éšè—çŠ¶æ€å’Œå•å…ƒçŠ¶æ€ï¼›$\mathbf{i}_t$ã€$\mathbf{f}_t$å’Œ$\mathbf{r}_t$åˆ†åˆ«æ˜¯è¾“å…¥ã€é—å¿˜å’Œé‡å»ºé—¨ï¼›$\mathbf{c}_\text{ep}$æ˜¯ä»è®°å¿†ä¸­æ£€ç´¢åˆ°çš„å•å…ƒçŠ¶æ€ã€‚æ–°å¢çš„è®°å¿†ç»„ä»¶ç”¨ç»¿è‰²æ ‡å‡ºã€‚
- en: This architecture provides a shortcut to the prior experience through context-based
    retrieval. Meanwhile, explicitly saving the task-dependent experience in an external
    memory avoids forgetting. In the paper, all the experiments have manually designed
    context vectors. How to construct an effective and efficient format of task context
    embeddings for more free-formed tasks would be an interesting topic.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¶æ„é€šè¿‡åŸºäºä¸Šä¸‹æ–‡çš„æ£€ç´¢æä¾›äº†å¯¹å…ˆå‰ç»éªŒçš„å¿«æ·æ–¹å¼ã€‚åŒæ—¶ï¼Œæ˜ç¡®åœ°å°†ä»»åŠ¡ç›¸å…³ç»éªŒä¿å­˜åœ¨å¤–éƒ¨å­˜å‚¨å™¨ä¸­é¿å…äº†é—å¿˜ã€‚åœ¨è®ºæ–‡ä¸­ï¼Œæ‰€æœ‰å®éªŒéƒ½æ˜¯æ‰‹åŠ¨è®¾è®¡çš„ä¸Šä¸‹æ–‡å‘é‡ã€‚å¦‚ä½•ä¸ºæ›´è‡ªç”±å½¢å¼çš„ä»»åŠ¡æ„å»ºæœ‰æ•ˆå’Œé«˜æ•ˆçš„ä»»åŠ¡ä¸Šä¸‹æ–‡åµŒå…¥æ ¼å¼å°†æ˜¯ä¸€ä¸ªæœ‰è¶£çš„è¯¾é¢˜ã€‚
- en: Overall the capacity of episodic control is limited by the complexity of the
    environment. It is very rare for an agent to repeatedly visit exactly the same
    states in a real-world task, so properly encoding the states is critical. The
    learned embedding space compresses the observation data into a lower dimension
    space and, in the meantime, two states being close in this space are expected
    to demand similar strategies.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä½“è€Œè¨€ï¼Œè®°å¿†æ§åˆ¶çš„èƒ½åŠ›å—åˆ°ç¯å¢ƒå¤æ‚æ€§çš„é™åˆ¶ã€‚åœ¨çœŸå®ä»»åŠ¡ä¸­ï¼Œä»£ç†å¾ˆå°‘ä¼šé‡å¤è®¿é—®å®Œå…¨ç›¸åŒçš„çŠ¶æ€ï¼Œå› æ­¤æ­£ç¡®ç¼–ç çŠ¶æ€è‡³å…³é‡è¦ã€‚å­¦ä¹ çš„åµŒå…¥ç©ºé—´å°†è§‚å¯Ÿæ•°æ®å‹ç¼©åˆ°è¾ƒä½ç»´åº¦ç©ºé—´ä¸­ï¼ŒåŒæ—¶ï¼Œåœ¨æ­¤ç©ºé—´ä¸­æ¥è¿‘çš„ä¸¤ä¸ªçŠ¶æ€é¢„è®¡éœ€è¦ç›¸ä¼¼çš„ç­–ç•¥ã€‚
- en: Training Task Acquisition
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®­ç»ƒä»»åŠ¡è·å–
- en: 'Among three key components, how to design a proper distribution of tasks is
    the less studied and probably the most specific one to meta-RL itself. As described
    [above](#formulation), each task is a MDP: $M_i = \langle \mathcal{S}, \mathcal{A},
    P_i, R_i \rangle \in \mathcal{M}$. We can build a distribution of MDPs by modifying:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‰ä¸ªå…³é”®ç»„ä»¶ä¸­ï¼Œå¦‚ä½•è®¾è®¡é€‚å½“çš„ä»»åŠ¡åˆ†å¸ƒæ˜¯ç ”ç©¶è¾ƒå°‘ä¸”å¯èƒ½æ˜¯æœ€å…·ä½“äºå…ƒå¼ºåŒ–å­¦ä¹ æœ¬èº«çš„ã€‚å¦‚ä¸Šæ‰€è¿°ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½æ˜¯ä¸€ä¸ªé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼š$M_i = \langle
    \mathcal{S}, \mathcal{A}, P_i, R_i \rangle \in \mathcal{M}$ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¿®æ”¹æ¥æ„å»ºä¸€ä¸ªé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹çš„åˆ†å¸ƒï¼š
- en: 'The *reward configuration*: Among different tasks, same behavior might get
    rewarded differently according to $R_i$.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*å¥–åŠ±é…ç½®*ï¼šåœ¨ä¸åŒä»»åŠ¡ä¸­ï¼Œç›¸åŒè¡Œä¸ºå¯èƒ½æ ¹æ®$R_i$è€Œè·å¾—ä¸åŒçš„å¥–åŠ±ã€‚'
- en: 'Or, the *environment*: The transition function $P_i$ can be reshaped by initializing
    the environment with varying shifts between states.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ–è€…ï¼Œ*ç¯å¢ƒ*ï¼šè¿‡æ¸¡å‡½æ•°$P_i$å¯ä»¥é€šè¿‡åœ¨çŠ¶æ€ä¹‹é—´åˆå§‹åŒ–ä¸åŒçš„åç§»æ¥é‡æ–°å¡‘é€ ã€‚
- en: Task Generation by Domain Randomization
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é€šè¿‡é¢†åŸŸéšæœºåŒ–ç”Ÿæˆä»»åŠ¡
- en: Randomizing parameters in a simulator is an easy way to obtain tasks with modified
    transition functions. If interested in learning further, check my last [post](https://lilianweng.github.io/posts/2019-05-05-domain-randomization/)
    on **domain randomization**.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¨¡æ‹Ÿå™¨ä¸­éšæœºåŒ–å‚æ•°æ˜¯è·å¾—å…·æœ‰ä¿®æ”¹è¿‡æ¸¡å‡½æ•°çš„ä»»åŠ¡çš„ç®€å•æ–¹æ³•ã€‚ å¦‚æœæƒ³è¿›ä¸€æ­¥äº†è§£ï¼Œè¯·æŸ¥çœ‹æˆ‘ä¸Šä¸€ç¯‡å…³äº**é¢†åŸŸéšæœºåŒ–**çš„[æ–‡ç« ](https://lilianweng.github.io/posts/2019-05-05-domain-randomization/)ã€‚
- en: Evolutionary Algorithm on Environment Generation
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¯å¢ƒç”Ÿæˆä¸Šçš„è¿›åŒ–ç®—æ³•
- en: '[Evolutionary algorithm](https://en.wikipedia.org/wiki/Evolutionary_algorithm)
    is a gradient-free heuristic-based optimization method, inspired by natural selection.
    A population of solutions follows a loop of evaluation, selection, reproduction,
    and mutation. Eventually, good solutions survive and thus get selected.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[è¿›åŒ–ç®—æ³•](https://zh.wikipedia.org/wiki/%E9%80%B2%E5%8C%96%E7%AE%97%E6%B3%95)æ˜¯ä¸€ç§åŸºäºå¯å‘å¼çš„æ— æ¢¯åº¦ä¼˜åŒ–æ–¹æ³•ï¼Œçµæ„Ÿæ¥è‡ªè‡ªç„¶é€‰æ‹©ã€‚
    ä¸€ç¾¤è§£å†³æ–¹æ¡ˆéµå¾ªè¯„ä¼°ã€é€‰æ‹©ã€ç¹æ®–å’Œçªå˜çš„å¾ªç¯ã€‚ æœ€ç»ˆï¼Œå¥½çš„è§£å†³æ–¹æ¡ˆä¼šå­˜æ´»ä¸‹æ¥å¹¶è¢«é€‰ä¸­ã€‚'
- en: '**POET** ([Wang et al, 2019](https://arxiv.org/abs/1901.01753)), a framework
    based on the evolutionary algorithm, attempts to generate tasks while the problems
    themselves are being solved. The implementation of POET is only specifically designed
    for a simple 2D [bipedal walker](https://gym.openai.com/envs/BipedalWalkerHardcore-v2/)
    environment but points out an interesting direction. It is noteworthy that the
    evolutionary algorithm has had some compelling applications in Deep Learning like
    [EPG](#meta-learning-the-loss-function) and PBT (Population-Based Training; [Jaderberg
    et al, 2017](https://arxiv.org/abs/1711.09846)).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**POET**ï¼ˆ[Wang ç­‰äººï¼Œ2019](https://arxiv.org/abs/1901.01753)ï¼‰ï¼Œä¸€ä¸ªåŸºäºè¿›åŒ–ç®—æ³•çš„æ¡†æ¶ï¼Œè¯•å›¾åœ¨è§£å†³é—®é¢˜çš„åŒæ—¶ç”Ÿæˆä»»åŠ¡ã€‚
    POET çš„å®ç°ä»…é’ˆå¯¹ç®€å•çš„ 2D [åŒè¶³è¡Œèµ°è€…](https://gym.openai.com/envs/BipedalWalkerHardcore-v2/)
    ç¯å¢ƒï¼Œä½†æŒ‡å‡ºäº†ä¸€ä¸ªæœ‰è¶£çš„æ–¹å‘ã€‚ å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿›åŒ–ç®—æ³•åœ¨æ·±åº¦å­¦ä¹ ä¸­å·²ç»æœ‰ä¸€äº›å¼•äººæ³¨ç›®çš„åº”ç”¨ï¼Œå¦‚[EPG](#meta-learning-the-loss-function)å’Œ
    PBTï¼ˆåŸºäºç§ç¾¤çš„è®­ç»ƒï¼›[Jaderberg ç­‰äººï¼Œ2017](https://arxiv.org/abs/1711.09846)ï¼‰ã€‚'
- en: '![](../Images/3aee69cb0be032384774980606c2e0f4.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3aee69cb0be032384774980606c2e0f4.png)'
- en: 'Fig. 8\. An example bipedal walking environment (top) and an overview of POET
    (bottom). (Image source: [POET blog post](https://eng.uber.com/poet-open-ended-deep-learning/))'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8\. ä¸€ä¸ªåŒè¶³è¡Œèµ°ç¯å¢ƒç¤ºä¾‹ï¼ˆä¸Šï¼‰å’Œ POET æ¦‚è¿°ï¼ˆä¸‹ï¼‰ã€‚ (å›¾ç‰‡æ¥æºï¼š[POET åšå®¢æ–‡ç« ](https://eng.uber.com/poet-open-ended-deep-learning/))
- en: 'The 2D bipedal walking environment is evolving: from a simple flat surface
    to a much more difficult trail with potential gaps, stumps, and rough terrains.
    POET pairs the generation of environmental challenges and the optimization of
    agents together so as to (a) select agents that can resolve current challenges
    and (b) evolve environments to be solvable. The algorithm maintains a list of
    *environment-agent pairs* and repeats the following:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 2D åŒè¶³è¡Œèµ°ç¯å¢ƒæ­£åœ¨å‘å±•ï¼šä»ç®€å•çš„å¹³å¦è¡¨é¢åˆ°å…·æœ‰æ½œåœ¨é—´éš™ã€æ ‘æ¡©å’Œå´å²–åœ°å½¢çš„æ›´åŠ å›°éš¾çš„è·¯å¾„ã€‚ POET å°†ç¯å¢ƒæŒ‘æˆ˜çš„ç”Ÿæˆä¸ä»£ç†çš„ä¼˜åŒ–é…å¯¹åœ¨ä¸€èµ·ï¼Œä»¥ä¾¿ (a)
    é€‰æ‹©èƒ½å¤Ÿè§£å†³å½“å‰æŒ‘æˆ˜çš„ä»£ç†å’Œ (b) è¿›åŒ–ç¯å¢ƒä»¥ä¾¿è§£å†³ã€‚ è¯¥ç®—æ³•ç»´æŠ¤ä¸€ä¸ª*ç¯å¢ƒ-ä»£ç†å¯¹*åˆ—è¡¨ï¼Œå¹¶é‡å¤ä»¥ä¸‹æ­¥éª¤ï¼š
- en: '*Mutation*: Generate new environments from currently active environments. Note
    that here types of mutation operations are created just for bipedal walker and
    a new environment would demand a new set of configurations.'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*çªå˜*ï¼šä»å½“å‰æ´»è·ƒç¯å¢ƒç”Ÿæˆæ–°ç¯å¢ƒã€‚ è¯·æ³¨æ„ï¼Œè¿™é‡Œçš„çªå˜æ“ä½œç±»å‹ä»…é’ˆå¯¹åŒè¶³è¡Œèµ°è€…ï¼Œæ–°ç¯å¢ƒå°†éœ€è¦ä¸€ç»„æ–°çš„é…ç½®ã€‚'
- en: '*Optimization*: Train paired agents within their respective environments.'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*ä¼˜åŒ–*ï¼šåœ¨å„è‡ªçš„ç¯å¢ƒä¸­è®­ç»ƒé…å¯¹çš„ä»£ç†ã€‚'
- en: '*Selection*: Periodically attempt to transfer current agents from one environment
    to another. Copy and update the best performing agent for every environment. The
    intuition is that skills learned in one environment might be helpful for a different
    environment.'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*é€‰æ‹©*ï¼šå®šæœŸå°è¯•å°†å½“å‰ä»£ç†ä»ä¸€ä¸ªç¯å¢ƒè½¬ç§»åˆ°å¦ä¸€ä¸ªç¯å¢ƒã€‚ å¤åˆ¶å¹¶æ›´æ–°æ¯ä¸ªç¯å¢ƒä¸­è¡¨ç°æœ€ä½³çš„ä»£ç†ã€‚ ç›´è§‰æ˜¯åœ¨ä¸€ä¸ªç¯å¢ƒä¸­å­¦åˆ°çš„æŠ€èƒ½å¯èƒ½å¯¹å¦ä¸€ä¸ªç¯å¢ƒæœ‰æ‰€å¸®åŠ©ã€‚'
- en: The procedure above is quite similar to [PBT](https://arxiv.org/abs/1711.09846),
    but PBT mutates and evolves hyperparameters instead. To some extent, POET is doing
    [domain randomization](https://lilianweng.github.io/posts/2019-05-05-domain-randomization/),
    as all the gaps, stumps and terrain roughness are controlled by some randomization
    probability parameters. Different from DR, the agents are not exposed to a fully
    randomized difficult environment all at once, but instead they are learning gradually
    with a curriculum configured by the evolutionary algorithm.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°è¿‡ç¨‹ä¸[PBT](https://arxiv.org/abs/1711.09846)éå¸¸ç›¸ä¼¼ï¼Œä½†PBTä¼šæ”¹å˜å’Œæ¼”åŒ–è¶…å‚æ•°ã€‚åœ¨æŸç§ç¨‹åº¦ä¸Šï¼ŒPOETæ­£åœ¨è¿›è¡Œ[é¢†åŸŸéšæœºåŒ–](https://lilianweng.github.io/posts/2019-05-05-domain-randomization/)ï¼Œå› ä¸ºæ‰€æœ‰çš„é—´éš™ã€æ ‘æ¡©å’Œåœ°å½¢ç²—ç³™åº¦éƒ½ç”±ä¸€äº›éšæœºåŒ–æ¦‚ç‡å‚æ•°æ§åˆ¶ã€‚ä¸DRä¸åŒï¼Œä»£ç†ä»¬ä¸ä¼šä¸€æ¬¡æ€§æš´éœ²äºå®Œå…¨éšæœºåŒ–çš„å›°éš¾ç¯å¢ƒä¸­ï¼Œè€Œæ˜¯é€šè¿‡è¿›åŒ–ç®—æ³•é…ç½®çš„è¯¾ç¨‹é€æ¸å­¦ä¹ ã€‚
- en: Learning with Random Rewards
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨éšæœºå¥–åŠ±è¿›è¡Œå­¦ä¹ 
- en: An MDP without a reward function $R$ is known as a *Controlled Markov process*
    (CMP). Given a predefined CMP, $\langle \mathcal{S}, \mathcal{A}, P\rangle$, we
    can acquire a variety of tasks by generating a collection of reward functions
    $\mathcal{R}$ that encourage the training of an effective meta-learning policy.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: æ²¡æœ‰å¥–åŠ±å‡½æ•°$R$çš„MDPè¢«ç§°ä¸º*å—æ§é©¬å°”å¯å¤«è¿‡ç¨‹*ï¼ˆCMPï¼‰ã€‚ç»™å®šé¢„å®šä¹‰çš„CMPï¼Œ$\langle \mathcal{S}, \mathcal{A},
    P\rangle$ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ç”Ÿæˆä¸€ç³»åˆ—é¼“åŠ±æœ‰æ•ˆå…ƒå­¦ä¹ ç­–ç•¥è®­ç»ƒçš„å¥–åŠ±å‡½æ•°$\mathcal{R}$æ¥è·å¾—å„ç§ä»»åŠ¡ã€‚
- en: '[Gupta et al. (2018)](https://arxiv.org/abs/1806.04640) proposed two unsupervised
    approaches for growing the task distribution in the context of CMP. Assuming there
    is an underlying latent variable $z \sim p(z)$ associated with every task, it
    parameterizes/determines a reward function: $r_z(s) = \log D(z|s)$, where a â€œdiscriminatorâ€
    function $D(.)$ is used to extract the latent variable from the state. The paper
    described two ways to construct a discriminator function:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[Guptaç­‰äººï¼ˆ2018ï¼‰](https://arxiv.org/abs/1806.04640)æå‡ºäº†ä¸¤ç§æ— ç›‘ç£æ–¹æ³•ï¼Œç”¨äºåœ¨CMPèƒŒæ™¯ä¸‹æ‰©å±•ä»»åŠ¡åˆ†å¸ƒã€‚å‡è®¾æ¯ä¸ªä»»åŠ¡éƒ½æœ‰ä¸€ä¸ªæ½œåœ¨çš„æ½œå˜é‡$z
    \sim p(z)$ï¼Œä¸æ¯ä¸ªä»»åŠ¡ç›¸å…³è”ï¼Œå®ƒå‚æ•°åŒ–/ç¡®å®šäº†ä¸€ä¸ªå¥–åŠ±å‡½æ•°ï¼š$r_z(s) = \log D(z|s)$ï¼Œå…¶ä¸­â€œé‰´åˆ«å™¨â€å‡½æ•°$D(.)$ç”¨äºä»çŠ¶æ€ä¸­æå–æ½œå˜é‡ã€‚è®ºæ–‡æè¿°äº†æ„å»ºé‰´åˆ«å™¨å‡½æ•°çš„ä¸¤ç§æ–¹æ³•ï¼š'
- en: Sample random weights $\phi_\text{rand}$ of the discriminator, $D_{\phi_\text{rand}}(z
    \mid s)$.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ ·æœ¬éšæœºæƒé‡$\phi_\text{rand}$çš„é‰´åˆ«å™¨ï¼Œ$D_{\phi_\text{rand}}(z \mid s)$ã€‚
- en: Learn a discriminator function to encourage diversity-driven exploration. This
    method is introduced in more details in another sister paper â€œDIAYNâ€ ([Eysenbach
    et al., 2018](https://arxiv.org/abs/1802.06070)).
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å­¦ä¹ ä¸€ä¸ªé‰´åˆ«å™¨å‡½æ•°ä»¥é¼“åŠ±å¤šæ ·åŒ–é©±åŠ¨çš„æ¢ç´¢ã€‚è¿™ç§æ–¹æ³•åœ¨å¦ä¸€ç¯‡å§Šå¦¹è®ºæ–‡â€œDIAYNâ€ä¸­æœ‰æ›´è¯¦ç»†çš„ä»‹ç»ï¼ˆ[Eysenbachç­‰äººï¼Œ2018](https://arxiv.org/abs/1802.06070)ï¼‰ã€‚
- en: 'DIAYN, short for â€œDiversity is all you needâ€, is a framework to encourage a
    policy to learn useful skills without a reward function. It explicitly models
    the latent variable $z$ as a *skill* embedding and makes the policy conditioned
    on $z$ in addition to state $s$, $\pi_\theta(a \mid s, z)$. (Ok, this part is
    same as [MAESN](#meta-learning-the-exploration-strategies) unsurprisingly, as
    the papers are from the same group.) The design of DIAYN is motivated by a few
    hypotheses:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: DIAYNï¼Œå³â€œå¤šæ ·æ€§å°±æ˜¯ä½ æ‰€éœ€è¦çš„â€ï¼Œæ˜¯ä¸€ç§é¼“åŠ±ç­–ç•¥å­¦ä¹ æœ‰ç”¨æŠ€èƒ½è€Œæ— éœ€å¥–åŠ±å‡½æ•°çš„æ¡†æ¶ã€‚å®ƒæ˜ç¡®åœ°å°†æ½œå˜é‡$z$å»ºæ¨¡ä¸º*æŠ€èƒ½*åµŒå…¥ï¼Œå¹¶ä½¿ç­–ç•¥é™¤äº†çŠ¶æ€$s$å¤–è¿˜å–å†³äº$z$ï¼Œ$\pi_\theta(a
    \mid s, z)$ã€‚ï¼ˆå—¯ï¼Œè¿™éƒ¨åˆ†ä¸[MAESN](#meta-learning-the-exploration-strategies)ä¸€æ ·ï¼Œæ¯«ä¸å¥‡æ€ªï¼Œå› ä¸ºè¿™äº›è®ºæ–‡æ¥è‡ªåŒä¸€å›¢é˜Ÿã€‚ï¼‰DIAYNçš„è®¾è®¡å—åˆ°å‡ ä¸ªå‡è®¾çš„å¯å‘ï¼š
- en: Skills should be diverse and lead to visitations of different states. â†’ maximize
    the mutual information between states and skills, $I(S; Z)$
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æŠ€èƒ½åº”è¯¥å¤šæ ·åŒ–ï¼Œå¹¶å¯¼è‡´è®¿é—®ä¸åŒçŠ¶æ€ã€‚â†’ æœ€å¤§åŒ–çŠ¶æ€å’ŒæŠ€èƒ½ä¹‹é—´çš„äº’ä¿¡æ¯ï¼Œ$I(S; Z)$
- en: Skills should be distinguishable by states, not actions. â†’ minimize the mutual
    information between actions and skills, conditioned on states $I(A; Z \mid S)$
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æŠ€èƒ½åº”è¯¥ç”±çŠ¶æ€è€Œä¸æ˜¯åŠ¨ä½œæ¥åŒºåˆ†ã€‚â†’ æœ€å°åŒ–åœ¨ç»™å®šçŠ¶æ€çš„æƒ…å†µä¸‹åŠ¨ä½œå’ŒæŠ€èƒ½ä¹‹é—´çš„äº’ä¿¡æ¯ï¼Œ$I(A; Z \mid S)$
- en: 'The objective function to maximize is as follows, where the policy entropy
    is also added to encourage diversity:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æœ€å¤§åŒ–çš„ç›®æ ‡å‡½æ•°å¦‚ä¸‹ï¼ŒåŒæ—¶è¿˜æ·»åŠ äº†ç­–ç•¥ç†µä»¥é¼“åŠ±å¤šæ ·æ€§ï¼š
- en: $$ \begin{aligned} \mathcal{F}(\theta) &= I(S; Z) + H[A \mid S] - I(A; Z \mid
    S) & \\ &= (H(Z) - H(Z \mid S)) + H[A \mid S] - (H[A\mid S] - H[A\mid S, Z]) &
    \\ &= H[A\mid S, Z] \color{green}{- H(Z \mid S) + H(Z)} & \\ &= H[A\mid S, Z]
    + \mathbb{E}_{z\sim p(z), s\sim\rho(s)}[\log p(z \mid s)] - \mathbb{E}_{z\sim
    p(z)}[\log p(z)] & \scriptstyle{\text{; can infer skills from states & p(z) is
    diverse.}} \\ &\ge H[A\mid S, Z] + \mathbb{E}_{z\sim p(z), s\sim\rho(s)}[\color{red}{\log
    D_\phi(z \mid s) - \log p(z)}] & \scriptstyle{\text{; according to Jensen's inequality;
    "pseudo-reward" in red.}} \end{aligned} $$
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \mathcal{F}(\theta) &= I(S; Z) + H[A \mid S] - I(A; Z \mid
    S) & \\ &= (H(Z) - H(Z \mid S)) + H[A \mid S] - (H[A\mid S] - H[A\mid S, Z]) &
    \\ &= H[A\mid S, Z] \color{green}{- H(Z \mid S) + H(Z)} & \\ &= H[A\mid S, Z]
    + \mathbb{E}_{z\sim p(z), s\sim\rho(s)}[\log p(z \mid s)] - \mathbb{E}_{z\sim
    p(z)}[\log p(z)] & \scriptstyle{\text{ï¼›å¯ä»¥ä»çŠ¶æ€æ¨æ–­æŠ€èƒ½ & p(z)æ˜¯å¤šæ ·çš„ã€‚}} \\ &\ge H[A\mid
    S, Z] + \mathbb{E}_{z\sim p(z), s\sim\rho(s)}[\color{red}{\log D_\phi(z \mid s)
    - \log p(z)}] & \scriptstyle{\text{ï¼›æ ¹æ®Jensenä¸ç­‰å¼ï¼›çº¢è‰²çš„â€œä¼ªå¥–åŠ±â€ã€‚}} \end{aligned} $$
- en: where $I(.)$ is mutual information and $H[.]$ is entropy measure. We cannot
    integrate all states to compute $p(z \mid s)$, so approximate it with $D_\phi(z
    \mid s)$ â€” that is the diversity-driven discriminator function.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­$I(.)$æ˜¯äº’ä¿¡æ¯ï¼Œ$H[.]$æ˜¯ç†µåº¦é‡ã€‚æˆ‘ä»¬æ— æ³•æ•´åˆæ‰€æœ‰çŠ¶æ€æ¥è®¡ç®—$p(z \mid s)$ï¼Œå› æ­¤ç”¨$D_\phi(z \mid s)$æ¥è¿‘ä¼¼â€”â€”è¿™å°±æ˜¯ä»¥å¤šæ ·æ€§ä¸ºé©±åŠ¨çš„é‰´åˆ«å™¨å‡½æ•°ã€‚
- en: '![](../Images/e4eb400768f80615c99052b80b0afdcc.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4eb400768f80615c99052b80b0afdcc.png)'
- en: 'Fig. 9\. DIAYN Algorithm. (Image source: [Eysenbach et al., 2019](https://arxiv.org/abs/1802.06070))'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9\. DIAYNç®—æ³•ã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼š[Eysenbachç­‰äººï¼Œ2019](https://arxiv.org/abs/1802.06070)ï¼‰
- en: 'Once the discriminator function is learned, sampling a new MDP for training
    is strainght-forward: First, sample a latent variable, $z \sim p(z)$ and construct
    a reward function $r_z(s) = \log(D(z \vert s))$. Pairing the reward function with
    a predefined CMP creates a new MDP.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦å­¦ä¹ äº†é‰´åˆ«å™¨å‡½æ•°ï¼Œå¯¹äºè®­ç»ƒæ¥è¯´ï¼Œé‡‡æ ·ä¸€ä¸ªæ–°çš„MDPæ˜¯ç›´æˆªäº†å½“çš„ï¼šé¦–å…ˆï¼Œé‡‡æ ·ä¸€ä¸ªæ½œå˜é‡ï¼Œ$z \sim p(z)$å¹¶æ„å»ºä¸€ä¸ªå¥–åŠ±å‡½æ•°$r_z(s) =
    \log(D(z \vert s))$ã€‚å°†å¥–åŠ±å‡½æ•°ä¸é¢„å®šä¹‰çš„CMPé…å¯¹ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„MDPã€‚
- en: '* * *'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Cited as:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: å¼•ç”¨ä¸ºï¼š
- en: '[PRE0]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: References
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] Richard S. Sutton. [â€œThe Bitter Lesson.â€](http://incompleteideas.net/IncIdeas/BitterLesson.html)
    March 13, 2019.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Richard S. Suttonã€‚[â€œè‹¦æ¶©çš„æ•™è®­ã€‚â€](http://incompleteideas.net/IncIdeas/BitterLesson.html)
    2019å¹´3æœˆ13æ—¥ã€‚'
- en: '[2] Sepp Hochreiter, A. Steven Younger, and Peter R. Conwell. [â€œLearning to
    learn using gradient descent.â€](http://snowedin.net/tmp/Hochreiter2001.pdf) Intl.
    Conf. on Artificial Neural Networks. 2001.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Sepp Hochreiter, A. Steven Youngerå’ŒPeter R. Conwellã€‚[â€œä½¿ç”¨æ¢¯åº¦ä¸‹é™å­¦ä¹ å­¦ä¹ ã€‚â€](http://snowedin.net/tmp/Hochreiter2001.pdf)
    äººå·¥ç¥ç»ç½‘ç»œå›½é™…ä¼šè®®ã€‚2001å¹´ã€‚'
- en: '[3] Jane X Wang, et al. [â€œLearning to reinforcement learn.â€](https://arxiv.org/abs/1611.05763)
    arXiv preprint arXiv:1611.05763 (2016).'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] ç‹ç®€ç­‰äººã€‚[â€œå­¦ä¹ å¼ºåŒ–å­¦ä¹ ã€‚â€](https://arxiv.org/abs/1611.05763) arXivé¢„å°æœ¬arXiv:1611.05763ï¼ˆ2016å¹´ï¼‰ã€‚'
- en: '[4] Yan Duan, et al. [â€œRL $^ 2$: Fast Reinforcement Learning via Slow Reinforcement
    Learning.â€](https://arxiv.org/abs/1611.02779) ICLR 2017.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] æ®µç‡•ç­‰äººã€‚[â€œRL $^ 2$ï¼šé€šè¿‡ç¼“æ…¢å¼ºåŒ–å­¦ä¹ å¿«é€Ÿå¼ºåŒ–å­¦ä¹ ã€‚â€](https://arxiv.org/abs/1611.02779) ICLR
    2017ã€‚'
- en: '[5] Matthew Botvinick, et al. [â€œReinforcement Learning, Fast and Slowâ€](https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(19)30061-0)
    Cell Review, Volume 23, Issue 5, P408-422, May 01, 2019.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Matthew Botvinickç­‰äººã€‚[â€œå¼ºåŒ–å­¦ä¹ ï¼Œå¿«ä¸æ…¢â€](https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(19)30061-0)
    Cell Reviewï¼Œç¬¬23å·ï¼Œç¬¬5æœŸï¼ŒP408-422ï¼Œ2019å¹´5æœˆ1æ—¥ã€‚'
- en: '[6] Jeff Clune. [â€œAI-GAs: AI-generating algorithms, an alternate paradigm for
    producing general artificial intelligenceâ€](https://arxiv.org/abs/1905.10985)
    arXiv preprint arXiv:1905.10985 (2019).'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Jeff Cluneã€‚[â€œAI-GAsï¼šç”Ÿæˆäººå·¥æ™ºèƒ½çš„ç®—æ³•ï¼Œä¸€ç§äº§ç”Ÿé€šç”¨äººå·¥æ™ºèƒ½çš„æ›¿ä»£èŒƒå¼â€](https://arxiv.org/abs/1905.10985)
    arXivé¢„å°æœ¬arXiv:1905.10985ï¼ˆ2019å¹´ï¼‰ã€‚'
- en: '[7] Zhongwen Xu, et al. [â€œMeta-Gradient Reinforcement Learningâ€](http://papers.nips.cc/paper/7507-meta-gradient-reinforcement-learning.pdf)
    NIPS 2018.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] å¾ä¸­æ–‡ç­‰äººã€‚[â€œå…ƒæ¢¯åº¦å¼ºåŒ–å­¦ä¹ â€](http://papers.nips.cc/paper/7507-meta-gradient-reinforcement-learning.pdf)
    NIPS 2018ã€‚'
- en: '[8] Rein Houthooft, et al. [â€œEvolved Policy Gradients.â€](https://papers.nips.cc/paper/7785-evolved-policy-gradients.pdf)
    NIPS 2018.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Rein Houthooftç­‰äººã€‚[â€œè¿›åŒ–ç­–ç•¥æ¢¯åº¦ã€‚â€](https://papers.nips.cc/paper/7785-evolved-policy-gradients.pdf)
    NIPS 2018ã€‚'
- en: '[9] Tim Salimans, et al. [â€œEvolution strategies as a scalable alternative to
    reinforcement learning.â€](https://arxiv.org/abs/1703.03864) arXiv preprint arXiv:1703.03864
    (2017).'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Tim Salimansç­‰äººã€‚[â€œè¿›åŒ–ç­–ç•¥ä½œä¸ºå¯æ‰©å±•çš„å¼ºåŒ–å­¦ä¹ æ›¿ä»£æ–¹æ¡ˆã€‚â€](https://arxiv.org/abs/1703.03864)
    arXivé¢„å°æœ¬arXiv:1703.03864ï¼ˆ2017å¹´ï¼‰ã€‚'
- en: '[10] Abhishek Gupta, et al. [â€œMeta-Reinforcement Learning of Structured Exploration
    Strategies.â€](http://papers.nips.cc/paper/7776-meta-reinforcement-learning-of-structured-exploration-strategies.pdf)
    NIPS 2018.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Abhishek Guptaç­‰äººã€‚[â€œç»“æ„åŒ–æ¢ç´¢ç­–ç•¥çš„å…ƒå¼ºåŒ–å­¦ä¹ ã€‚â€](http://papers.nips.cc/paper/7776-meta-reinforcement-learning-of-structured-exploration-strategies.pdf)
    NIPS 2018ã€‚'
- en: '[11] Alexander Pritzel, et al. [â€œNeural episodic control.â€](https://arxiv.org/abs/1703.01988)
    Proc. Intl. Conf. on Machine Learning, Volume 70, 2017.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Alexander Pritzelç­‰äºº [â€œç¥ç»å…ƒæƒ…èŠ‚æ§åˆ¶ã€‚â€](https://arxiv.org/abs/1703.01988) æœºå™¨å­¦ä¹ å›½é™…ä¼šè®®è®ºæ–‡é›†ï¼Œç¬¬70å·ï¼Œ2017.'
- en: '[12] Charles Blundell, et al. [â€œModel-free episodic control.â€](https://arxiv.org/abs/1606.04460)
    arXiv preprint arXiv:1606.04460 (2016).'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Charles Blundellç­‰äºº [â€œæ— æ¨¡å‹çš„æƒ…èŠ‚æ§åˆ¶ã€‚â€](https://arxiv.org/abs/1606.04460) arXivé¢„å°æœ¬
    arXiv:1606.04460 (2016).'
- en: '[13] Samuel Ritter, et al. [â€œBeen there, done that: Meta-learning with episodic
    recall.â€](https://arxiv.org/abs/1805.09692) ICML, 2018.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Samuel Ritterç­‰äºº [â€œæ›¾ç»å†è¿‡ï¼Œå·²ç»åšè¿‡ï¼šå…·æœ‰æƒ…èŠ‚å›å¿†çš„å…ƒå­¦ä¹ ã€‚â€](https://arxiv.org/abs/1805.09692)
    ICML, 2018.'
- en: '[14] Rui Wang et al. [â€œPaired Open-Ended Trailblazer (POET): Endlessly Generating
    Increasingly Complex and Diverse Learning Environments and Their Solutionsâ€](https://arxiv.org/abs/1901.01753)
    arXiv preprint arXiv:1901.01753 (2019).'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Rui Wangç­‰äºº [â€œé…å¯¹å¼€æ”¾å¼å…ˆé©±è€…ï¼ˆPOETï¼‰ï¼šä¸æ–­ç”Ÿæˆè¶Šæ¥è¶Šå¤æ‚å’Œå¤šæ ·åŒ–çš„å­¦ä¹ ç¯å¢ƒåŠå…¶è§£å†³æ–¹æ¡ˆâ€](https://arxiv.org/abs/1901.01753)
    arXivé¢„å°æœ¬ arXiv:1901.01753 (2019).'
- en: '[15] Uber Engineering Blog: [â€œPOET: Endlessly Generating Increasingly Complex
    and Diverse Learning Environments and their Solutions through the Paired Open-Ended
    Trailblazer.â€](https://eng.uber.com/poet-open-ended-deep-learning/) Jan 8, 2019.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Uberå·¥ç¨‹åšå®¢: [â€œPOETï¼šé€šè¿‡é…å¯¹å¼€æ”¾å¼å…ˆé©±è€…ä¸æ–­ç”Ÿæˆè¶Šæ¥è¶Šå¤æ‚å’Œå¤šæ ·åŒ–çš„å­¦ä¹ ç¯å¢ƒåŠå…¶è§£å†³æ–¹æ¡ˆã€‚â€](https://eng.uber.com/poet-open-ended-deep-learning/)
    2019å¹´1æœˆ8æ—¥.'
- en: '[16] Abhishek Gupta, et al.[â€œUnsupervised meta-learning for Reinforcement Learningâ€](https://arxiv.org/abs/1806.04640)
    arXiv preprint arXiv:1806.04640 (2018).'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Abhishek Guptaç­‰äºº [â€œå¼ºåŒ–å­¦ä¹ çš„æ— ç›‘ç£å…ƒå­¦ä¹ â€](https://arxiv.org/abs/1806.04640) arXivé¢„å°æœ¬
    arXiv:1806.04640 (2018).'
- en: '[17] Eysenbach, Benjamin, et al. [â€œDiversity is all you need: Learning skills
    without a reward function.â€](https://arxiv.org/abs/1802.06070) ICLR 2019.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Eysenbach, Benjaminç­‰äºº [â€œå¤šæ ·æ€§å°±æ˜¯ä½ æ‰€éœ€è¦çš„ï¼šåœ¨æ²¡æœ‰å¥–åŠ±å‡½æ•°çš„æƒ…å†µä¸‹å­¦ä¹ æŠ€èƒ½ã€‚â€](https://arxiv.org/abs/1802.06070)
    ICLR 2019.'
- en: '[18] Max Jaderberg, et al. [â€œPopulation Based Training of Neural Networks.â€](https://arxiv.org/abs/1711.09846)
    arXiv preprint arXiv:1711.09846 (2017).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Max Jaderbergç­‰äºº [â€œåŸºäºç§ç¾¤çš„ç¥ç»ç½‘ç»œè®­ç»ƒã€‚â€](https://arxiv.org/abs/1711.09846) arXivé¢„å°æœ¬
    arXiv:1711.09846 (2017).'
